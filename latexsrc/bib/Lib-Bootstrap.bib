% Encoding: US-ASCII

@Article{
	Lib-Bootstrap-Efron-1979a
	,
	author = {
		Efron, Bradley
	},
	date = {
		1979-01
	},
	journaltitle = {
		The Annals of Statistics
	},
	title = {
		Bootstrap methods: 
		Another look at the jackknife
	},
	doi = {
		10.1214/aos/1176344552
	},
	number = {
		1
	},
	volume = {
		7
	},
	publisher = {
		Institute of Mathematical Statistics
	},
	file = {
		references/10.1214%2Faos%2F1176344552.pdf
	},
	library = {},
	keywords = {
		bootstrap,
		discriminant analysis,
		error rate estimation,
		jackknife,
		nonlinear regression,
		nonparametric variance estimation,
		resampling,
		subsample values
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Bootstrap
	},
	abstract = {
		We discuss the following problem:
		given a random sample
		$
		\mathbf{X}
		=
		\left(
		X_1 ,
		X_2 ,
		\dots ,
		X_n
		\right)
		$
		from an unknown probability distribution
		$
		F
		$,
		estimate the sampling distribution
		of some prespecified random variable
		$
		R
		\left(
		\mathbf{X},
		F
		\right)
		$,
		on the basis of the observed data
		$
		\mathbf{x}
		$.
		(Standard jackknife theory
		gives an approximate mean and variance
		in the case
		$
		R
		\left(
		\mathbf{X},
		F
		\right)
		=
		\theta
		\left(
		\hat{F}
		\right)
		-
		\theta
		\left(
		F
		\right)
		$,
		$
		\theta
		$ some parameter of interest.)
		A general method,
		called the ``bootstrap''
		is introduced,
		and shown to work satisfactorily
		on a variety of estimation problems.
		The jackknife is shown to be a linear approximation method
		for the bootstrap.
		The exposition proceeds by a series of examples:
		variance of the sample median,
		error rates in a linear discriminant analysis,
		ratio estimation,
		estimating regression parameters, etc.
	},
}

@Article{
	Lib-Bootstrap-Efron-1979b
	,
	author       = {
		Efron, Bradley
	},
	date         = {
		1979-10
	},
	journaltitle = {
		{SIAM} Review
	},
	title        = {
		Computers and the Theory of Statistics: Thinking the Unthinkable
	},
	doi          = {
		10.1137/1021092
	},
	number       = {
		4
	},
	pages        = {
		460--480
	},
	volume       = {
		21
	},
	publisher    = {
		Society for Industrial {\&} Applied Mathematics ({SIAM})
	},
	file = {
		references/10.1137%2F1021092.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Bootstrap
	},
	abstract     = {
		This is a survey article concerning recent advances in certain areas of statistical theory,
		written for a mathematical audience with no background in statistics.
		The topics are chosen to illustrate a special point:
		how the advent of the high-speed computer has affected the development of statistical theory.
		The topics discussed include nonparametric methods,
		the jackknife,
		the bootstrap,
		cross-validation,
		error-rate estimation in discriminant analysis,
		robust estimation,
		the influence function,
		censored data,
		the EM algorithm,
		and Cox's likelihood function.
		The exposition is mainly by example, with only a little offered in the way of theoretical development.
	},
}

@Article{
	Lib-Bootstrap-Efron-1981a
	,
	author       = {
		Efron, Bradley
	},
	date         = {
		1981
	},
	journaltitle = {
		Canadian Journal of Statistics / La Revue Canadienne de Statistique 
	},
	title        = {
		Nonparametric standard errors and confidence intervals
	},
	doi          = {
		10.2307/3314608
	},
	number       = {
		2
	},
	pages        = {
		139--158
	},
	volume       = {
		9
	},
	publisher    = {Wiley},
	file = {
		references/10.2307%252F3314608.pdf
	},
	library = {},
	keywords = {
		bootstrap,
		jackknife,
		delta method,
		nonparametric confidence intervals,
		nonparametric standard errors
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Bootstrap
	},
	abstract     = {
		We investigate several nonparametric methods; 
		the bootstrap, 
		the jackknife, 
		the delta method, 
		and other related techniques. 
		The first and simplest goal is the assignment of nonparametric standard errors 
		to a real-valued statistic. 
		More ambitiously, 
		we consider setting nonparametric confidence intervals 
		for a real-valued parameter. 
		Building on the well understood case of confidence intervals 
		for the median, 
		some hopeful evidence is presented that such a theory may be possible.
	},
}

@Article{
	Lib-Bootstrap-Barnard-1981,
	author       = {
		Barnard, George A.
		and
		Collins, J. R.
		and
		Farewell, V. T.
		and
		Field, C. A.
		and
		Kalbfleisch, J. D.
		and
		Nash, Stanley W.
		and
		Parzen, Emanuel
		and
		Prentice, Ross L.
		and
		Reid, Nancy
		and
		Sprott, D. A.
		and
		Switzer, Paul
		and
		Warren, W. G.
		and
		Weldon, K. L.
	},
	date         = {1981},
	journaltitle = {The Canadian Journal of Statistics / La Revue Canadienne de Statistique},
	title        = {Nonparametric Standard Errors and Confidence Intervals: Discussion},
	doi          = {10.2307/3314609},
	number       = {2},
	pages        = {158--170},
	volume       = {9},
	publisher    = {Wiley},
	file = {
		references/10.2307%2F3314609.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Bootstrap
	},
	abstract = {},
}

@Article{
	Lib-Bootstrap-Efron-1981b,
	author       = {Efron, Bradley},
	date         = {1981},
	journaltitle = {The Canadian Journal of Statistics / La Revue Canadienne de Statistique},
	title        = {Nonparametric Standard Errors and Confidence Intervals: Rejoinder},
	doi          = {10.2307/3314610},
	number       = {2},
	pages        = {170--172},
	volume       = {9},
	publisher    = {Wiley},
	file = {
		references/10.2307%2F3314610.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Bootstrap
	},
	abstract = {},
}


@Article{
	Lib-Bootstrap-Efron-1987
	,
	author = {
		Efron, Bradley
	},
	date = {
		1987-03
	},
	journaltitle = {
		Journal of the American Statistical Association
	},
	title = {
		Better bootstrap confidence intervals
	},
	doi = {
		10.1080/01621459.1987.10478410
	},
	number = {
		397
	},
	pages = {
		171--185
	},
	volume = {
		82
	},
	publisher = {
		Informa {UK} Limited
	},
	file = {
		references/10.1080%2F01621459.1987.10478410.pdf
	},
	library = {},
	keywords = {
		resampling methods,
		approximate confidence intervals,
		transformations,
		nonparametric intervals,
		second-order theory,
		skewness corrections
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Bootstrap
	},
	abstract = {
		We consider the problem
		of setting approximate confidence intervals
		for a single parameter
		$
		\theta
		$ in a multiparameter family.
		The standard approximate intervals
		based on maximum likelihood theory,
		$
		\hat{
			\theta
		} 
		\pm 
		\hat{
			\sigma
		} 
		z^{
			\left( 
			\alpha 
			\right)
		}
		$,
		can be quite misleading.
		In practice,
		tricks based on transformations,
		bias corrections,
		and so forth,
		are often used to improve their accuracy.
		The bootstrap confidence intervals
		discussed in this article
		automatically incorporate such tricks
		without requiring the statistician
		to think them through for each new application,
		at the price of a considerable increase in computational effort.
		The new intervals incorporate an improvement
		over previously suggested methods,
		which results in second-order correctness
		in a wide variety of problems.
		In addition to parametric families,
		bootstrap intervals are also developed
		for nonparametric situations.
	},
}

@Article{
	Lib-Bootstrap-Schenker-1987
	,
	author = {
		Schenker, Nathaniel
	},
	date = {
		1987-03
	},
	journaltitle = {
		Journal of the American Statistical Association
	},
	title = {
		Better bootstrap confidence intervals: Comment
	},
	doi = {
		10.2307/2289150
	},
	number = {
		397
	},
	pages = {
		192
	},
	volume = {
		82
	},
	publisher = {
		{JSTOR}
	},
	file = {
		references/10.2307%2F2289150.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Bootstrap
	},
}

@Article{
	Lib-Bootstrap-Rasmussen-1987
	,
	author = {
		Rasmussen, Jeffrey L.
	},
	date = {
		1987
	},
	journaltitle = {
		Psychological Bulletin
	},
	title = {
		Estimating correlation coefficients:
		Bootstrap and parametric approaches.
	},
	doi = {
		10.1037/0033-2909.101.1.136
	},
	number = {
		1
	},
	pages = {
		136--139
	},
	volume = {
		101
	},
	publisher = {
		American Psychological Association ({APA})
	},
	file = {
		references/10.1037%252F0033-2909.101.1.136.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Bootstrap
	},
	abstract = {
		The bootstrap, 
		a computer-intensive approach to statistical data analysis, 
		has been recommended as an alternative to parametric approaches. 
		Advocates claim it is superior because it is not burdened 
		by potentially unwarranted normal theory assumptions 
		and because it retains information
		about the form of the original sample. 
		Empirical support for its superiority, 
		however, 
		is quite limited. 
		The present article compares the bootstrap 
		and parametric approaches to estimating confidence intervals 
		and Type I error rates of the correlation coefficient. 
		The parametric approach is superior to the bootstrap 
		under both assumption violation and nonviolation. 
		The bootstrap results in overly restricted confidence intervals 
		and overly liberal Type I error rates.
	},
}

@Article{
	Lib-Bootstrap-Efron-1988
	,
	author = {
		Efron, Bradley
	},
	date = {
		1988
	},
	journaltitle = {
		Psychological Bulletin
	},
	title = {
		Bootstrap confidence intervals: Good or bad?
	},
	doi = {
		10.1037/0033-2909.104.2.293
	},
	number = {
		2
	},
	pages = {
		293--296
	},
	volume = {
		104
	},
	publisher = {
		American Psychological Association ({APA})
	},
	file = {
		references/10.1037%252F0033-2909.104.2.293.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Bootstrap
	},
	abstract = {
		The bootstrap is a nonparametric technique
		for estimating standard errors
		and approximate confidence intervals.
		Rasmussen has used a simulation experiment
		to suggest that bootstrap confidence intervals
		perform very poorly
		in the estimation of a correlation coefficient.
		Part of Rasmussen's simulation is repeated.
		A careful look at the results
		shows the bootstrap intervals performing quite well.
		Some remarks are made concerning the virtues
		and defects of bootstrap intervals in general.
	},
}

@Article{
	Lib-Bootstrap-Andrews-2000
	,
	author = {
		Andrews, Donald W. K.
	},
	date = {
		2000-03
	},
	journaltitle = {
		Econometrica
	},
	title = {
		Inconsistency of the bootstrap
		when a parameter is on the boundary of the parameter space
	},
	doi = {
		10.1111/1468-0262.00114
	},
	number = {
		2
	},
	pages = {
		399--405
	},
	volume = {
		68
	},
	publisher = {
		The Econometric Society
	},
	file = {
		references/10.1111%2F1468-0262.00114.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Bootstrap
	},
}

@misc{
	Lib-Bootstrap-Hesterberg-2014
	,
	title = {
		What teachers should know about the bootstrap: Resampling in the undergraduate statistics curriculum
	}, 
	author = {
		Hesterberg, Tim C.
	},
	date = {
		2014
	},
	eprint = {
		1411.5279
	},
	archivePrefix = {
		arXiv
	},
	primaryClass = {
		stat.OT
	},
	url = {
		https://arxiv.org/abs/1411.5279
	},
	file = {
		references/1411.5279.pdf
	},
	library = {},
	keywords = {
		teaching,
		bootstrap,
		permutation test,
		randomization test
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Bootstrap
	},
	abstract = {
		I have three goals in this article:
		\begin{enumerate}
			\item To show the enormous potential of bootstrapping and permutation tests to help students understand statistical concepts including sampling distributions, standard errors, bias, confidence intervals, null distributions, and P-values.
			\item To dig deeper, understand why these methods work and when they don't, things to watch out for, and how to deal with these issues when teaching.
			\item To change statistical practice---by comparing these methods to common $t$ tests and intervals,
			      we see how inaccurate the latter are;
			      we confirm this with asymptotics.
			      $n \geq 30$ isn't enough---think $n \geq 5000$.
		\end{enumerate}
		Resampling provides diagnostics,
		and more accurate alternatives.
		Sadly,
		the common bootstrap percentile interval badly under-covers in small samples; 
		there are better alternatives.
		The tone is informal, with a few stories and jokes. 
	},
}

@Article{
	Lib-Bootstrap-Hesterberg-2015
	,
	author = {
		Hesterberg, Tim C.
	},
	date = {
		2015-10
	},
	journaltitle = {
		The American Statistician
	},
	title = {
		What teachers should know about the bootstrap: Resampling in the undergraduate statistics curriculum
	},
	doi = {
		10.1080/00031305.2015.1089789
	},
	number = {
		4
	},
	pages = {
		371--386
	},
	volume = {
		69
	},
	publisher = {
		Informa {UK} Limited
	},
	file = {
		references/10.1080%2F00031305.2015.1089789.pdf
	},
	library = {},
	keywords = {
		bias,
		confidence intervals,
		sampling distribution,
		standard error,
		statistical concepts,
		teaching
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Bootstrap
	},
	abstract = {
		Bootstrapping has enormous potential in statistics education and practice,
		but there are subtle issues and ways to go wrong.
		For example,
		the common combination of nonparametric bootstrapping and bootstrap percentile confidence intervals is less accurate than using $t$-intervals for small samples,
		though more accurate for larger samples.
		My goals in this article are to provide a deeper understanding of bootstrap methods--how they work,
		when they work or not,
		and which methods work better-and to highlight pedagogical issues.
		Supplementary materials for this article are available online.
	},
}

@BookInBook{
	Lib-Bootstrap-Efron-2016a
	,
	author    = {
		Efron, Bradley 
		and 
		Hastie, Trevor
	},
	booktitle     = {
		Computer age statistical inference
	},
	date      = {
		2016-07
	},
	title      = {
		The jackknife and the bootstrap
	},
	bookauthor = {
		Efron, Bradley and Hastie, Trevor
	},
	chapter    = {
		10
	},
	doi       = {
		10.1017/cbo9781316576533
	},
	isbn      = {
		9781316576533
	},
	publisher = {
		Cambridge University Press
	},
	file = {
		references/9781316576533_10.pdf
	},
	library = {
		QA276.4 .E376 2016
	},
	keywords = {
		Mathematical statistics--Data processing
	},
	addendum = {
		https://lccn.loc.gov/2016028353
	},
	note = {},
	annotation = {
		Lib-Bootstrap
	},
	abstract  = {
		The twenty-first century has seen a breathtaking expansion of statistical methodology, 
		both in scope and in influence. 
		'Big data', 
		'data science', 
		and 'machine learning' have become familiar terms in the news, 
		as statistical methods are brought to bear upon the enormous data sets of modern science and commerce. 
		How did we get here? 
		And where are we going? 
		This book takes us on an exhilarating journey through the revolution in data analysis following the introduction of electronic computation in the 1950s. 
		Beginning with classical inferential theories - 
		Bayesian, 
		frequentist, 
		Fisherian 
		- individual chapters take up a series of influential topics: 
		survival analysis, 
		logistic regression, 
		empirical Bayes, 
		the jackknife and bootstrap, 
		random forests, 
		neural networks, 
		Markov chain Monte Carlo, 
		inference after model selection, 
		and dozens more. 
		The distinctly modern approach integrates methodology and algorithms with statistical inference. 
		The book ends with speculation on the future direction of statistics and data science.
	},
}

@BookInBook{
	Lib-Bootstrap-Efron-2016b
	,
	author    = {
		Efron, Bradley 
		and 
		Hastie, Trevor
	},
	booktitle     = {
		Computer age statistical inference
	},
	date      = {
		2016-07
	},
	title      = {
		Bootstrap confidence intervals
	},
	bookauthor = {
		Efron, Bradley and Hastie, Trevor
	},
	chapter    = {
		11
	},
	doi       = {
		10.1017/cbo9781316576533
	},
	isbn      = {
		9781316576533
	},
	publisher = {
		Cambridge University Press
	},
	file = {
		references/9781316576533_11.pdf
	},
	library = {
		QA276.4 .E376 2016
	},
	keywords = {
		Mathematical statistics--Data processing
	},
	addendum = {
		https://lccn.loc.gov/2016028353
	},
	note = {},
	annotation = {
		Lib-Bootstrap
	},
	abstract  = {
		The twenty-first century has seen a breathtaking expansion of statistical methodology, 
		both in scope and in influence. 
		'Big data', 
		'data science', 
		and 'machine learning' have become familiar terms in the news, 
		as statistical methods are brought to bear upon the enormous data sets of modern science and commerce. 
		How did we get here? 
		And where are we going? 
		This book takes us on an exhilarating journey through the revolution in data analysis following the introduction of electronic computation in the 1950s. 
		Beginning with classical inferential theories - 
		Bayesian, 
		frequentist, 
		Fisherian 
		- individual chapters take up a series of influential topics: 
		survival analysis, 
		logistic regression, 
		empirical Bayes, 
		the jackknife and bootstrap, 
		random forests, 
		neural networks, 
		Markov chain Monte Carlo, 
		inference after model selection, 
		and dozens more. 
		The distinctly modern approach integrates methodology and algorithms with statistical inference. 
		The book ends with speculation on the future direction of statistics and data science.
	},
}

@Article{
	Lib-Bootstrap-Rousselet-2021
	,
	author = {
		Rousselet, Guillaume A.
		and
		Pernet, Cyril R. 
		and
		Wilcox, Rand R.
	},
	date = {
		2021-01
	},
	journaltitle = {
		Advances in Methods and Practices in Psychological Science
	},
	title = {
		The percentile bootstrap: A primer with step-by-step instructions in {R}
	},
	doi = {
		10.1177/2515245920911881
	},
	number = {
		1
	},
	pages = {
		1--10
	},
	volume = {
		4
	},
	publisher = {{SAGE} Publications},
	file = {
		references/10.1177%2F2515245920911881.pdf
	},
	library = {},
	keywords = {
		bootstrap,
		confidence interval,
		correlation,
		R,
		simulation,
		trimmed mean,
		median,
		reaction time,
		skewness,
		group comparison,
		open materials
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Bootstrap
	},
	abstract = {
		The percentile bootstrap is the Swiss Army knife of statistics:
		It is a nonparametric method based on data-driven simulations.
		It can be applied to many statistical problems,
		as a substitute to standard parametric approaches,
		or in situations for which parametric methods do not exist.
		In this Tutorial,
		we cover \texttt{R} code to implement the percentile bootstrap
		to make inferences about central tendency
		(e.g., means and trimmed means) and spread
		in a one-sample example
		and in an example comparing two independent groups.
		For each example,
		we explain how to derive a bootstrap distribution
		and how to get a confidence interval
		and a $p$ value from that distribution.
		We also demonstrate how to run a simulation
		to assess the behavior of the bootstrap.
		For some purposes,
		such as making inferences about the mean,
		the bootstrap performs poorly.
		But for other purposes,
		it is the only known method that works well over a broad range of situations.
		More broadly,
		combining the percentile bootstrap with robust estimators
		(i.e., estimators that are not overly sensitive to outliers)
		can help users gain a deeper understanding of their data
		than they would using conventional methods.
	},
}

@Article{
	Lib-Bootstrap-Efron-2012,
	author       = {
		Efron, Bradley
	},
	date         = {
		2012-12
	},
	journaltitle = {
		The Annals of Applied Statistics
	},
	title        = {
		Bayesian inference and the parametric bootstrap
	},
	doi          = {
		10.1214/12-aoas571
	},
	number       = {
		4
	},
	volume       = {
		6
	},
	publisher    = {
		Institute of Mathematical Statistics
	},
	file = {
		references/10.1214%2F12-aoas571.pdf
	},
	library = {},
	keywords = {
		deviance,
		exponential families,
		generalized linear models,
		Jeffreys prior
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Bootstrap
	},
	abstract     = {
		The parametric bootstrap can be used for the efficient computation of Bayes posterior distributions.
		Importance sampling formulas take on an easy form relating to the deviance in exponential families and are particularly simple starting from Jeffreys invariant prior.
		Because of the i.i.d. nature of bootstrap sampling, familiar formulas describe the computational accuracy of the Bayes estimates.
		Besides computational methods,
		the theory provides a connection between Bayesian and frequentist analysis.
		Efficient algorithms for the frequentist accuracy of Bayesian inferences are developed and demonstrated in a model selection example.
	},
}

@Comment{jabref-meta: databaseType:biblatex;}