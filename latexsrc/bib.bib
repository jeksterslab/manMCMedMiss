% Encoding: US-ASCII

@Book{
	Lib-Bootstrap-Books-Efron-1982
	,
	author = {
		Efron, Bradley
	},
	publisher = {
		Society for Industrial and Applied Mathematics
	},
	title = {
		The jackknife, the bootstrap and other resampling plans
	},
	series = {
		CBMS-NSF Regional conference series in applied mathematics ; 38
	},
	date = {
		1982
	},
	location = {
		Philadelphia, Pa.
	},
	doi = {
		10.1137/1.9781611970319
	},
	isbn = {
		9780898711790
	},
	file = {
		references/81084708.pdf
	},
	library = {
		QA276.8 .E375 1982
	},
	keywords = {
		Estimation theory,
		Error analysis (Mathematics),
		Jackknife (Statistics),
		Bootstrap (Statistics),
		Resampling (Statistics)
	},
	addendum = {
		https://lccn.loc.gov/81084708
	},
	note = {},
	annotation = {
		Lib-Bootstrap-Books
	},
	abstract = {
		The jackknife and the bootstrap are nonparametric methods
		for assessing the errors in a statistical estimation problem.
		They provide several advantages over the traditional parametric approach:
		the methods are easy to describe
		and they apply to arbitrarily complicated situations;
		distribution assumptions,
		such as normality,
		are never made.
														
		This monograph connects the jackknife,
		the bootstrap,
		and many other related ideas such as cross-validation,
		random subsampling,
		and balanced repeated replications into a unified exposition.
		The theoretical development is at an easy mathematical level
		and is supplemented by a large number of numerical examples.
														
		The methods described in this monograph form a useful set of tools
		for the applied statistician.
		They are particularly useful in problem areas
		where complicated data structures are common,
		for example,
		in censoring,
		missing data,
		and highly multivariate situations.
	}
}

@Book{
	Lib-Bootstrap-Books-Hall-1992
	,
	author = {
		Hall, Peter
	},
	publisher = {
		Springer-Verlag
	},
	title = {
		The bootstrap and {Edgeworth} expansion
	},
	series = {
		Springer series in statistics
	},
	date = {
		1992
	},
	location = {
		New York
	},
	doi = {
		10.1007/978-1-4612-4384-7
	},
	isbn = {
		978-1-4612-4384-7
	},
	file = {
		references/91034951.pdf
	},
	library = {
		QA276.8 .H34 1992
	},
	keywords = {
		Bootstrap (Statistics),
		Edgeworth expansions
	},
	addendum = {
		https://lccn.loc.gov/91034951
	},
	note = {},
	annotation = {
		Lib-Bootstrap-Books
	},
	abstract = {
		This monograph addresses two quite different topics, in the belief that each can shed light on the other.
		Firstly, it lays the foundation for a particular view of the bootstrap.
		Secondly, it gives an account of Edgeworth expansion.
		Chapter 1 is about the bootstrap, witih almost no mention of Edgeworth expansion;
		Chapter 2 is about Edgeworth expansion, with scarcely a word about the bootstrap;
		and Chapters 3 and 4 bring these two themes together, using Edgeworth expansion to explore and develop the properites of the bootstrap.
		The book is aimed a a graduate level audience who has some exposure to the methods of theoretical statistics.
		However, technical details are delayed until the last chapter (entitled ``Details of Mathematical Rigour''), and so a mathematically able reader without knowledge of the rigorous theory of probability will have no trouble understanding the first four-fifths of the book.
		The book simultaneously fills two gaps in the literature; it provides a very readable graduate level account of the theory of Edgeworth expansion, and it gives a detailed introduction to the theory of bootstrap methods.
	}
}

@Book{
	Lib-Bootstrap-Books-Efron-1993
	,
	author = {
		Efron, Bradley
		and
		Tibshirani, Robert J.
	},
	publisher = {
		Chapman \& Hall
	},
	title = {
		An introduction to the bootstrap
	},
	series = {
		Monographs on statistics and applied probability ; 57
	},
	date = {
		1993
	},
	location = {
		New York
	},
	doi = {
		10.1201/9780429246593 
	},
	isbn = {
		9780412042317
	},
	file = {
		references/93004489.pdf
	},
	library = {
		QA276.8 .E3745 1993
	},
	keywords = {
		Bootstrap (Statistics)
	},
	addendum = {
		https://lccn.loc.gov/93004489
	},
	note = {},
	annotation = {
		Lib-Bootstrap-Books
	},
	abstract = {
		Statistics is a subject of many uses
		and surprisingly few effective practitioners.
		The traditional road to statistical knowledge is blocked,
		for most,
		by a formidable wall of mathematics.
		The approach in An Introduction to the Bootstrap avoids that wall.
		It arms scientists and engineers,
		as well as statisticians,
		with the computational techniques they need to analyze
		and understand complicated data sets.
	}
}

@Book{
	Lib-Bootstrap-Books-Shao-1995
	,
	author = {
		Shao, Jun
		and
		Tu, Dongsheng
	},
	publisher = {
		Springer Verlag
	},
	title = {
		The jackknife and bootstrap
	},
	series = {
		Springer series in statistics
	},
	date = {
		1995
	},
	location = {
		New York, NY, USA
	},
	doi = {
		10.1007/978-1-4612-0795-5
	},
	isbn = {
		9781461207955
	},
	file = {
		references/95015074.pdf
	},
	library = {
		QA276.6 .S46 1995
	},
	keywords = {
		Jackknife (Statistics),
		Bootstrap (Statistics),
		Resampling (Statistics),
		Estimation theory
	},
	addendum = {
		https://lccn.loc.gov/95015074
	},
	note = {},
	annotation = {
		Lib-Bootstrap-Books
	},
	abstract = {
		The jackknife and bootstrap are the most popular data-resampling methods used in statistical analysis.
		The resampling methods replace theoretical derivations required in applying traditional methods (such as substitution and linearization) in statistical analysis by repeatedly resampling the original data and making inferences from the resamples.
		Because of the availability of inexpensive and fast computing, these computer-intensive methods have caught on very rapidly in recent years and are particularly appreciated by applied statisticians.
		The primary aims of this book are (1) to provide a systematic introduction to the theory of the jackknife, the bootstrap, and other resampling methods developed in the last twenty years;
		(2) to provide a guide for applied statisticians: practitioners often use (or misuse) the resampling methods in situations where no theoretical confirmation has been made;
		and (3) to stimulate the use of the jackknife and bootstrap and further developments of the resampling methods.
		The theoretical properties of the jackknife and bootstrap methods are studied in this book in an asymptotic framework.
		Theorems are illustrated by examples.
		Finite sample properties of the jackknife and bootstrap are mostly investigated by examples and/or empirical simulation studies.
		In addition to the theory for the jackknife and bootstrap methods in problems with independent and identically distributed (i.i.d.) data, we try to cover, as much as we can, the applications of the jackknife and bootstrap in various complicated non-i.i.d. data problems.
	}
}

@Book{
	Lib-Bootstrap-Books-Davison-1997
	,
	author = {
		Davison, Anthony Christopher
		and
		Hinkley, David Victor
	},
	publisher = {
		Cambridge University Press
	},
	title = {
		Bootstrap methods and their application
	},
	series = {
		Cambridge Series in Statistical and Probabilistic Mathematics
	},
	date = {
		1997
	},
	location = {
		Cambridge
		and
		New York, NY, USA
	},
	doi = {
		10.1017/CBO9780511802843
	},
	isbn = {
		9780521573917
	},
	file = {
		references/96030064-*.pdf
	},
	library = {
		QA276.8 .D38 1997
	},
	keywords = {
		Bootstrap (Statistics)
	},
	addendum = {
		https://lccn.loc.gov/96030064	
	},
	note = {},
	annotation = {
		Lib-Bootstrap-Books
	},
	abstract = {
		Bootstrap methods are computer-intensive methods of statistical analysis,
		which use simulation to calculate standard errors,
		confidence intervals,
		and significance tests.
		The methods apply for any level of modelling,
		and so can be used for fully parametric,
		semiparametric, and
		completely nonparametric analysis.
		This 1997 book gives a broad and up-to-date coverage of bootstrap methods,
		with numerous applied examples,
		developed in a coherent way with the necessary theoretical basis.
		Applications include stratified data;
		finite populations;
		censored and missing data;
		linear, nonlinear, and smooth regression models;
		classification;
		time series and spatial problems.
		Special features of the book include:
		extensive discussion of significance tests and confidence intervals;
		material on various diagnostic methods;
		and methods for efficient computation,
		including improved Monte Carlo simulation.
		Each chapter includes both practical and theoretical exercises.
		S-Plus programs for implementing the methods described in the text
		are available from the supporting website.
	}
}

@Book{
	Lib-Bootstrap-Books-Good-2005
	,
	author = {
		Good, Phillip I.
	},
	publisher = {
		Springer
	},
	title = {
		Permutation, parametric and bootstrap tests of hypotheses
	},
	series = {
		Springer series in statistics
	},
	date = {
		2005
	},
	location = {
		New York
	},
	doi = {
		10.1007/b138696
	},
	isbn = {
		9780387271583
	},
	file = {
		references/2004050436.pdf
	},
	library = {
		QA277 .G643 2005
	},
	keywords = {
		Statistical hypothesis testing,
		Resampling (Statistics)
	},
	addendum = {
		https://lccn.loc.gov/2004050436	
	},
	note = {},
	annotation = {
		Lib-Bootstrap-Books
	},
	abstract = {
		This text will equip both practitioners and theorists with the necessary background in testing hypothesis and decision theory to enable innumerable practical applications of statistics.
		Its intuitive and informal style makes it suitable as a text for both students and researchers.
		It can serve as the basis a one- or two-semester graduate course as well as a standard handbook of statistical procedures for the practitioners' desk.
										
		Parametric, permutation, and bootstrap procedures for testing hypotheses are developed side by side.
		The emphasis on distribution-free permutation procedures will enable workers in applied fields to use the most powerful statistic for their applications and satisfy regulatory agency demands for methods that yield exact significance levels, not approximations.
		Algebra and an understanding of discrete probability will take the reader through all but the appendix, which utilizes probability measures in its proofs.
										
		The revised and expanded text of the 3rd edition includes many more real-world illustrations from biology, business, clinical trials, economics, geology, law, medicine, social science and engineering along with twice the number of exercises.
		Real-world problems of missing and censored data, multiple comparisons, nonresponders, after-the-fact covariates, and outliers are dealt with at length.
		New sections are added on sequential analysis and multivariate analysis plus a chapter on the exact analysis of multi-factor designs based on the recently developed theory of synchronous permutations.
										
		The book's main features include:
										
		\begin{itemize}
			\item Detailed consideration of one-,
			      two-,
			      and k-sample tests,
			      contingency tables,
			      clinical trials,
			      cluster analysis,
			      multiple comparisons,
			      multivariate analysis,
			      and repeated measures
			\item Numerous practical applications in archeology,
			      biology,
			      business,
			      climatology,
			      clinical trials,
			      economics,
			      education,
			      engineering,
			      geology,
			      law,
			      medicine,
			      and the social sciences
			\item Valuable techniques for reducing computation time
			\item Practical advice on experimental design
			\item Sections on sequential analysis
			\item Comparisons among competing bootstrap,
			      parametric,
			      and permutation techniques.
		\end{itemize}
	}
}

@Book{
	Lib-Bootstrap-Books-Chernick-2008
	,
	author = {
		Chernick, Michael R.
	},
	publisher = {
		Wiley-Interscience
	},
	title = {
		Bootstrap methods: A guide for practitioners and researchers
	},
	edition = {
		2
	},
	series = {
		Wiley series in probability and statistics
	},
	date = {
		2008
	},
	location = {
		Hoboken, N.J.
	},
	doi = {
		10.1002/9780470192573
	},
	isbn = {
		9780471756217
	},
	file = {
		references/2007029309.pdf
	},
	library = {
		QA276.8 .C479 2008
	},
	keywords = {
		Bootstrap (Statistics)
	},
	addendum = {
		https://lccn.loc.gov/2007029309
	},
	note = {},
	annotation = {
		Lib-Bootstrap-Books
	},
	abstract = {
		A practical and accessible introduction to the bootstrap method--newly revised and updated.
														
		Over the past decade,
		the application of bootstrap methods
		to new areas of study has expanded,
		resulting in theoretical and applied advances across various fields.
		Bootstrap Methods, Second Edition is a highly approachable guide to the multidisciplinary,
		real-world uses of bootstrapping
		and is ideal for readers who have a professional interest in its methods,
		but are without an advanced background in mathematics.
														
		Updated to reflect current techniques
		and the most up-to-date work on the topic,
		the Second Edition features:
														
		\begin{itemize}
			\item The addition of a second,
			      extended bibliography devoted solely to publications from 1999-2007,
			      which is a valuable collection of references on the latest research in the field
			\item A discussion of the new areas of applicability for bootstrap methods,
			      including use in the pharmaceutical industry for estimating individual
			      and population bioequivalence in clinical trials
			\item A revised chapter on when and why bootstrap fails
			      and remedies for overcoming these drawbacks
			\item Added coverage on regression,
			      censored data applications,
			      P-value adjustment,
			      ratio estimators, and
			      missing data
			\item New examples and illustrations as well as extensive historical notes
			      at the end of each chapter
		\end{itemize}
														
		With a strong focus on application,
		detailed explanations of methodology,
		and complete coverage of modern developments in the field,
		Bootstrap Methods, Second Edition is an indispensable reference
		for applied statisticians, engineers, scientists, clinicians, and other practitioners
		who regularly use statistical methods in research.
		It is also suitable as a supplementary text for courses in statistics
		and resampling methods at the upper-undergraduate and graduate levels.  
	}
}

@Book{
	Lib-Bootstrap-Books-Godfrey-2009
	,
	author = {
		Godfrey, Leslie
	},
	publisher = {
		Palgrave Macmillan
	},
	title = {
		Bootstrap tests for regression models
	},
	series = {
		Palgrave texts in econometrics
	},
	date = {
		2009
	},
	location = {
		Basingstoke, Hampshire
		and
		New York, NY
	},
	doi = {
		10.1057/9780230233737
	},
	isbn = {
		9780230202306
	},
	file = {
		references/2009504055.pdf
	},
	library = {
		HB141 .G63 2009
	},
	keywords = {
		Econometric models,
		Regression analysis,
		Bootstrap (Statistics)
	},
	addendum = {
		https://lccn.loc.gov/2009504055	
	},
	note = {},
	annotation = {
		Lib-Bootstrap-Books
	},
	abstract = {
		An accessible discussion examining computationally-intensive techniques
		and bootstrap methods,
		providing ways to improve the finite-sample performance
		of well-known asymptotic tests for regression models.
		This book uses the linear regression model
		as a framework for introducing simulation-based tests
		to help perform econometric analyses.
	}
}

@Book{
	Lib-Bootstrap-Books-Chernick-2011
	,
	author = {
		Chernick, Michael R. 
		and 
		LaBudde, Robert A.
	},
	publisher = {
		Wiley
	},
	title = {
		An introduction to bootstrap methods with applications to {R}
	},
	series = {},
	date = {
		2011
	},
	location = {
		Hoboken, N.J.
	},
	doi = {},
	isbn = {
		9781118625415
	},
	file = {
		references/2011010972.pdf
	},
	library = {
		QA276.8 .C478 2011
	},
	keywords = {
		Bootstrap (Statistics),
		R (Computer program language)
	},
	addendum = {
		https://lccn.loc.gov/2011010972	
	},
	note = {},
	annotation = {
		Lib-Bootstrap-Books
	},
	abstract = {
		Bootstrap methods provide a powerful approach to statistical data analysis,
		as they have more general applications than standard parametric methods.
		An Introduction to Bootstrap Methods with Applications to R
		explores the practicality of this approach and successfully utilizes \texttt{R} to illustrate applications
		for the bootstrap and other resampling methods.
		This book provides a modern introduction to bootstrap methods
		for readers who do not have an extensive background in advanced mathematics.
		Emphasis throughout is on the use of bootstrap methods as an exploratory tool,
		including its value in variable selection and other modeling environments.
														
		The authors begin with a description of bootstrap methods and its relationship to other resampling methods,
		along with an overview of the wide variety of applications of the approach.
		Subsequent chapters offer coverage of improved confidence set estimation,
		estimation of error rates in discriminant analysis,
		and applications to a wide variety of hypothesis testing and estimation problems,
		including pharmaceutical, genomics, and economics.
		To inform readers on the limitations of the method,
		the book also exhibits counterexamples to the consistency of bootstrap methods.
														
		An introduction to \texttt{R} programming provides the needed preparation to work
		with the numerous exercises and applications presented throughout the book.
		A related website houses the book's \texttt{R} subroutines,
		and an extensive listing of references provides resources for further study.
														
		Discussing the topic at a remarkably practical and accessible level,
		An Introduction to Bootstrap Methods with Applications to \texttt{R} is an excellent book
		for introductory courses on bootstrap and resampling methods at the upper-undergraduate and graduate levels.
		It also serves as an insightful reference for practitioners working with data in engineering,
		medicine,
		and the social sciences who would like to acquire a basic understanding of bootstrap methods.
	}
}

@Comment{jabref-meta: databaseType:bibtex;}
% Encoding: US-ASCII

@Article{
	Lib-Bootstrap-Efron-1979a
	,
	author = {
		Efron, Bradley
	},
	date = {
		1979-01
	},
	journaltitle = {
		The Annals of Statistics
	},
	title = {
		Bootstrap methods: 
		Another look at the jackknife
	},
	doi = {
		10.1214/aos/1176344552
	},
	number = {
		1
	},
	volume = {
		7
	},
	publisher = {
		Institute of Mathematical Statistics
	},
	file = {
		references/10.1214%2Faos%2F1176344552.pdf
	},
	library = {},
	keywords = {
		bootstrap,
		discriminant analysis,
		error rate estimation,
		jackknife,
		nonlinear regression,
		nonparametric variance estimation,
		resampling,
		subsample values
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Bootstrap
	},
	abstract = {
		We discuss the following problem:
		given a random sample
		$
		\mathbf{X}
		=
		\left(
		X_1 ,
		X_2 ,
		\dots ,
		X_n
		\right)
		$
		from an unknown probability distribution
		$
		F
		$,
		estimate the sampling distribution
		of some prespecified random variable
		$
		R
		\left(
		\mathbf{X},
		F
		\right)
		$,
		on the basis of the observed data
		$
		\mathbf{x}
		$.
		(Standard jackknife theory
		gives an approximate mean and variance
		in the case
		$
		R
		\left(
		\mathbf{X},
		F
		\right)
		=
		\theta
		\left(
		\hat{F}
		\right)
		-
		\theta
		\left(
		F
		\right)
		$,
		$
		\theta
		$ some parameter of interest.)
		A general method,
		called the ``bootstrap''
		is introduced,
		and shown to work satisfactorily
		on a variety of estimation problems.
		The jackknife is shown to be a linear approximation method
		for the bootstrap.
		The exposition proceeds by a series of examples:
		variance of the sample median,
		error rates in a linear discriminant analysis,
		ratio estimation,
		estimating regression parameters, etc.
	},
}

@Article{
	Lib-Bootstrap-Efron-1979b
	,
	author       = {
		Efron, Bradley
	},
	date         = {
		1979-10
	},
	journaltitle = {
		{SIAM} Review
	},
	title        = {
		Computers and the Theory of Statistics: Thinking the Unthinkable
	},
	doi          = {
		10.1137/1021092
	},
	number       = {
		4
	},
	pages        = {
		460--480
	},
	volume       = {
		21
	},
	publisher    = {
		Society for Industrial {\&} Applied Mathematics ({SIAM})
	},
	file = {
		references/10.1137%2F1021092.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Bootstrap
	},
	abstract     = {
		This is a survey article concerning recent advances in certain areas of statistical theory,
		written for a mathematical audience with no background in statistics.
		The topics are chosen to illustrate a special point:
		how the advent of the high-speed computer has affected the development of statistical theory.
		The topics discussed include nonparametric methods,
		the jackknife,
		the bootstrap,
		cross-validation,
		error-rate estimation in discriminant analysis,
		robust estimation,
		the influence function,
		censored data,
		the EM algorithm,
		and Cox's likelihood function.
		The exposition is mainly by example, with only a little offered in the way of theoretical development.
	},
}

@Article{
	Lib-Bootstrap-Efron-1981a
	,
	author       = {
		Efron, Bradley
	},
	date         = {
		1981
	},
	journaltitle = {
		Canadian Journal of Statistics / La Revue Canadienne de Statistique 
	},
	title        = {
		Nonparametric standard errors and confidence intervals
	},
	doi          = {
		10.2307/3314608
	},
	number       = {
		2
	},
	pages        = {
		139--158
	},
	volume       = {
		9
	},
	publisher    = {Wiley},
	file = {
		references/10.2307%252F3314608.pdf
	},
	library = {},
	keywords = {
		bootstrap,
		jackknife,
		delta method,
		nonparametric confidence intervals,
		nonparametric standard errors
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Bootstrap
	},
	abstract     = {
		We investigate several nonparametric methods; 
		the bootstrap, 
		the jackknife, 
		the delta method, 
		and other related techniques. 
		The first and simplest goal is the assignment of nonparametric standard errors 
		to a real-valued statistic. 
		More ambitiously, 
		we consider setting nonparametric confidence intervals 
		for a real-valued parameter. 
		Building on the well understood case of confidence intervals 
		for the median, 
		some hopeful evidence is presented that such a theory may be possible.
	},
}

@Article{
	Lib-Bootstrap-Barnard-1981,
	author       = {
		Barnard, George A.
		and
		Collins, J. R.
		and
		Farewell, V. T.
		and
		Field, C. A.
		and
		Kalbfleisch, J. D.
		and
		Nash, Stanley W.
		and
		Parzen, Emanuel
		and
		Prentice, Ross L.
		and
		Reid, Nancy
		and
		Sprott, D. A.
		and
		Switzer, Paul
		and
		Warren, W. G.
		and
		Weldon, K. L.
	},
	date         = {1981},
	journaltitle = {The Canadian Journal of Statistics / La Revue Canadienne de Statistique},
	title        = {Nonparametric Standard Errors and Confidence Intervals: Discussion},
	doi          = {10.2307/3314609},
	number       = {2},
	pages        = {158--170},
	volume       = {9},
	publisher    = {Wiley},
	file = {
		references/10.2307%2F3314609.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Bootstrap
	},
	abstract = {},
}

@Article{
	Lib-Bootstrap-Efron-1981b,
	author       = {Efron, Bradley},
	date         = {1981},
	journaltitle = {The Canadian Journal of Statistics / La Revue Canadienne de Statistique},
	title        = {Nonparametric Standard Errors and Confidence Intervals: Rejoinder},
	doi          = {10.2307/3314610},
	number       = {2},
	pages        = {170--172},
	volume       = {9},
	publisher    = {Wiley},
	file = {
		references/10.2307%2F3314610.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Bootstrap
	},
	abstract = {},
}


@Article{
	Lib-Bootstrap-Efron-1987
	,
	author = {
		Efron, Bradley
	},
	date = {
		1987-03
	},
	journaltitle = {
		Journal of the American Statistical Association
	},
	title = {
		Better bootstrap confidence intervals
	},
	doi = {
		10.1080/01621459.1987.10478410
	},
	number = {
		397
	},
	pages = {
		171--185
	},
	volume = {
		82
	},
	publisher = {
		Informa {UK} Limited
	},
	file = {
		references/10.1080%2F01621459.1987.10478410.pdf
	},
	library = {},
	keywords = {
		resampling methods,
		approximate confidence intervals,
		transformations,
		nonparametric intervals,
		second-order theory,
		skewness corrections
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Bootstrap
	},
	abstract = {
		We consider the problem
		of setting approximate confidence intervals
		for a single parameter
		$
		\theta
		$ in a multiparameter family.
		The standard approximate intervals
		based on maximum likelihood theory,
		$
		\hat{
			\theta
		} 
		\pm 
		\hat{
			\sigma
		} 
		z^{
			\left( 
			\alpha 
			\right)
		}
		$,
		can be quite misleading.
		In practice,
		tricks based on transformations,
		bias corrections,
		and so forth,
		are often used to improve their accuracy.
		The bootstrap confidence intervals
		discussed in this article
		automatically incorporate such tricks
		without requiring the statistician
		to think them through for each new application,
		at the price of a considerable increase in computational effort.
		The new intervals incorporate an improvement
		over previously suggested methods,
		which results in second-order correctness
		in a wide variety of problems.
		In addition to parametric families,
		bootstrap intervals are also developed
		for nonparametric situations.
	},
}

@Article{
	Lib-Bootstrap-Schenker-1987
	,
	author = {
		Schenker, Nathaniel
	},
	date = {
		1987-03
	},
	journaltitle = {
		Journal of the American Statistical Association
	},
	title = {
		Better bootstrap confidence intervals: Comment
	},
	doi = {
		10.2307/2289150
	},
	number = {
		397
	},
	pages = {
		192
	},
	volume = {
		82
	},
	publisher = {
		{JSTOR}
	},
	file = {
		references/10.2307%2F2289150.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Bootstrap
	},
}

@Article{
	Lib-Bootstrap-Rasmussen-1987
	,
	author = {
		Rasmussen, Jeffrey L.
	},
	date = {
		1987
	},
	journaltitle = {
		Psychological Bulletin
	},
	title = {
		Estimating correlation coefficients:
		Bootstrap and parametric approaches.
	},
	doi = {
		10.1037/0033-2909.101.1.136
	},
	number = {
		1
	},
	pages = {
		136--139
	},
	volume = {
		101
	},
	publisher = {
		American Psychological Association ({APA})
	},
	file = {
		references/10.1037%252F0033-2909.101.1.136.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Bootstrap
	},
	abstract = {
		The bootstrap, 
		a computer-intensive approach to statistical data analysis, 
		has been recommended as an alternative to parametric approaches. 
		Advocates claim it is superior because it is not burdened 
		by potentially unwarranted normal theory assumptions 
		and because it retains information
		about the form of the original sample. 
		Empirical support for its superiority, 
		however, 
		is quite limited. 
		The present article compares the bootstrap 
		and parametric approaches to estimating confidence intervals 
		and Type I error rates of the correlation coefficient. 
		The parametric approach is superior to the bootstrap 
		under both assumption violation and nonviolation. 
		The bootstrap results in overly restricted confidence intervals 
		and overly liberal Type I error rates.
	},
}

@Article{
	Lib-Bootstrap-Efron-1988
	,
	author = {
		Efron, Bradley
	},
	date = {
		1988
	},
	journaltitle = {
		Psychological Bulletin
	},
	title = {
		Bootstrap confidence intervals: Good or bad?
	},
	doi = {
		10.1037/0033-2909.104.2.293
	},
	number = {
		2
	},
	pages = {
		293--296
	},
	volume = {
		104
	},
	publisher = {
		American Psychological Association ({APA})
	},
	file = {
		references/10.1037%252F0033-2909.104.2.293.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Bootstrap
	},
	abstract = {
		The bootstrap is a nonparametric technique
		for estimating standard errors
		and approximate confidence intervals.
		Rasmussen has used a simulation experiment
		to suggest that bootstrap confidence intervals
		perform very poorly
		in the estimation of a correlation coefficient.
		Part of Rasmussen's simulation is repeated.
		A careful look at the results
		shows the bootstrap intervals performing quite well.
		Some remarks are made concerning the virtues
		and defects of bootstrap intervals in general.
	},
}

@Article{
	Lib-Bootstrap-Andrews-2000
	,
	author = {
		Andrews, Donald W. K.
	},
	date = {
		2000-03
	},
	journaltitle = {
		Econometrica
	},
	title = {
		Inconsistency of the bootstrap
		when a parameter is on the boundary of the parameter space
	},
	doi = {
		10.1111/1468-0262.00114
	},
	number = {
		2
	},
	pages = {
		399--405
	},
	volume = {
		68
	},
	publisher = {
		The Econometric Society
	},
	file = {
		references/10.1111%2F1468-0262.00114.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Bootstrap
	},
}

@misc{
	Lib-Bootstrap-Hesterberg-2014
	,
	title = {
		What teachers should know about the bootstrap: Resampling in the undergraduate statistics curriculum
	}, 
	author = {
		Hesterberg, Tim C.
	},
	date = {
		2014
	},
	eprint = {
		1411.5279
	},
	archivePrefix = {
		arXiv
	},
	primaryClass = {
		stat.OT
	},
	url = {
		https://arxiv.org/abs/1411.5279
	},
	file = {
		references/1411.5279.pdf
	},
	library = {},
	keywords = {
		teaching,
		bootstrap,
		permutation test,
		randomization test
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Bootstrap
	},
	abstract = {
		I have three goals in this article:
		\begin{enumerate}
			\item To show the enormous potential of bootstrapping and permutation tests to help students understand statistical concepts including sampling distributions, standard errors, bias, confidence intervals, null distributions, and P-values.
			\item To dig deeper, understand why these methods work and when they don't, things to watch out for, and how to deal with these issues when teaching.
			\item To change statistical practice---by comparing these methods to common $t$ tests and intervals,
			      we see how inaccurate the latter are;
			      we confirm this with asymptotics.
			      $n \geq 30$ isn't enough---think $n \geq 5000$.
		\end{enumerate}
		Resampling provides diagnostics,
		and more accurate alternatives.
		Sadly,
		the common bootstrap percentile interval badly under-covers in small samples; 
		there are better alternatives.
		The tone is informal, with a few stories and jokes. 
	},
}

@Article{
	Lib-Bootstrap-Hesterberg-2015
	,
	author = {
		Hesterberg, Tim C.
	},
	date = {
		2015-10
	},
	journaltitle = {
		The American Statistician
	},
	title = {
		What teachers should know about the bootstrap: Resampling in the undergraduate statistics curriculum
	},
	doi = {
		10.1080/00031305.2015.1089789
	},
	number = {
		4
	},
	pages = {
		371--386
	},
	volume = {
		69
	},
	publisher = {
		Informa {UK} Limited
	},
	file = {
		references/10.1080%2F00031305.2015.1089789.pdf
	},
	library = {},
	keywords = {
		bias,
		confidence intervals,
		sampling distribution,
		standard error,
		statistical concepts,
		teaching
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Bootstrap
	},
	abstract = {
		Bootstrapping has enormous potential in statistics education and practice,
		but there are subtle issues and ways to go wrong.
		For example,
		the common combination of nonparametric bootstrapping and bootstrap percentile confidence intervals is less accurate than using $t$-intervals for small samples,
		though more accurate for larger samples.
		My goals in this article are to provide a deeper understanding of bootstrap methods--how they work,
		when they work or not,
		and which methods work better-and to highlight pedagogical issues.
		Supplementary materials for this article are available online.
	},
}

@BookInBook{
	Lib-Bootstrap-Efron-2016a
	,
	author    = {
		Efron, Bradley 
		and 
		Hastie, Trevor
	},
	booktitle     = {
		Computer age statistical inference
	},
	date      = {
		2016-07
	},
	title      = {
		The jackknife and the bootstrap
	},
	bookauthor = {
		Efron, Bradley and Hastie, Trevor
	},
	chapter    = {
		10
	},
	doi       = {
		10.1017/cbo9781316576533
	},
	isbn      = {
		9781316576533
	},
	publisher = {
		Cambridge University Press
	},
	file = {
		references/9781316576533_10.pdf
	},
	library = {
		QA276.4 .E376 2016
	},
	keywords = {
		Mathematical statistics--Data processing
	},
	addendum = {
		https://lccn.loc.gov/2016028353
	},
	note = {},
	annotation = {
		Lib-Bootstrap
	},
	abstract  = {
		The twenty-first century has seen a breathtaking expansion of statistical methodology, 
		both in scope and in influence. 
		'Big data', 
		'data science', 
		and 'machine learning' have become familiar terms in the news, 
		as statistical methods are brought to bear upon the enormous data sets of modern science and commerce. 
		How did we get here? 
		And where are we going? 
		This book takes us on an exhilarating journey through the revolution in data analysis following the introduction of electronic computation in the 1950s. 
		Beginning with classical inferential theories - 
		Bayesian, 
		frequentist, 
		Fisherian 
		- individual chapters take up a series of influential topics: 
		survival analysis, 
		logistic regression, 
		empirical Bayes, 
		the jackknife and bootstrap, 
		random forests, 
		neural networks, 
		Markov chain Monte Carlo, 
		inference after model selection, 
		and dozens more. 
		The distinctly modern approach integrates methodology and algorithms with statistical inference. 
		The book ends with speculation on the future direction of statistics and data science.
	},
}

@BookInBook{
	Lib-Bootstrap-Efron-2016b
	,
	author    = {
		Efron, Bradley 
		and 
		Hastie, Trevor
	},
	booktitle     = {
		Computer age statistical inference
	},
	date      = {
		2016-07
	},
	title      = {
		Bootstrap confidence intervals
	},
	bookauthor = {
		Efron, Bradley and Hastie, Trevor
	},
	chapter    = {
		11
	},
	doi       = {
		10.1017/cbo9781316576533
	},
	isbn      = {
		9781316576533
	},
	publisher = {
		Cambridge University Press
	},
	file = {
		references/9781316576533_11.pdf
	},
	library = {
		QA276.4 .E376 2016
	},
	keywords = {
		Mathematical statistics--Data processing
	},
	addendum = {
		https://lccn.loc.gov/2016028353
	},
	note = {},
	annotation = {
		Lib-Bootstrap
	},
	abstract  = {
		The twenty-first century has seen a breathtaking expansion of statistical methodology, 
		both in scope and in influence. 
		'Big data', 
		'data science', 
		and 'machine learning' have become familiar terms in the news, 
		as statistical methods are brought to bear upon the enormous data sets of modern science and commerce. 
		How did we get here? 
		And where are we going? 
		This book takes us on an exhilarating journey through the revolution in data analysis following the introduction of electronic computation in the 1950s. 
		Beginning with classical inferential theories - 
		Bayesian, 
		frequentist, 
		Fisherian 
		- individual chapters take up a series of influential topics: 
		survival analysis, 
		logistic regression, 
		empirical Bayes, 
		the jackknife and bootstrap, 
		random forests, 
		neural networks, 
		Markov chain Monte Carlo, 
		inference after model selection, 
		and dozens more. 
		The distinctly modern approach integrates methodology and algorithms with statistical inference. 
		The book ends with speculation on the future direction of statistics and data science.
	},
}

@Article{
	Lib-Bootstrap-Rousselet-2021
	,
	author = {
		Rousselet, Guillaume A.
		and
		Pernet, Cyril R. 
		and
		Wilcox, Rand R.
	},
	date = {
		2021-01
	},
	journaltitle = {
		Advances in Methods and Practices in Psychological Science
	},
	title = {
		The percentile bootstrap: A primer with step-by-step instructions in {R}
	},
	doi = {
		10.1177/2515245920911881
	},
	number = {
		1
	},
	pages = {
		1--10
	},
	volume = {
		4
	},
	publisher = {{SAGE} Publications},
	file = {
		references/10.1177%2F2515245920911881.pdf
	},
	library = {},
	keywords = {
		bootstrap,
		confidence interval,
		correlation,
		R,
		simulation,
		trimmed mean,
		median,
		reaction time,
		skewness,
		group comparison,
		open materials
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Bootstrap
	},
	abstract = {
		The percentile bootstrap is the Swiss Army knife of statistics:
		It is a nonparametric method based on data-driven simulations.
		It can be applied to many statistical problems,
		as a substitute to standard parametric approaches,
		or in situations for which parametric methods do not exist.
		In this Tutorial,
		we cover \texttt{R} code to implement the percentile bootstrap
		to make inferences about central tendency
		(e.g., means and trimmed means) and spread
		in a one-sample example
		and in an example comparing two independent groups.
		For each example,
		we explain how to derive a bootstrap distribution
		and how to get a confidence interval
		and a $p$ value from that distribution.
		We also demonstrate how to run a simulation
		to assess the behavior of the bootstrap.
		For some purposes,
		such as making inferences about the mean,
		the bootstrap performs poorly.
		But for other purposes,
		it is the only known method that works well over a broad range of situations.
		More broadly,
		combining the percentile bootstrap with robust estimators
		(i.e., estimators that are not overly sensitive to outliers)
		can help users gain a deeper understanding of their data
		than they would using conventional methods.
	},
}

@Article{
	Lib-Bootstrap-Efron-2012,
	author       = {
		Efron, Bradley
	},
	date         = {
		2012-12
	},
	journaltitle = {
		The Annals of Applied Statistics
	},
	title        = {
		Bayesian inference and the parametric bootstrap
	},
	doi          = {
		10.1214/12-aoas571
	},
	number       = {
		4
	},
	volume       = {
		6
	},
	publisher    = {
		Institute of Mathematical Statistics
	},
	file = {
		references/10.1214%2F12-aoas571.pdf
	},
	library = {},
	keywords = {
		deviance,
		exponential families,
		generalized linear models,
		Jeffreys prior
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Bootstrap
	},
	abstract     = {
		The parametric bootstrap can be used for the efficient computation of Bayes posterior distributions.
		Importance sampling formulas take on an easy form relating to the deviance in exponential families and are particularly simple starting from Jeffreys invariant prior.
		Because of the i.i.d. nature of bootstrap sampling, familiar formulas describe the computational accuracy of the Bayes estimates.
		Besides computational methods,
		the theory provides a connection between Bayesian and frequentist analysis.
		Efficient algorithms for the frequentist accuracy of Bayesian inferences are developed and demonstrated in a model selection example.
	},
}

@Comment{jabref-meta: databaseType:biblatex;}
% Encoding: US-ASCII

@Book{
	Lib-Confidence-Intervals-Profile-Likelihood-Pawitan-2013
	,
	author    = {
		Pawitan, Yudi
	},
	date      = {
		2013-01-17
	},
	title     = {
		In all likelihood: Statistical modelling and inference using likelihood
	},
	isbn      = {
		9780199671229
	},
	pagetotal = {
		544
	},
	publisher = {
		Oxford University Press
	},
	annotation = {
		Lib-Confidence-Intervals-Profile-Likelihood
	},
}

@Article{
	Lib-Confidence-Intervals-Profile-Likelihood-Venzon-1988
	,
	author       = {
		Venzon, D. J. 
		and 
		Moolgavkar, S. H.
	},
	date         = {
		1988
	},
	journaltitle = {
		Applied Statistics
	},
	title        = {
		A method for computing profile-likelihood-based confidence intervals
	},
	doi          = {
		10.2307/2347496
	},
	number       = {
		1
	},
	pages        = {
		87
	},
	volume       = {
		37
	},
	publisher    = {
		{JSTOR}
	},
	file = {
		references/10.2307%2F2347496.pdf
	},
	library = {},
	keywords = {
		confidence intervals,
		profile likelihood
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Confidence-Intervals-Profile-Likelihood
	},
	abstract = {
		The method of constructing confidence regions based on the generalised likelihood ratio statistic is well known for parameter vectors. 
		A similar construction of a confidence interval for a single entry of a vector can be implemented by repeatedly maximising over the other parameters. 
		We present an algorithm for finding these confidence interval endpoints that requires less computation. 
		It employs a modified Newton-Raphson iteration to solve a system of equations that defines the endpoints.
	},
}

@Comment{jabref-meta: databaseType:biblatex;}
% Encoding: US-ASCII

@Book{
	Lib-Mediation-Books-MacKinnon-2008
	,
	author    = {
		MacKinnon, David Peter
	},
	series    = {
		Multivariate applications
	},
	date      = {
		2008
	},
	title     = {
		Introduction to statistical mediation analysis
	},
	doi = {
		10.4324/9780203809556
	},
	isbn      = {
		9780805864298
	},
	location  = {
		Hoboken
	},
	pages     = {
		488
	},
	publisher = {
		Erlbaum Psych Press
	},
	file = {
		references/9780805864298.pdf
	},
	library = {
		QA278.2 .M29 2008
	},
	keywords = {
		Mediation (Statistics)
	},
	addendum = {
		https://lccn.loc.gov/2007011793
	},
	note = {},
	annotation = {
		Lib-Mediation-Books
	},
	abstract = {
		This volume introduces the statistical, 
		methodological, 
		and conceptual aspects of mediation analysis. 
		Applications from health, 
		social, 
		and developmental psychology, 
		sociology, 
		communication, 
		exercise science, 
		and epidemiology are emphasized throughout. 
		Single-mediator, 
		multilevel, 
		and longitudinal models are reviewed. 
		The author's goal is to help the reader apply mediation analysis to their own data and understand its limitations.
						
		Each chapter features an overview, 
		numerous worked examples, 
		a summary,
		and exercises (with answers to the odd numbered questions). 
		The accompanying downloadable resources contain outputs described in the book from SAS, 
		SPSS, 
		LISREL, 
		EQS, 
		MPLUS, 
		and CALIS, 
		and a program to simulate the model. 
		The notation used is consistent with existing literature on mediation in psychology.
						
		The book opens with a review of the types of research questions the mediation model addresses. 
		Part II describes the estimation of mediation effects including assumptions, 
		statistical tests, 
		and the construction of confidence limits. 
		Advanced models including mediation in path analysis, 
		longitudinal models, 
		multilevel data, 
		categorical variables, 
		and mediation in the context of moderation are then described. 
		The book closes with a discussion of the limits of mediation analysis, 
		additional approaches to identifying mediating variables, 
		and future directions.
						
		Introduction to Statistical Mediation Analysis is intended for researchers and advanced students in health, 
		social, 
		clinical, 
		and developmental psychology as well as communication, 
		public health, 
		nursing, 
		epidemiology, 
		and sociology. 
		Some exposure to a graduate level research methods or statistics course is assumed. 
		The overview of mediation analysis and the guidelines for conducting a mediation analysis will be appreciated by all readers.
	}
}

@Book{Lib-Mediation-Books-Jose-2013,
	author    = {
		Jose, Paul E.
	},
	date      = {
		2013
	},
	title     = {
		Doing statistical mediation and moderation
	},
	isbn      = {
		9781462508150
	},
	pages     = {
		336
	},
	publisher = {
		Guilford Publications
	},
	file = {
		references/9781462508150.pdf
	},
	library = {
		QA278.2 .J67 2013
	},
	keywords = {
		Mediation (Statistics),
		Social sciences--Statistical methods
	},
	addendum = {
		https://lccn.loc.gov/2012047804
	},
	note = {},
	annotation = {
		Lib-Mediation-Books
	},
	abstract = {
		Written in a friendly, 
		conversational style, 
		this book offers a hands-on approach to statistical mediation and moderation for both beginning researchers and those familiar with modeling. 
		Starting with a gentle review of regression-based analysis,
		Paul Jose covers basic mediation and moderation techniques before moving on to advanced topics in multilevel modeling, 
		structural equation modeling, 
		and hybrid combinations, 
		such as moderated mediation. 
		User-friendly features include numerous graphs and carefully worked-through examples; 
		``Helpful Suggestions'' about procedures and pitfalls; 
		``Knowledge Boxes'' delving into special topics, 
		such as dummy coding; 
		and end-of-chapter exercises and problems (with answers). 
		The companion website provides downloadable sample data sets that are used in the book to demonstrate particular analytic strategies, 
		and explains how researchers and students can execute analyses using Jose's online programs, 
		MedGraph and ModGraph. 
		Appendices present SPSS, 
		AMOS, 
		and Mplus syntax for conducting the key types of analyses.
	},
}

@Book{
	Lib-Mediation-Books-Hayes-2013,
	author    = {
		Hayes, Andrew F.
	},
	date      = {
		2013
	},
	title     = {
		Introduction to mediation, moderation, and conditional process analysis: A regression-based approach
	},
	isbn      = {
		9781609182304
	},
	pages     = {
		507
	},
	publisher = {
		Guilford Publications
	},
	file = {
		references/9781609182304.pdf
	},
	library = {
		HA31.3 .H39 2013
	},
	keywords = {
		Social sciences--Statistical methods,
		Mediation (Statistics),
		Regression analysis
	},
	addendum = {
		https://lccn.loc.gov/2013008807
	},
	note = {},
	annotation = {
		Lib-Mediation-Books
	},
}

@Book{
	Lib-Mediation-Books-Hayes-2018
	,
	author    = {
		Hayes, Andrew F.
	},
	date      = {
		2018
	},
	title     = {
		Introduction to mediation, moderation, and conditional process analysis: A regression-based approach
	},
	series = {
		Methodology in the social sciences
	},
	edition   = {
		2
	},
	isbn      = {
		9781462534654
	},
	pages     = {
		692
	},
	publisher = {
		Guilford Publications
	},
	file = {
		references/9781462534654.pdf
	},
	library = {
		HA31.3 .H39 2018
	},
	keywords = {
		Social sciences--Statistical methods,
		Mediation (Statistics),
		Regression analysis
	},
	addendum = {
		https://lccn.loc.gov/2017039263
	},
	note = {},
	annotation = {
		Lib-Mediation-Books
	},
}

@Book{
	Lib-Mediation-Books-Hayes-2022
	,
	author    = {
		Hayes, Andrew F.
	},
	date      = {
		2022
	},
	title     = {
		Introduction to mediation, moderation, and conditional process analysis: A regression-based approach
	},
	series = {
		Methodology in the social sciences
	},
	edition   = {
		3
	},
	isbn      = {
		9781462549030
	},
	pages     = {
		732
	},
	publisher = {
		Guilford Publications
	},
	file = {},
	library = {
		HA31.3 .H39 2022
	},
	keywords = {
		Social sciences--Statistical methods,
		Mediation (Statistics),
		Regression analysis
	},
	addendum = {
		https://lccn.loc.gov/2021031108
	},
	note = {},
	annotation = {
		Lib-Mediation-Books
	},
	abstract = {
		Lauded for its easy-to-understand,
		conversational discussion of the fundamentals of mediation, 
		moderation, 
		and conditional process analysis, 
		this book has been fully revised with 50\% new content,
		including sections on working with multicategorical antecedent variables, 
		the use of PROCESS version 3 for SPSS and SAS for model estimation, 
		and annotated PROCESS v3 outputs. 
		Using the principles of ordinary least squares regression, 
		Andrew F. Hayes carefully explains procedures for testing hypotheses about the conditions under and the mechanisms by which causal effects operate, 
		as well as the moderation of such mechanisms. 
		Hayes shows how to estimate and interpret direct, 
		indirect, 
		and conditional effects; 
		probe and visualize interactions; 
		test questions about moderated mediation; 
		and report different types of analyses. 
		Data for all the examples are available on the companion website (www.afhayes.com)
		along with links to download PROCESS.
	},
}

@Comment{jabref-meta: databaseType:bibtex;}
% Encoding: US-ASCII

@Article{
	Lib-Mediation-Bootstrap-Bollen-1990
	,
	author       = {
		Bollen, Kenneth A. 
		and 
		Stine, Robert
	},
	date         = {
		1990
	},
	journaltitle = {
		Sociological Methodology
	},
	title        = {
		Direct and indirect effects: Classical and bootstrap estimates of variability
	},
	doi          = {
		10.2307/271084
	},
	pages        = {
		115
	},
	volume       = {
		20
	},
	publisher    = {
		{JSTOR}
	},
	file = {
		references/10.2307%2F271084.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Bootstrap
	},
	abstract     = {
		The decomposition of effects in structural equation models has been of considerable interest to social scientists. 
		Finite-sample or asymptotic results for the sampling distribution of estimators of direct effects are widely available. 
		Statistical inferences about indirect effects have relied exclusively on asymptotic methods which assume that the limiting distribution of the estimator is normal, 
		with a standard error derived from the delta method. 
		We examine bootstrap procedures as another way to generate standard errors and confidence intervals and to estimate the sampling distributions of estimators of direct and indirect effects. 
		We illustrate the classical and the bootstrap methods with three empirical examples. 
		We find that in a moderately large sample, the bootstrap distribution of an estimator is close to that assumed with the classical and delta methods but that in small samples, there are some differences. 
		Bootstrap methods provide a check on the classical and delta methods when the latter are applied under less than ideal conditions.
	},
}

@Article{
	Lib-Mediation-Bootstrap-Shrout-2002
	,
	author       = {
		Shrout, Patrick E. 
		and 
		Bolger, Niall
	},
	date         = {
		2002
	},
	journaltitle = {
		Psychological Methods
	},
	title        = {
		Mediation in experimental and nonexperimental studies: New procedures and recommendations
	},
	doi          = {
		10.1037/1082-989x.7.4.422
	},
	number       = {
		4
	},
	pages        = {
		422--445
	},
	volume       = {
		7
	},
	publisher    = {
		American Psychological Association ({APA})
	},
	file = {
		references/10.1037%2F1082-989x.7.4.422.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Bootstrap
	},
	abstract     = {
		Mediation is said to occur when a causal effect of some variable $X$ on an outcome $Y$ is explained by some intervening variable $M$. 
		The authors recommend that with small to moderate samples, 
		bootstrap methods (B. Efron \& R. Tibshirani, 1993) be used to assess mediation. 
		Bootstrap tests are powerful because they detect that the sampling distribution of the mediated effect is skewed away from 0. 
		They argue that R. M. Baron and D. A. Kenny's (1986) recommendation of first testing the $X \to Y$ association for statistical significance should not be a requirement when there is a priori belief that the effect size is small or suppression is a possibility. 
		Empirical examples and computer setups for bootstrap analyses are provided.
	},
}

@Article{
	Lib-Mediation-Bootstrap-Preacher-2004
	,
	author       = {
		Preacher, Kristopher J. 
		and 
		Hayes, Andrew F.
	},
	date         = {
		2004-11
	},
	journaltitle = {
		Behavior Research Methods, Instruments, \& Computers
	},
	title        = {
		{SPSS} and {SAS} procedures for estimating indirect effects in simple mediation models
	},
	doi          = {
		10.3758/bf03206553
	},
	number       = {
		4
	},
	pages        = {
		717--731
	},
	volume       = {
		36
	},
	publisher    = {
		Springer Science and Business Media {LLC}
	},
	file = {
		references/10.3758%2Fbf03206553.pdf
	},
	library = {},
	keywords = {
		life satisfaction,
		indirect effect,
		mediation analysis,
		cognitive therapy,
		Sobel test
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Bootstrap
	},
	abstract     = {
		Researchers often conduct mediation analysis in order to indirectly assess the effect of a proposed cause on some outcome through a proposed mediator. 
		The utility of mediation analysis stems from its ability to go beyond the merely descriptive to a more functional understanding of the relationships among variables. 
		A necessary component of mediation is a statistically and practically significant indirect effect. 
		Although mediation hypotheses are frequently explored in psychological research, 
		formal significance tests of indirect effects are rarely conducted. 
		After a brief overview of mediation, we argue the importance of directly testing the significance of indirect effects and provide SPSS and SAS macros that facilitate estimation of the indirect effect with a normal theory approach and a bootstrap approach to obtaining confidence intervals, 
		as well as the traditional approach advocated by Baron and Kenny (1986). 
		We hope that this discussion and the macros will enhance the frequency of formal mediation tests in the psychology literature. 
		Electronic copies of these macros may be downloaded from the Psychonomic Society’s Web archive at www.psychonomic.org/archive/.
	},
}

@Article{
	Lib-Mediation-Bootstrap-Cheung-2007-07
	,
	author       = {
		Cheung, Gordon W. 
		and 
		Lau, Rebecca S.
	},
	date         = {
		2007-07
	},
	journaltitle = {
		Organizational Research Methods
	},
	title        = {
		Testing mediation and suppression effects of latent variables
	},
	doi          = {
		10.1177/1094428107300343
	},
	number       = {
		2
	},
	pages        = {
		296--325
	},
	volume       = {
		11
	},
	publisher    = {
		{SAGE} Publications
	},
	file = {
		references/10.1177%2F1094428107300343.pdf
	},
	library = {},
	keywords = {
		mediating effects, 
		suppression effects, 
		structural equation modeling
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Bootstrap
	},
	abstract     = {
		Because of the importance of mediation studies, 
		researchers have been continuously searching for the best statistical test for mediation effect. 
		The approaches that have been most commonly employed include those that use zero-order and partial correlation, 
		hierarchical regression models, 
		and structural equation modeling (SEM). 
		This study extends MacKinnon and colleagues (MacKinnon, Lockwood, Hoffmann, West, \& Sheets, 2002; MacKinnon, Lockwood, \& Williams, 2004, MacKinnon, Warsi, \& Dwyer, 1995) works by conducting a simulation that examines the distribution of mediation and suppression effects of latent variables with SEM, 
		and the properties of confidence intervals developed from eight different methods. 
		Results show that SEM provides unbiased estimates of mediation and suppression effects, and that the bias-corrected bootstrap confidence intervals perform best in testing for mediation and suppression effects. 
		Steps to implement the recommended procedures with Amos are presented.
	},
}

@Article{
	Lib-Mediation-Bootstrap-Cheung-2007-05
	,
	author       = {
		Cheung, Mike W.-L.
	},
	date         = {
		2007-05
	},
	journaltitle = {
		Structural Equation Modeling: A Multidisciplinary Journal
	},
	title        = {
		Comparison of approaches to constructing confidence intervals for mediating effects using structural equation models
	},
	doi          = {
		10.1080/10705510709336745
	},
	number       = {
		2
	},
	pages        = {
		227--246
	},
	volume       = {
		14
	},
	publisher    = {
		Informa {UK} Limited
	},
	file = {
		references/10.1080%2F10705510709336745.pdf
	},
	library = {},
	keywords = {
		mediation, 
		bootstrapping
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Bootstrap
	},
	abstract     = {
		Mediators are variables that explain the association between an independent variable and a dependent variable. 
		Structural equation modeling (SEM) is widely used to test models with mediating effects. 
		This article illustrates how to construct confidence intervals (CIs) of the mediating effects for a variety of models in SEM. 
		Specifically, mediating models with 1 mediator, 2 intermediate mediators, 2 specific mediators, and 1 mediator in 2 independent groups are illustrated. 
		By using phantom variables (Rindskopf, 1984), 
		a Wald CI, 
		percentile bootstrap CI, 
		bias-corrected bootstrap CI, 
		and a likelihood-based CI on the mediating effect are easily constructed with some existing SEM packages, 
		such as LISREL, Mplus, and Mx. 
		Monte Carlo simulation studies are used to compare the coverage probabilities of these CIs. 
		The results show that the coverage probabilities of these CIs are comparable when the mediating effect is large or when the sample size is large. 
		However, when the mediating effect and the sample size are both small, the bootstrap CI and likelihood-based CI are preferred over the Wald CI. 
		Extensions of this SEM approach for future research are discussed.
	},
}

@Article{
	Lib-Mediation-Bootstrap-Taylor-2007
	,
	author       = {
		Taylor, Aaron B. 
		and 
		MacKinnon, David P. 
		and 
		Tein, Jenn-Yun
	},
	date         = {
		2007-07
	},
	journaltitle = {
		Organizational Research Methods
	},
	title        = {
		Tests of the three-path mediated effect
	},
	doi          = {
		10.1177/1094428107300344
	},
	number       = {
		2
	},
	pages        = {
		241--269
	},
	volume       = {
		11
	},
	publisher    = {
		{SAGE} Publications
	},
	file = {
		references/10.1177%2F1094428107300344.pdf
	},
	library = {},
	keywords = {
		mediation, 
		bootstrapping
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Bootstrap
	},
	abstract     = {
		In a three-path mediational model, 
		two mediators intervene in a series between an independent and a dependent variable. 
		Methods of testing for mediation in such a model are generalized from the more often used single-mediator model. 
		Six such methods are introduced and compared in a Monte Carlo study in terms of their Type I error, power, and coverage. 
		Based on its results, the joint significance test is preferred when only a hypothesis test is of interest. 
		The percentile bootstrap and bias-corrected bootstrap are preferred when a confidence interval on the mediated effect is desired, 
		with the latter having more power but also slightly inflated Type I error in some conditions.
	},
}

@Article{
	Lib-Mediation-Bootstrap-Preacher-2008
	,
	author       = {
		Preacher, Kristopher J. 
		and 
		Hayes, Andrew F.
	},
	date         = {
		2008-08
	},
	journaltitle = {
		Behavior Research Methods
	},
	title        = {
		Asymptotic and resampling strategies for assessing and comparing indirect effects in multiple mediator models
	},
	doi          = {
		10.3758/brm.40.3.879
	},
	number       = {
		3
	},
	pages        = {
		879--891
	},
	volume       = {
		40
	},
	publisher    = {
		Springer Science and Business Media {LLC}
	},
	file = {
		references/10.3758%2Fbrm.40.3.879.pdf
	},
	library = {},
	keywords = {
		indirect effect,
		structural equation modeling,
		residual covariance,
		total indirect effect,
		multiple mediator model
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Bootstrap
	},
	abstract     = {
		Hypotheses involving mediation are common in the behavioral sciences. 
		Mediation exists when a predictor affects a dependent variable indirectly through at least one intervening variable, or mediator. 
		Methods to assess mediation involving multiple simultaneous mediators have received little attention in the methodological literature despite a clear need. 
		We provide an overview of simple and multiple mediation and explore three approaches that can be used to investigate indirect processes, 
		as well as methods for contrasting two or more mediators within a single model. 
		We present an illustrative example, assessing and contrasting potential mediators of the relationship between the helpfulness of socialization agents and job satisfaction. 
		We also provide SAS and SPSS macros, 
		as well as Mplus and LISREL syntax, 
		to facilitate the use of these methods in applications.
	},
}

@Article{
	Lib-Mediation-Bootstrap-Cheung-2009
	,
	author       = {
		Cheung, Mike W.-L.
	},
	date         = {
		2009-05
	},
	journaltitle = {
		Behavior Research Methods
	},
	title        = {
		Comparison of methods for constructing confidence intervals of standardized indirect effects
	},
	doi          = {
		10.3758/brm.41.2.425
	},
	number       = {
		2
	},
	pages        = {
		425--438
	},
	volume       = {
		41
	},
	publisher    = {
		Springer Science and Business Media {LLC}
	},
	file = {
		references/10.3758%2Fbrm.41.2.425.pdf
	},
	library = {},
	keywords = {
		mediation analysis,
		coverage probability,
		structural equation modeling approach
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Bootstrap
	},
	abstract     = {
		Mediation models are often used as a means to explain the psychological mechanisms between an independent and a dependent variable in the behavioral and social sciences. 
		A major limitation of the unstandardized indirect effect calculated from raw scores is that it cannot be interpreted as an effect-size measure. 
		In contrast, the standardized indirect effect calculated from standardized scores can be a good candidate as a measure of effect size because it is scale invariant. 
		In the present article, 11 methods for constructing the confidence intervals (CIs) of the standardized indirect effects were evaluated via a computer simulation. 
		These included six Wald CIs, three bootstrap CIs, one likelihood-based CI, and the PRODCLIN CI. 
		The results consistently showed that the percentile bootstrap, the bias-corrected bootstrap, and the likelihood-based approaches had the best coverage probability. 
		Mplus, LISREL, and Mx syntax were included to facilitate the use of these preferred methods in applied settings. Future issues on the use of the standardized indirect effects are discussed.
	},
}

@Article{
	Lib-Mediation-Bootstrap-Hayes-2009
	,
	author       = {
		Hayes, Andrew F.
	},
	date         = {
		2009-12
	},
	journaltitle = {
		Communication Monographs
	},
	title        = {
		Beyond {Baron} and {Kenny}: Statistical mediation analysis in the new millennium
	},
	doi          = {
		10.1080/03637750903310360
	},
	number       = {
		4
	},
	pages        = {
		408--420
	},
	volume       = {
		76
	},
	publisher    = {
		Informa {UK} Limited
	},
	file = {
		references/10.1080%2F03637750903310360.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Bootstrap
	},
	abstract     = {
		Understanding communication processes is the goal of most communication researchers. 
		Rarely are we satisfied merely ascertaining whether messages have an effect on some outcome of focus in a specific context. 
		Instead, we seek to understand how such effects come to be. 
		What kinds of causal sequences does exposure to a message initiate? 
		What are the causal pathways through which a message exerts its effect? 
		And what role does communication play in the transmission of the effects of other variables over time and space? 
		Numerous communication models attempt to describe the mechanism through which messages or other communication-related variables transmit their effects or intervene between two other variables in a causal model. 
		The communication literature is replete with tests of such models.
		
		Over the years, methods used to test such process models have grown in sophistication. 
		An example includes the rise of structural equation modeling (SEM), which allows investigators to examine how well a process model that links some focal variable X to some outcome Y through one or more intervening pathways fits the observed data. 
		Yet frequently, the analytical choices communication researchers make when testing intervening variables models are out of step with advances made in the statistical methods literature. 
		My goal here is to update the field on some of these new advances. 
		While at it, I challenge some conventional wisdom and nudge the field toward a more modern way of thinking about the analysis of intervening variable effects.
	},
}

@Article{
	Lib-Mediation-Bootstrap-Biesanz-2010
	,
	author       = {
		Biesanz, Jeremy C. 
		and 
		Falk, Carl F. 
		and 
		Savalei, Victoria
	},
	date         = {
		2010-08
	},
	journaltitle = {
		Multivariate Behavioral Research
	},
	title        = {
		Assessing mediational models: Testing and interval estimation for indirect effects
	},
	doi          = {
		10.1080/00273171.2010.498292
	},
	number       = {
		4
	},
	pages        = {
		661--701
	},
	volume       = {
		45
	},
	publisher    = {
		Informa {UK} Limited
	},
	file = {
		references/10.1080%2F00273171.2010.498292.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Bootstrap
	},
	abstract     = {
		Theoretical models specifying indirect or mediated effects are common in the social sciences. 
		An indirect effect exists when an independent variable's influence on the dependent variable is mediated through an intervening variable. 
		Classic approaches to assessing such mediational hypotheses (Baron \& Kenny, 1986; Sobel, 1982) have in recent years been supplemented by computationally intensive methods such as bootstrapping, 
		the distribution of the product methods, 
		and hierarchical Bayesian Markov chain Monte Carlo (MCMC) methods. 
		These different approaches for assessing mediation are illustrated using data from Dunn, Biesanz, Human, and Finn (2007). 
		However, little is known about how these methods perform relative to each other, particularly in more challenging situations, 
		such as with data that are incomplete and/or nonnormal. 
		This article presents an extensive Monte Carlo simulation evaluating a host of approaches for assessing mediation. 
		We examine Type I error rates, power, and coverage. 
		We study normal and nonnormal data as well as complete and incomplete data. In addition, we adapt a method, 
		recently proposed in statistical literature, 
		that does not rely on confidence intervals (CIs) to test the null hypothesis of no indirect effect. 
		The results suggest that the new inferential method--the partial posterior p value--slightly outperforms existing ones in terms of maintaining Type I error rates while maximizing power, 
		especially with incomplete data. 
		Among confidence interval approaches, 
		the bias-corrected accelerated (BCa) bootstrapping approach often has inflated Type I error rates and inconsistent coverage and is not recommended.
		In contrast, the bootstrapped percentile confidence interval and the hierarchical Bayesian MCMC method perform best overall,
		maintaining Type I error rates,
		exhibiting reasonable power,
		and producing stable and accurate coverage rates.
	},
}

@Article{
	Lib-Mediation-Bootstrap-Fritz-2012
	,
	author       = {
		Fritz, Matthew S. 
		and 
		Taylor, Aaron B. 
		and 
		MacKinnon, David P.
	},
	date         = {
		2012-02
	},
	journaltitle = {
		Multivariate Behavioral Research
	},
	title        = {
		Explanation of two anomalous results in statistical mediation analysis
	},
	doi          = {
		10.1080/00273171.2012.640596
	},
	number       = {
		1
	},
	pages        = {
		61--87
	},
	volume       = {
		47
	},
	publisher    = {
		Informa {UK} Limited
	},
	file = {
		references/10.1080%2F00273171.2012.640596.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Bootstrap
	},
	abstract     = {
		Previous studies of different methods of testing mediation models have consistently found two anomalous results. 
		The first result is elevated Type I error rates for the bias-corrected and accelerated bias-corrected bootstrap tests not found in nonresampling tests or in resampling tests that did not include a bias correction. 
		This is of special concern as the bias-corrected bootstrap is often recommended and used due to its higher statistical power compared with other tests. 
		The second result is statistical power reaching an asymptote far below 1.0 and in some conditions even declining slightly as the size of the relationship between X and M, a, increased. 
		Two computer simulations were conducted to examine these findings in greater detail. Results from the first simulation found that the increased Type I error rates for the bias-corrected and accelerated bias-corrected bootstrap are a function of an interaction between the size of the individual paths making up the mediated effect and the sample size, 
		such that elevated Type I error rates occur when the sample size is small and the effect size of the nonzero path is medium or larger. 
		Results from the second simulation found that stagnation and decreases in statistical power as a function of the effect size of the a path occurred primarily when the path between M and Y, b, was small. 
		Two empirical mediation examples are provided using data from a steroid prevention and health promotion program aimed at high school football players (Athletes Training and Learning to Avoid Steroids; Goldberg et al., 1996), 
		one to illustrate a possible Type I error for the bias-corrected bootstrap test and a second to illustrate a loss in power related to the size of a. 
		Implications of these findings are discussed.
	},
}

@Article{
	Lib-Mediation-Bootstrap-Taylor-2012
	,
	author       = {
		Taylor, Aaron B. 
		and 
		MacKinnon, David P.
	},
	date         = {
		2012-02
	},
	journaltitle = {
		Behavior Research Methods
	},
	title        = {
		Four applications of permutation methods to testing a single-mediator model
	},
	doi          = {
		10.3758/s13428-011-0181-x
	},
	number       = {
		3
	},
	pages        = {
		806--844
	},
	volume       = {
		44
	},
	publisher    = {
		Springer Science and Business Media {LLC}
	},
	file = {
		references/10.3758%2Fs13428-011-0181-x.pdf
	},
	library = {},
	keywords = {
		mediation, 
		bootstrapping, 
		permutation, 
		Bayes
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Bootstrap
	},
	abstract     = {
		Four applications of permutation tests to the single-mediator model are described and evaluated in this study. 
		Permutation tests work by rearranging data in many possible ways in order to estimate the sampling distribution for the test statistic.
		The four applications to mediation evaluated here are the permutation test of ab, 
		the permutation joint significance test, and the noniterative and iterative permutation confidence intervals for ab. 
		A Monte Carlo simulation study was used to compare these four tests with the four best available tests for mediation found in previous research: 
		the joint significance test, 
		the distribution of the product test, 
		and the percentile and bias-corrected bootstrap tests. 
		We compared the different methods on Type I error, 
		power, 
		and confidence interval coverage. 
		The noniterative permutation confidence interval for ab was the best performer among the new methods. 
		It successfully controlled Type I error, 
		had power nearly as good as the most powerful existing methods, 
		and had better coverage than any existing method. 
		The iterative permutation confidence interval for ab had lower power than do some existing methods, 
		but it performed better than any other method in terms of coverage. 
		The permutation confidence interval methods are recommended when estimating a confidence interval is a primary concern. 
		SPSS and SAS macros that estimate these confidence intervals are provided.
	},
}

@Article{
	Lib-Mediation-Bootstrap-Hayes-2013
	,
	author       = {
		Hayes, Andrew F. 
		and 
		Scharkow, Michael
	},
	date         = {
		2013-08
	},
	journaltitle = {
		Psychological Science
	},
	title        = {
		The relative trustworthiness of inferential tests of the indirect effect in statistical mediation analysis
	},
	doi          = {
		10.1177/0956797613480187
	},
	number       = {
		10
	},
	pages        = {
		1918--1927
	},
	volume       = {
		24
	},
	publisher    = {
		{SAGE} Publications
	},
	abstract     = {
		A content analysis of 2 years of Psychological Science articles reveals inconsistencies in how researchers make inferences about indirect effects when conducting a statistical mediation analysis. 
		In this study, we examined the frequency with which popularly used tests disagree, whether the method an investigator uses makes a difference in the conclusion he or she will reach, and whether there is a most trustworthy test that can be recommended to balance practical and performance considerations. 
		We found that tests agree much more frequently than they disagree, but disagreements are more common when an indirect effect exists than when it does not. 
		We recommend the bias-corrected bootstrap confidence interval as the most trustworthy test if power is of utmost concern, although it can be slightly liberal in some circumstances. 
		Investigators concerned about Type I errors should choose the Monte Carlo confidence interval or the distribution-of-the-product approach, which rarely disagree. 
		The percentile bootstrap confidence interval is a good compromise test.
	},
}

@InBook{
	Lib-Mediation-Bootstrap-Koopman-2014
	,
	author     = {
		Koopman, Joel 
		and 
		Howe, Michael 
		and 
		Hollenbeck, John R.
	},
	booktitle  = {
		More statistical and methodological myths and urban legends: Doctrine, verity and fable in organizational and social sciences
	},
	date       = {
		2014
	},
	title      = {
		Pulling the {Sobel} test up by its bootstraps
	},
	bookauthor = {
		Lance, Charles E. 
		and 
		Vandenberg, Robert J.
	},
	isbn       = {
		9780203775851
	},
	pages      = {
		224--243
	},
	publisher  = {
		Routledge/Taylor \& Francis Group
	},
	location = {},
	doi = {
		10.4324/9780203775851 
	},
	isbn = {
		9780203775851
	},
	file = {
		references/9780203775851_11.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Bootstrap
	},
	abstract   = {
		In the domain of building and testing theory, mediation relationships are among the most important that can be proposed. 
		Mediation helps to explicate our theoretical models (Leavitt, Mitchell, \& Peterson, 2010) and addresses the fundamental question of why two constructs are related (Whetten, 1989). 
		One of the better-known methods for testing mediation is commonly referred to as the ``Sobel test,'' named for the researcher who derived a standard error (Sobel, 1982) to test the significance of the indirect effect. 
		Recently, a number of different research teams (e.g., Preacher \& Hayes, 2004; Shrout \& Bolger, 2002) have criticized the Sobel test because this standard error requires an assumption of normality for the indirect effect sampling distribution. This distribution tends to be positively skewed (i.e,. not normal), 
		particularly in small samples, 
		and so this assumption can be problematic (Preacher \& Hayes, 2004; Stone \& Sobel, 1990).
		As a result, the statistical power of the Sobel test may be lessened in these contexts (Preacher \& Hayes 2004; Shrout \& Bolger, 2002). 
		In light of this concern, 
		some scholars have advocated instead for the use of bootstrapping to test the significance of the indirect effect (e.g.. Shrout \& Bolger 2002). 
		Bootstrapping requires no a priori assumption about the shape of the sampling distribution because this distribution is empirically estimated using a resampling procedure (Efron \& Tibshirani, 1993). 
		As a result, 
		departures from normality are less troublesome when creating a confidence interval for the indirect effect. 
		For this reason, 
		bootstrapping is now widely believed to be inherently superior to the Sobel test when testing the significance of the indirect effect in organizational research. 
		Our position is that this belief constitutes an urban legend. As with all statistical urban legends, there is an underlying kernel of truth to the belief that bootstrapping is superior to the Sobel test. 
		However, 
		as we discuss in this chapter, 
		there are several reasons to be concerned with a broad belief in the superiority of bootstrapping. 
		We begin with a brief overview of mediation testing focusing on the Sobel test and bootstrapping and then explain the underlying kernel of truth that has propelled bootstrapping to the forefront of mediation testing in organizational research. 
		Subsequently, 
		we discuss four areas of concern that cast doubt on the belief of the inherent superiority of bootstrapping. 
		Finally, 
		we conclude with recommendations concerning the future of mediation testing in organizational research.
	},
}

@Article{
	Lib-Mediation-Bootstrap-Koopman-2015
	,
	author       = {
		Koopman, Joel 
		and 
		Howe, Michael 
		and 
		Hollenbeck, John R. 
		and 
		Sin, 
		Hock-Peng
	},
	date         = {
		2015
	},
	journaltitle = {
		Journal of Applied Psychology
	},
	title        = {
		Small sample mediation testing: Misplaced confidence in bootstrapped confidence intervals
	},
	doi          = {
		10.1037/a0036635
	},
	number       = {
		1
	},
	pages        = {
		194--202
	},
	volume       = {
		100
	},
	publisher    = {
		American Psychological Association ({APA})
	},
	file = {
		references/10.1037%2Fa0036635.pdf
	},
	library = {},
	keywords = {
		mediation, 
		bootstrapping, 
		permutation, 
		Bayes
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Bootstrap
	},
	abstract     = {
		Bootstrapping is an analytical tool commonly used in psychology to test the statistical significance of the indirect effect in mediation models. 
		Bootstrapping proponents have particularly advocated for its use for samples of 20-80 cases. 
		This advocacy has been heeded, 
		especially in the Journal of Applied Psychology, 
		as researchers are increasingly utilizing bootstrapping to test mediation with samples in this range. 
		We discuss reasons to be concerned with this escalation, 
		and in a simulation study focused specifically on this range of sample sizes, 
		we demonstrate not only that bootstrapping has insufficient statistical power to provide a rigorous hypothesis test in most conditions but also that bootstrapping has a tendency to exhibit an inflated Type I error rate.
		We then extend our simulations to investigate an alternative empirical resampling method as well as a Bayesian approach and demonstrate that they exhibit comparable statistical power to bootstrapping in small samples without the associated inflated Type I error. 
		Implications for researchers testing mediation hypotheses in small samples are presented. 
		For researchers wishing to use these methods in their own research, 
		we have provided R syntax in the online supplemental materials.
	},
}

@Article{
	Lib-Mediation-Bootstrap-Tofighi-2020
	,
	author       = {
		Tofighi, Davood
		and
		Kelley, Ken
	},
	date         = {
		2020
	},
	journaltitle = {
		Psychological Methods
	},
	title        = {
		Improved inference in mediation analysis: Introducing the model-based constrained optimization procedure
	},
	doi          = {
		10.1037/met0000259
	},
	pages        = {
		496--515
	},
	volume       = {
		25
	},
	publisher    = {
		{American Psychological Association ({APA})}
	},
	file = {
		references/10.1037%2Fmet0000259.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Bootstrap
	},
	abstract     = {
		Mediation analysis is an important approach for investigating causal pathways.
		One approach used in mediation analysis is the test of an indirect effect,
		which seeks to measure how the effect of an independent variable impacts an outcome variable through one or more mediators.
		However, in many situations the proposed tests of indirect effects, including popular confidence interval-based methods,
		tend to produce poor Type I error rates when mediation does not occur and,
		more generally, only allow dichotomous decisions of ``not significant'' or ``significant'' with regards to the statistical conclusion.
		To remedy these issues, we propose a new method, a likelihood ratio test (LRT),
		that uses non-linear constraints in what we term the model-based constrained optimization (MBCO) procedure.
		The MBCO procedure (a) offers a more robust Type I error rate than existing methods;
		(b) provides a p-value, which serves as a continuous measure of compatibility of data with the hypothesized null model
		(not just a dichotomous reject or fail-to-reject decision rule);
		(c) allows simple and complex hypotheses about mediation
		(i.e., one or more mediators; different mediational pathways),
		and (d) allows the mediation model to use observed or latent variables.
		The MBCO procedure is based on a structural equation modeling framework
		(even if latent variables are not specified)
		with specialized fitting routines,
		namely with the use of non-linear constraints.
		We advocate using the MBCO procedure to test hypotheses about an indirect effect
		in addition to reporting a confidence interval to capture uncertainty about the indirect effect
		because this combination transcends existing methods.
	},
}

@Article{
	Lib-Mediation-Bootstrap-Pesigan-2020
	,
	author       = {
		Pesigan, Ivan Jacob Agaloos
		and
		Cheung, Shu Fai
	},
	date         = {
		2020-12
	},
	journaltitle = {
		Frontiers in Psychology
	},
	title        = {
		{SEM}-based methods to form confidence intervals for indirect effect: Still applicable given nonnormality, under certain conditions
	},
	doi          = {
		10.3389/fpsyg.2020.571928
	},
	volume       = {
		11
	},
	publisher    = {
		Frontiers Media {SA}
	},  
	file = {
		references/10.3389%2Ffpsyg.2020.571928.pdf
	},
	library = {},
	keywords = {
		mediation, 
		nonnormal, 
		confidence interval, 
		structural equation modeling,
		bootstrapping
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Bootstrap
	},
	abstract     = {
		A SEM-based approach using likelihood-based confidence interval (LBCI) has been proposed to form confidence intervals for unstandardized and standardized indirect effect in mediation models.
		However,
		when used with the maximum likelihood estimation,
		this approach requires that the variables are multivariate normally distributed.
		This can affect the LBCIs of unstandardized and standardized effect differently.
		In the present study,
		the robustness of this approach when the predictor is not normally distributed but the error terms are conditionally normal,
		which does not violate the distributional assumption of ordinary least squares (OLS) estimation,
		is compared to four other approaches:
		nonparametric bootstrapping,
		two variants of LBCI,
		LBCI assuming the predictor is fixed (LBCI-Fixed-X) and LBCI based on ADF estimation (LBCI-ADF),
		and Monte Carlo.
		A simulation study was conducted using a simple mediation model and a serial mediation model,
		manipulating the distribution of the predictor.
		The Monte Carlo method performed worst among the methods.
		LBCI and LBCI-Fixed-X had suboptimal performance when the distributions had high kurtosis and the population indirect effects were medium to large.
		In some conditions,
		the problem was severe even when the sample size was large.
		LBCI-ADF and nonparametric bootstrapping had coverage probabilities close to the nominal value in nearly all conditions,
		although the coverage probabilities were still suboptimal for the serial mediation model when the sample size was small with respect to the model.
		Implications of these findings in the context of this special case of nonnormal data were discussed.
	},
}

@Article{
	Lib-Mediation-Bootstrap-Cheung-2022
	,
	author       = {
		Cheung, Shu Fai 
		and 
		Pesigan, Ivan Jacob Agaloos 
		and 
		Vong, Weng Ngai
	},
	date         = {
		2022-03
	},
	journaltitle = {
		Behavior Research Methods
	},
	title        = {
		{DIY} bootstrapping: Getting the nonparametric bootstrap confidence interval in {SPSS} for any statistics or function of statistics (when this bootstrapping is appropriate)
	},
	doi          = {
		10.3758/s13428-022-01808-5
	},
	publisher    = {
		Springer Science and Business Media {LLC}
	},
	file = {
		references/10.3758%2Fs13428-022-01808-5.pdf
	},
	library = {},
	keywords = {
		bootstrapping,
		effect sizes,
		confidence intervals
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Bootstrap
	},
	abstract     = {
		Researchers can generate bootstrap confidence intervals for some statistics in SPSS using the BOOTSTRAP command. 
		However, 
		this command can only be applied to selected procedures, 
		and only to selected statistics in these procedures. 
		We developed an extension command and prepared some sample syntax files based on existing approaches from the Internet to illustrate how researchers can 
		(a) generate a large number of nonparametric bootstrap samples, 
		(b) do desired analysis on all these samples, and 
		(c) form the bootstrap confidence intervals for selected statistics using the OMS commands. 
		We developed these tools to help researchers apply nonparametric bootstrapping to any statistics for which this method is appropriate, including statistics derived from other statistics, 
		such as standardized effect size measures computed from the t test results. 
		We also discussed how researchers can extend the tools for other statistics and scenarios they encounter.
	},
}

@Comment{jabref-meta: databaseType:biblatex;}
% Encoding: US-ASCII

@Article{
	Lib-Mediation-Causal-Steps-Judd-1981
	,
	author       = {
		Judd, Charles M. 
		and 
		Kenny, David A.
	},
	date         = {
		1981-10
	},
	journaltitle = {
		Evaluation Review
	},
	title        = {
		Process analysis
	},
	doi          = {
		10.1177/0193841x8100500502
	},
	number       = {
		5
	},
	pages        = {
		602--619
	},
	volume       = {
		5
	},
	publisher    = {
		{SAGE} Publications
	},
	file = {
		references/10.1177%2F0193841x8100500502.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Causal-Steps
	},
	abstract     = {
		This article presents the rationale and procedures for conducting a process analysis in evaluation research. 
		Such an analysis attempts to identify the process that mediates the effects of some treatment, 
		by estimating the parameters of a causal chain between the treatment and some outcome variable. 
		Two different procedures for estimating mediation are discussed. 
		In addition we present procedures for examining whether a treatment exerts its effects, 
		in part, 
		by altering the mediating process that produces the outcome. 
		Finally, the benefits of process analysis in evaluation research are underlined.
	},
	
}

@Article{
	Lib-Mediation-Causal-Steps-James-1984
	,
	author       = {
		James, Lawrence R. 
		and 
		Brett, Jeanne M.
	},
	date         = {
		1984
	},
	journaltitle = {
		Journal of Applied Psychology
	},
	title        = {
		Mediators, moderators, and tests for mediation
	},
	doi          = {
		10.1037/0021-9010.69.2.307
	},
	number       = {
		2
	},
	pages        = {
		307--321
	},
	volume       = {
		69
	},
	publisher    = {
		American Psychological Association ({APA})
	},
	file = {
		references/10.1037%2F0021-9010.69.2.307.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Causal-Steps
	},
	abstract     = {
		Discusses mediation relations in causal terms. 
		Influences of an antecedent are transmitted to a consequence through an intervening mediator. 
		Mediation relations may assume a number of functional forms, 
		including nonadditive, 
		nonlinear, 
		and nonrecursive forms. 
		Although mediation and moderation are distinguishable processes, 
		with nonadditive forms (moderated mediation) a particular variable may be both a mediator and a moderator within a single set of functional relations. 
		Current models for testing mediation relations in industrial and organizational psychology often involve an interplay between exploratory (correlational) statistical tests and causal inference. 
		It is suggested that no middle ground exists between exploratory and confirmatory (causal) analysis and that attempts to explain how mediation processes occur require specified causal models.
	},
}

@Article{
	Lib-Mediation-Causal-Steps-Baron-1986
	,
	author       = {
		Baron, Reuben M. 
		and 
		Kenny, David A.
	},
	date         = {
		1986
	},
	journaltitle = {
		Journal of Personality and Social Psychology
	},
	title        = {
		The moderator-mediator variable distinction in social psychological research: Conceptual, strategic, and statistical considerations
	},
	doi          = {
		10.1037/0022-3514.51.6.1173
	},
	number       = {
		6
	},
	pages        = {
		1173--1182
	},
	volume       = {
		51
	},
	publisher    = {
		American Psychological Association ({APA})
	},
	file = {
		references/10.1037%2F0022-3514.51.6.1173.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Causal-Steps
	},
	abstract     = {
		In this article, 
		we attempt to distinguish between the properties of moderator and mediator variables at a number of levels. 
		First, 
		we seek to make theorists and researchers aware of the importance of not using the terms moderator and mediator interchangeably by carefully elaborating, 
		both conceptually and strategically, 
		the many ways in which moderators and mediators differ. 
		We then go beyond this largely pedagogical function and delineate the conceptual and strategic implications of making use of such distinctions with regard to a wide range of phenomena, 
		including control and stress, 
		attitudes, 
		and personality traits. 
		We also provide a specific compendium of analytic procedures appropriate for making the most effective use of the moderator and mediator distinction, 
		both separately and in terms of a broader causal system that includes both moderators and mediators.
	},
}

@Comment{jabref-meta: databaseType:biblatex;}
% Encoding: US-ASCII

@Article{
	Lib-Mediation-Delta-Method-Craig-1936
	,
	author       = {
		Craig, Cecil C.
	},
	date         = {
		1936-03
	},
	journaltitle = {
		The Annals of Mathematical Statistics
	},
	title        = {
		On the frequency function of $xy$
	},
	doi          = {
		10.1214/aoms/1177732541
	},
	number       = {
		1
	},
	pages        = {
		1--15
	},
	volume       = {
		7
	},
	publisher    = {
		Institute of Mathematical Statistics
	},
	file = {
		references/10.1214%2Faoms%2F1177732541.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Delta-Method
	},
}


@Article{
	Lib-Mediation-Delta-Method-Aroian-1947
	,
	author       = {
		Aroian, Leo A.
	},
	date         = {
		1947-06
	},
	journaltitle = {
		The Annals of Mathematical Statistics
	},
	title        = {
		The probability function of the product of two normally distributed variables
	},
	doi          = {
		10.1214/aoms/1177730442
	},
	number       = {
		2
	},
	pages        = {
		265--271
	},
	volume       = {
		18
	},
	publisher    = {
		Institute of Mathematical Statistics
	},
	file = {
		references/10.1214%2Faoms%2F1177730442.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Delta-Method
	},
	abstract     = {
		Let $x$ and $y$ follow a normal bivariate probability function with means $\bar X, \bar Y$, standard deviations $\sigma_1, \sigma_2$, respectively, $r$ the coefficient of correlation, and $\rho_1 = \bar X/\sigma_1, \rho_2 = \bar Y/\sigma_2$. Professor C. C. Craig [1] has found the probability function of $z = xy/\sigma_1\sigma_2$ in closed form as the difference of two integrals. For purposes of numerical computation he has expanded this result in an infinite series involving powers of $z, \rho_1, \rho_2$, and Bessel functions of a certain type; in addition, he has determined the moments, semin-variants, and the moment generating function of $z$. However, for $\rho_1$ and $\rho_2$ large, as Craig points out, the series expansion converges very slowly. Even for $\rho_1$ and $\rho_2$ as small as 2, the expansion is unwieldy. We shall show that as $\rho_1$ and $\rho_2 \rightarrow \infty$, the probability function of $z$ approaches a normal curve and in case $r = 0$ the Type III function and the Gram-Charlier Type A series are excellent approximations to the $z$ distribution in the proper region. Numerical integration provides a substitute for the infinite series wherever the exact values of the probability function of $z$ are needed. Some extensions of the main theorem are given in section 5 and a practical problem involving the probability function of $z$ is solved.
	},
}

@Article{
	Lib-Mediation-Delta-Method-Goodman-1960
	,
	author       = {
		Goodman, Leo A.
	},
	date         = {
		1960-12
	},
	journaltitle = {
		Journal of the American Statistical Association
	},
	title        = {
		On the exact variance of products
	},
	doi          = {
		10.1080/01621459.1960.10483369
	},
	number       = {
		292
	},
	pages        = {
		708--713
	},
	volume       = {
		55
	},
	publisher    = {
		Informa {UK} Limited
	},
	file = {
		references/10.1080%2F01621459.1960.10483369.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Delta-Method
	},
	abstract     = {
		A simple exact formula for the variance of the product of two random variables, say, x and y, is given as a function of the means and central product-moments of x and y. The usual approximate variance formula for xy is compared with this exact formula; e.g., we note, in the special case where x and y are independent, that the ``variance'' computed by the approximate formula is less than the exact variance, and that the accuracy of the approximation depends on the sum of the reciprocals of the squared coefficients of variation of x and y. The case where x and y need not be independent is also studied, and exact variance formulas are presented for several different ``product estimates.'' (The usefulness of exact formulas becomes apparent when the variances of these estimates are compared.) When x and y are independent, simple unbiased estimates of these exact variances are suggested; in the more general case, consistent estimates are presented.
	},
}

@Article{
	Lib-Mediation-Delta-Method-Sobel-1982
	,
	author       = {
		Sobel, Michael E.
	},
	date         = {
		1982
	},
	journaltitle = {
		Sociological Methodology
	},
	title        = {
		Asymptotic confidence intervals for indirect effects in structural equation models
	},
	doi          = {
		10.2307/270723
	},
	pages        = {
		290
	},
	volume       = {
		13
	},
	publisher    = {
		{JSTOR}
	},
	file = {
		references/10.2307%2F270723.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Delta-Method
	},
}

@Article{
	Lib-Mediation-Delta-Method-Sobel-1986
	,
	author       = {
		Sobel, Michael E.
	},
	date         = {
		1986
	},
	journaltitle = {
		Sociological Methodology
	},
	title        = {
		Some new results on indirect effects and their standard errors in covariance structure models
	},
	doi          = {
		10.2307/270922
	},
	pages        = {
		159
	},
	volume       = {
		16
	},
	publisher    = {
		{JSTOR}
	},
	file = {
		references/10.2307%2F270922.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Delta-Method
	},
}

@Article{
	Lib-Mediation-Delta-Method-Sobel-1987
	,
	author       = {
		Sobel, Michael E.
	},
	date         = {
		1987-08
	},
	journaltitle = {
		Sociological Methods {\&} Research
	},
	title        = {
		Direct and indirect effects in linear structural equation models
	},
	doi          = {
		10.1177/0049124187016001006
	},
	number       = {
		1
	},
	pages        = {
		155--176
	},
	volume       = {
		16
	},
	publisher    = {
		{SAGE} Publications
	},
	file = {
		references/10.1177%2F0049124187016001006.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Delta-Method
	},
	abstract     = {
		This article discusses total indirect effects in linear structural equation models.
		First,
		I define these effects.
		Second,
		I show how the delta method may be used to obtain the standard errors of the sample estimates of these effects and test hypotheses about the magnitudes of the indirect effects.
		To keep matters simple,
		I focus throughout on a particularly simple linear structural equation system;
		for a treatment of the general case,
		see Sobel (1986).
		To illustrate the ideas and results,
		a detailed example is presented.
	},
}

@Comment{jabref-meta: databaseType:biblatex;}
% Encoding: US-ASCII

@Article{
	Lib-Mediation-Joint-Significance-Test-MacKinnon-2002
	,
	author       = {
		MacKinnon, David P. 
		and 
		Lockwood, Chondra M. 
		and 
		Hoffman, Jeanne M. 
		and 
		West, Stephen G. 
		and 
		Sheets, Virgil
	},
	date         = {
		2002
	},
	journaltitle = {
		Psychological Methods
	},
	title        = {
		A comparison of methods to test mediation and other intervening variable effects
	},
	doi          = {
		10.1037/1082-989x.7.1.83
	},
	number       = {
		1
	},
	pages        = {
		83--104
	},
	volume       = {
		7
	},
	publisher    = {
		American Psychological Association ({APA})
	},
	file = {
		references/10.1037%2F1082-989x.7.1.83.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Joint-Significance-Test
	},
	abstract     = {
		A Monte Carlo study compared 14 methods to test the statistical significance of the intervening variable effect. 
		An intervening variable (mediator) transmits the effect of an independent variable to a dependent variable. 
		The commonly used R. M. Baron and D. A. Kenny (1986) approach has low statistical power. 
		Two methods based on the distribution of the product and 2 difference-in-coefficients methods have the most accurate Type I error rates and greatest statistical power except in 1 important case in which Type I error rates are too high. 
		The best balance of Type I error and statistical power across all cases is the test of the joint significance of the two effects comprising the intervening variable effect.
	},
}

@Article{
	Lib-Mediation-Joint-Significance-Test-Yzerbyt-2018
	,
	author       = {
		Yzerbyt, Vincent
		and
		Muller, Dominique
		and
		Batailler, C{\'{e}}dric
		and
		Judd, Charles M.
	},
	date         = {
		2018-12
	},
	journaltitle = {
		Journal of Personality and Social Psychology
	},
	title        = {
		New recommendations for testing indirect effects in mediational models: The need to report and test component paths
	},
	doi          = {
		10.1037/pspa0000132
	},
	number       = {
		6
	},
	pages        = {
		929--943
	},
	volume       = {
		115
	},
	publisher    = {
		American Psychological Association ({APA})
	},
	file = {
		references/10.1037%2Fpspa0000132.pdf
	},
	library = {},
	keywords = {
		indirect effects, 
		mediation, 
		joint-significance, 
		bootstrap
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Joint-Significance-Test
	},
	abstract     = {
		In light of current concerns with replicability and reporting false-positive effects in psychology,
		we examine Type I errors and power associated with 2 distinct approaches for the assessment of mediation,
		namely the component approach (testing individual parameter estimates in the model) and the index approach (testing a single mediational index).
		We conduct simulations that examine both approaches and show that the most commonly used tests under the index approach risk inflated Type I errors compared with the joint-significance test inspired by the component approach.
		We argue that the tendency to report only a single mediational index is worrisome for this reason and also because it is often accompanied by a failure to critically examine the individual causal paths underlying the mediational model.
		We recommend testing individual components of the indirect effect to argue for the presence of an indirect effect and then using other recommended procedures to calculate the size of that effect. 
		Beyond simple mediation,
		we show that our conclusions also apply in cases of within-participant mediation and moderated mediation.
		We also provide a new R-package that allows for an easy implementation of our recommendations.
	},
}

@Comment{jabref-meta: databaseType:biblatex;}
% Encoding: US-ASCII

@Article{
	Lib-Mediation-Meta-Analytic-Structural-Equation-Modeling-Cheung-2021
	,
	author = {
		Cheung, Mike W.-L.
	},
	date = {
		2021-06
	},
	journaltitle = {
		Alcohol and Alcoholism
	},
	title = {
		Synthesizing indirect effects in mediation models with meta-analytic methods
	},
	doi = {
		10.1093/alcalc/agab044
	},
	number = {
		1
	},
	pages = {
		5--15
	},
	volume = {
		57
	},
	publisher = {
		Oxford University Press ({OUP})
	},
	file = {
		references/10.1093%2Falcalc%2Fagab044.pdf
	},
	library = {},
	keywords = {
		heterogeneity,
		gold standard,
		outcome variable,
		datasets,
		mediation analysis 
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Meta-Analysis-as-Structural-Equation-Models
	},
	abstract = {
		Aims
						 
		A mediator is a variable that explains the underlying mechanism between an independent variable and a dependent variable.
		The indirect effect indicates the effect from the predictor to the outcome variable via the mediator.
		In contrast,
		the direct effect represents the predictor's effort on the outcome variable after controlling for the mediator.
						 
		Methods
						 
		A single study rarely provides enough evidence to answer research questions in a particular domain.
		Replications are generally recommended as the gold standard to conduct scientific research.
		When a sufficient number of studies have been conducted addressing similar research questions,
		a meta-analysis can be used to synthesize those studies' findings.
						 
		Results
						 
		The main objective of this paper is to introduce two frameworks to integrating studies using mediation analysis.
		The first framework involves calculating standardized indirect effects and direct effects and conducting a multivariate meta-analysis on those effect sizes.
		The second one uses meta-analytic structural equation modeling to synthesize correlation matrices and fit mediation models on the average correlation matrix.
		We illustrate these procedures on a real dataset using the R statistical platform.
						 
		Conclusion
						 
		This paper closes with some further directions for future studies.
	},
}

@Comment{jabref-meta: databaseType:biblatex;}
% Encoding: US-ASCII

@Article{
	Lib-Mediation-Missing-Data-Zhang-2012
	,
	author       = {
		Zhang, Zhiyong 
		and 
		Wang, Lijuan
	},
	date         = {
		2012-12
	},
	journaltitle = {
		Psychometrika
	},
	title        = {
		Methods for mediation analysis with missing data
	},
	doi          = {
		10.1007/s11336-012-9301-5
	},
	number       = {
		1
	},
	pages        = {
		154--184
	},
	volume       = {
		78
	},
	publisher    = {
		Springer Science and Business Media {LLC}
	},
	file = {
		references/10.1007%2Fs11336-012-9301-5.pdf
	},
	library = {},
	keywords = {
		mediation analysis,
		missing data,
		MI,
		TS-ML,
		bootstrap,
		auxiliary variables
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Missing-Data
	},
	abstract = {
		Despite wide applications of both mediation models and missing data techniques, formal discussion of mediation analysis with missing data is still rare. 
		We introduce and compare four approaches to dealing with missing data in mediation analysis including listwise deletion, pairwise deletion, multiple imputation (MI), and a two-stage maximum likelihood (TS-ML) method. 
		An R package bmem is developed to implement the four methods for mediation analysis with missing data in the structural equation modeling framework, and two real examples are used to illustrate the application of the four methods. 
		The four methods are evaluated and compared under MCAR, MAR, and MNAR missing data mechanisms through simulation studies. 
		Both MI and TS-ML perform well for MCAR and MAR data regardless of the inclusion of auxiliary variables and for AV-MNAR data with auxiliary variables. 
		Although listwise deletion and pairwise deletion have low power and large parameter estimation bias in many studied conditions, they may provide useful information for exploring missing mechanisms.
	},
}

@Article{
	Lib-Mediation-Missing-Data-Wu-2013
	,
	author       = {
		Wu, Wei 
		and 
		Jia, Fan
	},
	date         = {
		2013-09
	},
	journaltitle = {
		Multivariate Behavioral Research
	},
	title        = {
		A new procedure to test mediation with missing data through nonparametric bootstrapping and multiple imputation
	},
	doi          = {
		10.1080/00273171.2013.816235
	},
	number       = {
		5
	},
	pages        = {
		663--691
	},
	volume       = {
		48
	},
	publisher    = {
		Informa {UK} Limited
	},
	file = {
		references/10.1080%2F00273171.2013.816235.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Missing-Data
	},
	abstract = {
		This article proposes a new procedure to test mediation with the presence of missing data by combining nonparametric bootstrapping with multiple imputation (MI). 
		This procedure performs MI first and then bootstrapping for each imputed data set. 
		The proposed procedure is more computationally efficient than the procedure that performs bootstrapping first and then MI for each bootstrap sample. 
		The validity of the procedure is evaluated using a simulation study under different sample size, missing data mechanism, missing data proportion, and shape of distribution conditions. 
		The result suggests that the proposed procedure performs comparably to the procedure that combines bootstrapping with full information maximum likelihood under most conditions. 
		However, caution needs to be taken when using this procedure to handle missing not-at-random or nonnormal data.
	},
}

@InCollection{
	Lib-Mediation-Missing-Data-Zhang-2015
	,
	author    = {
		Zhang, Zhiyong 
		and 
		Wang, Lijuan 
		and 
		Tong, Xin
	},
	booktitle = {
		Quantitative Psychology Research
	},
	date      = {
		2015
	},
	title     = {
		Mediation analysis with missing data through multiple imputation and bootstrap
	},
	doi       = {
		10.1007/978-3-319-19977-1_24
	},
	pages     = {
		341--355
	},
	publisher = {
		Springer International Publishing
	},
	file = {
		references/10.1007%2F978-3-319-19977-1_24.pdf
	},
	library = {},
	keywords = {
		mediation analysis,
		missing data,
		multiple imputation,
		bootstrap
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Missing-Data
	},
	abtract = {
		A method using multiple imputation and bootstrap for dealing with missing data in mediation analysis is introduced and implemented in both SAS and R. Through simulation studies, it is shown that the method performs well for both MCAR and MAR data without and with auxiliary variables. It is also shown that the method can work for MNAR data if auxiliary variables related to missingness are included. The application of the method is demonstrated through the analysis of a subset of data from the National Longitudinal Survey of Youth. Mediation analysis with missing data can be conducted using the provided SAS macros and R package bmem.
	},
}

@Comment{jabref-meta: databaseType:biblatex;}
% Encoding: US-ASCII

@Article{
	Lib-Mediation-Monte-Carlo-Method-MacKinnon-2004
	,
	author       = {
		MacKinnon, David P.
		and
		Lockwood, Chondra M.
		and
		Williams, Jason
	},
	date         = {
		2004-01
	},
	journaltitle = {
		Multivariate Behavioral Research
	},
	title        = {
		Confidence limits for the indirect effect: Distribution of the product and resampling methods
	},
	doi          = {
		10.1207/s15327906mbr3901_4
	},
	number       = {
		1
	},
	pages        = {
		99--128
	},
	volume       = {
		39
	},
	publisher    = {Informa {UK} Limited},
	file = {
		references/10.1207%2Fs15327906mbr3901_4.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Monte-Carlo-Method
	},
	abstract     = {
		The most commonly used method to test an indirect effect is to divide the estimate of the indirect effect by its standard error and compare the resulting $z$ statistic with a critical value from the standard normal distribution.
		Confidence limits for the indirect effect are also typically based on critical values from the standard normal distribution.
		This article uses a simulation study to demonstrate that confidence limits are imbalanced because the distribution of the indirect effect is normal only in special cases.
		Two alternatives for improving the performance of confidence limits for the indirect effect are evaluated:
		(a) a method based on the distribution of the product of two normal random variables,
		and (b) resampling methods.
		In Study 1,
		confidence limits based on the distribution of the product are more accurate than methods based on an assumed normal distribution but confidence limits are still imbalanced.
		Study 2 demonstrates that more accurate confidence limits are obtained using resampling methods,
		with the bias-corrected bootstrap the best method overall.
	},
}

@Article{
	Lib-Mediation-Monte-Carlo-Method-Preacher-2012
	,
	author       = {
		Preacher, Kristopher J. 
		and 
		Selig, James P.
	},
	date         = {
		2012-04
	},
	journaltitle = {
		Communication Methods and Measures
	},
	title        = {
		Advantages of {Monte Carlo} confidence intervals for indirect effects
	},
	doi          = {
		10.1080/19312458.2012.679848
	},
	number       = {
		2
	},
	pages        = {
		77--98
	},
	volume       = {
		6
	},
	publisher    = {Informa {UK} Limited},  
	file = {
		references/10.1080%2F19312458.2012.679848.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Monte-Carlo-Method
	},
	abstract     = {
		Monte Carlo simulation is a useful but underutilized method of constructing confidence intervals for indirect effects in mediation analysis.
		The Monte Carlo confidence interval method has several distinct advantages over rival methods.
		Its performance is comparable to other widely accepted methods of interval construction,
		it can be used when only summary data are available,
		it can be used in situations where rival methods (e.g., bootstrapping and distribution of the product methods) are difficult or impossible,
		and it is not as computer-intensive as some other methods.
		In this study we discuss Monte Carlo confidence intervals for indirect effects,
		report the results of a simulation study comparing their performance to that of competing methods,
		demonstrate the method in applied examples,
		and discuss several software options for implementation in applied settings.
	},
}

@Article{
	Lib-Mediation-Monte-Carlo-Method-Hayes-2013
	,
	author       = {
		Hayes, Andrew F. 
		and 
		Scharkow, Michael
	},
	date         = {
		2013-08
	},
	journaltitle = {
		Psychological Science
	},
	title        = {
		The relative trustworthiness of inferential tests of the indirect effect in statistical mediation analysis
	},
	doi          = {
		10.1177/0956797613480187
	},
	number       = {
		10
	},
	pages        = {
		1918--1927
	},
	volume       = {
		24
	},
	publisher    = {
		{SAGE} Publications
	},
	file = {
		references/10.1177%2F0956797613480187.pdf
	},
	library = {},
	keywords = {
		mediation analysis,
		indirect effects,
		bootstrapping,
		Sobel test,
		statistical analyses,
		hypothesis testing
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Monte-Carlo-Method
	},
	abstract     = {
		A content analysis of 2 years of Psychological Science articles reveals inconsistencies in how researchers make inferences about indirect effects when conducting a statistical mediation analysis.
		In this study,
		we examined the frequency with which popularly used tests disagree,
		whether the method an investigator uses makes a difference in the conclusion he or she will reach,
		and whether there is a most trustworthy test that can be recommended to balance practical and performance considerations.
		We found that tests agree much more frequently than they disagree,
		but disagreements are more common when an indirect effect exists than when it does not.
		We recommend the bias-corrected bootstrap confidence interval as the most trustworthy test if power is of utmost concern,
		although it can be slightly liberal in some circumstances.
		Investigators concerned about Type I errors should choose the Monte Carlo confidence interval or the distribution-of-the-product approach,
		which rarely disagree.
		The percentile bootstrap confidence interval is a good compromise test.
	},
}

@Article{
	Lib-Mediation-Monte-Carlo-Method-Kisbu-Sakarya-2014
	,
	author       = {
		Kisbu-Sakarya, Yasemin 
		and 
		MacKinnon, David P. 
		and 
		Mio{\v{c}}evi{\'{c}}, Milica
	},
	date         = {
		2014-05
	},
	journaltitle = {
		Multivariate Behavioral Research
	},
	title        = {
		The distribution of the product explains normal theory mediation confidence interval estimation
	},
	doi          = {
		10.1080/00273171.2014.903162
	},
	number       = {
		3
	},
	pages        = {
		261--268
	},
	volume       = {
		49
	},
	publisher    = {
		Informa {UK} Limited
	},
	file = {
		references/10.1080%2F00273171.2014.903162.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Monte-Carlo-Method
	},
	abstract     = {
		The distribution of the product has several useful applications.
		One of these applications is its use to form confidence intervals for the indirect effect as the product of 2 regression coefficients.
		The purpose of this article is to investigate how the moments of the distribution of the product explain normal theory mediation confidence interval coverage and imbalance.
		Values of the critical ratio for each random variable are used to demonstrate how the moments of the distribution of the product change across values of the critical ratio observed in research studies. 
		Results of the simulation study showed that as skewness in absolute value increases,
		coverage decreases.
		And as skewness in absolute value and kurtosis increases,
		imbalance increases.
		The difference between testing the significance of the indirect effect using the normal theory versus the asymmetric distribution of the product is further illustrated with a real data example.
		This article is the first study to show the direct link between the distribution of the product and indirect effect confidence intervals and clarifies the results of previous simulation studies by showing why normal theory confidence intervals for indirect effects are often less accurate than those obtained from the asymmetric distribution of the product or from resampling methods.
	},
}

@Article{
	Lib-Mediation-Monte-Carlo-Method-Tofighi-2015
	,
	author       = {
		Tofighi, Davood 
		and 
		MacKinnon, David P.
	},
	date         = {
		2015-08
	},
	journaltitle = {
		Structural Equation Modeling: A Multidisciplinary Journal
	},
	title        = {
		{Monte Carlo} confidence intervals for complex functions of indirect effects
	},
	doi          = {
		10.1080/10705511.2015.1057284
	},
	number       = {
		2
	},
	pages        = {
		194--205
	},
	volume       = {
		23
	},
	publisher    = {
		Informa {UK} Limited
	},
	file = {
		references/10.1080%2F10705511.2015.1057284.pdf
	},
	library = {},
	keywords = {
		confidence interval,
		mediation analysis,
		Monte Carlo
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Monte-Carlo-Method
	},
	abstract     = {
		One challenge in mediation analysis is to generate a confidence interval (CI) with high coverage and power that maintains a nominal significance level for any well-defined function of indirect and direct effects in the general context of structural equation modeling (SEM).
		This study discusses a proposed Monte Carlo extension that finds the CIs for any well-defined function of the coefficients of SEM such as the product of k coefficients and the ratio of the contrasts of indirect effects,
		using the Monte Carlo method.
		Finally,
		we conduct a small-scale simulation study to compare CIs produced by the Monte Carlo,
		nonparametric bootstrap,
		and asymptotic-delta methods.
		Based on our simulation study,
		we recommend researchers use the Monte Carlo method to test a complex function of indirect effects.
	},
}

@Article{
	Lib-Mediation-Monte-Carlo-Method-Tofighi-2019
	,
	author       = {
		Tofighi, Davood 
		and 
		Kelley, Ken
	},
	date         = {
		2019-06
	},
	journaltitle = {
		Multivariate Behavioral Research
	},
	title        = {
		Indirect effects in sequential mediation models: Evaluating methods for hypothesis testing and confidence interval formation
	},
	doi          = {
		10.1080/00273171.2019.1618545
	},
	number       = {
		2
	},
	pages        = {
		188--210
	},
	volume       = {
		55
	},
	publisher    = {
		Informa {UK} Limited
	},
	file = {
		references/10.1080%2F00273171.2019.1618545.pdf
	},
	library = {},
	keywords = {
		Indirect effect,
		confidence interval,
		sequential mediation,
		Bayesian credible interval
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Monte-Carlo-Method
	},
	abstract     = {
		Complex mediation models,
		such as a two-mediator sequential model,
		have become more prevalent in the literature.
		To test an indirect effect in a two-mediator model,
		we conducted a large-scale Monte Carlo simulation study of the Type I error,
		statistical power,
		and confidence interval coverage rates of 10 frequentist and Bayesian confidence/credible intervals (CIs) for normally and nonnormally distributed data.
		The simulation included never-studied methods and conditions (e.g., Bayesian CI with flat and weakly informative prior methods,
		two model-based bootstrap methods,
		and two nonnormality conditions) as well as understudied methods (e.g., profile-likelihood, Monte Carlo with maximum likelihood standard error [MC-ML] and robust standard error [MC-Robust]).
		The popular BC bootstrap showed inflated Type I error rates and CI under-coverage.
		We recommend different methods depending on the purpose of the analysis.
		For testing the null hypothesis of no mediation,
		we recommend MC-ML, profile-likelihood, and two Bayesian methods.
		To report a CI,
		if data has a multivariate normal distribution,
		we recommend MC-ML,
		profile-likelihood,
		and the two Bayesian methods;
		otherwise,
		for multivariate nonnormal data we recommend the percentile bootstrap.
		We argue that the best method for testing hypotheses is not necessarily the best method for CI construction, which is consistent with the findings we present.
	},
}

@Article{
	Lib-Mediation-Monte-Carlo-Method-Yzerbyt-2018
	,
	author       = {
		Yzerbyt, Vincent
		and
		Muller, Dominique
		and
		Batailler, C{\'{e}}dric
		and
		Judd, Charles M.
	},
	date         = {
		2018-12
	},
	journaltitle = {
		Journal of Personality and Social Psychology
	},
	title        = {
		New recommendations for testing indirect effects in mediational models: The need to report and test component paths
	},
	doi          = {
		10.1037/pspa0000132
	},
	number       = {
		6
	},
	pages        = {
		929--943
	},
	volume       = {
		115
	},
	publisher    = {
		American Psychological Association ({APA})
	},
	file = {
		references/10.1037%2Fpspa0000132.pdf
	},
	library = {},
	keywords = {
		indirect effects, 
		mediation, 
		joint-significance, 
		bootstrap
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Monte-Carlo-Method
	},
	abstract     = {
		In light of current concerns with replicability and reporting false-positive effects in psychology,
		we examine Type I errors and power associated with 2 distinct approaches for the assessment of mediation,
		namely the component approach (testing individual parameter estimates in the model) and the index approach (testing a single mediational index).
		We conduct simulations that examine both approaches and show that the most commonly used tests under the index approach risk inflated Type I errors compared with the joint-significance test inspired by the component approach.
		We argue that the tendency to report only a single mediational index is worrisome for this reason and also because it is often accompanied by a failure to critically examine the individual causal paths underlying the mediational model.
		We recommend testing individual components of the indirect effect to argue for the presence of an indirect effect and then using other recommended procedures to calculate the size of that effect. 
		Beyond simple mediation,
		we show that our conclusions also apply in cases of within-participant mediation and moderated mediation.
		We also provide a new R-package that allows for an easy implementation of our recommendations.
	},
}

@Article{
	Lib-Mediation-Monte-Carlo-Method-Pesigan-2020
	,
	author       = {
		Pesigan, Ivan Jacob Agaloos
		and
		Cheung, Shu Fai
	},
	date         = {
		2020-12
	},
	journaltitle = {
		Frontiers in Psychology
	},
	title        = {
		{SEM}-based methods to form confidence intervals for indirect effect: Still applicable given nonnormality, under certain conditions
	},
	doi          = {
		10.3389/fpsyg.2020.571928
	},
	volume       = {
		11
	},
	publisher    = {
		Frontiers Media {SA}
	},  
	file = {
		references/10.3389%2Ffpsyg.2020.571928.pdf
	},
	library = {},
	keywords = {
		mediation, 
		nonnormal, 
		confidence interval, 
		structural equation modeling,
		bootstrapping
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Monte-Carlo-Method
	},
	abstract     = {
		A SEM-based approach using likelihood-based confidence interval (LBCI) has been proposed to form confidence intervals for unstandardized and standardized indirect effect in mediation models.
		However,
		when used with the maximum likelihood estimation,
		this approach requires that the variables are multivariate normally distributed.
		This can affect the LBCIs of unstandardized and standardized effect differently.
		In the present study,
		the robustness of this approach when the predictor is not normally distributed but the error terms are conditionally normal,
		which does not violate the distributional assumption of ordinary least squares (OLS) estimation,
		is compared to four other approaches:
		nonparametric bootstrapping,
		two variants of LBCI,
		LBCI assuming the predictor is fixed (LBCI-Fixed-X) and LBCI based on ADF estimation (LBCI-ADF),
		and Monte Carlo.
		A simulation study was conducted using a simple mediation model and a serial mediation model,
		manipulating the distribution of the predictor.
		The Monte Carlo method performed worst among the methods.
		LBCI and LBCI-Fixed-X had suboptimal performance when the distributions had high kurtosis and the population indirect effects were medium to large.
		In some conditions,
		the problem was severe even when the sample size was large.
		LBCI-ADF and nonparametric bootstrapping had coverage probabilities close to the nominal value in nearly all conditions,
		although the coverage probabilities were still suboptimal for the serial mediation model when the sample size was small with respect to the model.
		Implications of these findings in the context of this special case of nonnormal data were discussed.
	},
}

@Article{
	Lib-Mediation-Monte-Carlo-Method-Pesigan-2023
	,
	author = {
		Pesigan, Ivan Jacob Agaloos
		and
		Cheung, Shu Fai
	},
	date = {
		2023
	},
	journaltitle = {
		Behavior Research Methods
	},
	title = {
		{Monte Carlo} confidence intervals for the indirect effect with missing data
	},
	doi = {
		10.3758/s13428-023-02114-4
	},
	number = {
	},
	volume = {
	},
	publisher = {
		Informa {UK} Limited
	},
	file = {
		references/10.3758%2Fs13428-023-02114-4
	},
	library = {},
	keywords = {
		Monte Carlo method,
		nonparametric bootstrap,
		indirect effect,
		mediation,
		missing completely at random,
		missing at random,
		full-information maximum likelihood,
		multiple imputation
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Monte-Carlo-Method
	},
	abstract = {
	},
}

@Comment{jabref-meta: databaseType:biblatex;}
% Encoding: US-ASCII

@InBook{
	Lib-Mediation-Power-Hoyle-1999
	,
	author    = {
		Hoyle, Rick H.
		and
		Kenny, David A.
	},
	booktitle = {
		Statistical strategies for small sample research
	},
	date      = {
		1999
	},
	title     = {
		Sample Size, Reliability and Tests of Statistical Mediation
	},
	editor    = {
		Hoyle, Rick H.
	},
	isbn      = {
		9780761908869
	},
	pages     = {
		195--222
	},
	publisher = {
		Sage Publications
	},
	file = {},
	library = {
		HA29 .S7844 1999
	},
	keywords = {
		Social sciences--Statistical methods
	},
	addendum = {
		https://lccn.loc.gov/98043490
	},
	note = {},
	annotation = {
		Lib-Mediation-Power
	},
}

@Article{
	Lib-Mediation-Power-Fritz-2007
	,
	author       = {
		Fritz, Matthew S. 
		and 
		MacKinnon, David P.
	},
	date         = {
		2007-03
	},
	journaltitle = {
		Psychological Science
	},
	title        = {
		Required sample size to detect the mediated effect
	},
	doi          = {
		10.1111/j.1467-9280.2007.01882.x
	},
	number       = {
		3
	},
	pages        = {
		233--239
	},
	volume       = {
		18
	},
	publisher    = {
		{SAGE} Publications
	},
	file = {
		references/10.1111%2Fj.1467-9280.2007.01882.x.pdf
	},
	library = {},
	keywords = {
		bootstrap,
		collinearity,
		mediation analysis,
		power,
		tolerance
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Power
	},
	abstract     = {
		Mediation models are widely used, 
		and there are many tests of the mediated effect. 
		One of the most common questions that researchers have when planning mediation studies is, 
		``How many subjects do I need to achieve adequate power when testing for mediation?'' 
		This article presents the necessary sample sizes for six of the most common and the most recommended tests of mediation for various combinations of parameters, 
		to provide a guide for researchers when designing studies or applying for grants.
	},
}

@Article{
	Lib-Mediation-Power-Beasley-2013
	,
	author       = {
		Beasley, T. Mark
	},
	date         = {
		2013-08
	},
	journaltitle = {
		The Journal of Experimental Education
	},
	title        = {
		Tests of mediation: Paradoxical decline in statistical power as a function of mediator collinearity
	},
	doi          = {
		10.1080/00220973.2013.813360
	},
	number       = {
		3
	},
	pages        = {
		283--306
	},
	volume       = {
		82
	},
	publisher    = {
		Informa {UK} Limited
	},
	file = {
		references/10.1080%2F00220973.2013.813360.pdf
	},
	library = {},
	keywords = {
		bootstrap,
		collinearity,
		mediation analysis,
		power,
		tolerance
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Power
	},
	abstract     = {
		Increasing the correlation between the independent variable and the mediator ($a$ coefficient) increases the effect size ($ab$) for mediation analysis;
		however,
		increasing a by definition increases collinearity in mediation models.
		As a result,
		the standard error of product tests increase.
		The variance inflation caused by increases in a at some point outweighs the increase of the effect size ($ab$) and results in a loss of statistical power.
		This phenomenon also occurs with nonparametric bootstrapping approaches because the variance of the bootstrap distribution of $ab$ approximates the variance expected from normal theory. 
		Both variances increase dramatically when a exceeds the $b$ coefficient,
		thus explaining the power decline with increases in $a$.
		Implications for statistical analysis and applied researchers are discussed.
	},
}

@Article{
	Lib-Mediation-Power-Kenny-2013
	,
	author       = {
		Kenny, David A. 
		and 
		Judd, Charles M.
	},
	date         = {
		2013-12
	},
	journaltitle = {
		Psychological Science
	},
	title        = {
		Power anomalies in testing mediation
	},
	doi          = {
		10.1177/0956797613502676
	},
	number       = {
		2
	},
	pages        = {
		334--339
	},
	volume       = {
		25
	},
	publisher    = {
		{SAGE} Publications
	},
	file = {
		references/10.1177%2F0956797613502676.pdf
	},
	library = {},
	keywords = {
		causality, 
		hypothesis testing, 
		intervention, 
		power, 
		mediation
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Power
	},
	abstract     = {
		Two rather surprising anomalies relating to statistical power occur in testing mediation.
		First,
		in a model with no direct effect for which the total effect and indirect effect are identical,
		the power for the test of the total effect can be dramatically smaller than the power for the test of the indirect effect.
		Second,
		when there is a direct effect of a causal variable on the outcome controlling for the mediator,
		the power of the test of the indirect effect is often considerably greater than the power of the test of the direct effect,
		even when the two are of the same magnitude.
		We try to explain the reasons for these anomalies and how they affect practice.
	},
}

@Comment{jabref-meta: databaseType:biblatex;}
% Encoding: US-ASCII

@Article{
	Lib-Mediation-PRODCLIN-MacKinnon-2004
	,
	author       = {
		MacKinnon, David P.
		and
		Lockwood, Chondra M.
		and
		Williams, Jason
	},
	date         = {
		2004-01
	},
	journaltitle = {
		Multivariate Behavioral Research
	},
	title        = {
		Confidence limits for the indirect effect: Distribution of the product and resampling methods
	},
	doi          = {
		10.1207/s15327906mbr3901_4
	},
	number       = {
		1
	},
	pages        = {
		99--128
	},
	volume       = {
		39
	},
	publisher    = {Informa {UK} Limited},
	file = {
		references/10.1207%2Fs15327906mbr3901_4.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-PRODCLIN
	},
	abstract     = {
		The most commonly used method to test an indirect effect is to divide the estimate of the indirect effect by its standard error and compare the resulting $z$ statistic with a critical value from the standard normal distribution.
		Confidence limits for the indirect effect are also typically based on critical values from the standard normal distribution.
		This article uses a simulation study to demonstrate that confidence limits are imbalanced because the distribution of the indirect effect is normal only in special cases.
		Two alternatives for improving the performance of confidence limits for the indirect effect are evaluated:
		(a) a method based on the distribution of the product of two normal random variables,
		and (b) resampling methods.
		In Study 1,
		confidence limits based on the distribution of the product are more accurate than methods based on an assumed normal distribution but confidence limits are still imbalanced.
		Study 2 demonstrates that more accurate confidence limits are obtained using resampling methods,
		with the bias-corrected bootstrap the best method overall.
	},
}

@Article{
	Lib-Mediation-PRODCLIN-MacKinnon-2007
	,
	author       = {
		MacKinnon, David P. 
		and 
		Fritz, Matthew S. 
		and 
		Williams, Jason 
		and 
		Lockwood, Chondra M.
	},
	date         = {
		2007-08
	},
	journaltitle = {
		Behavior Research Methods
	},
	title        = {
		Distribution of the product confidence limits for the indirect effect: Program {PRODCLIN}
	},
	doi          = {
		10.3758/bf03193007
	},
	number       = {
		3
	},
	pages        = {
		384--389
	},
	volume       = {
		39
	},
	publisher    = {Springer Science and Business Media {LLC}},
	file = {
		references/10.1207%2Fs15327906mbr3901_4.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-PRODCLIN
	},
	abstract     = {
		This article describes a program, PRODCLIN (distribution of the PRODuct Confidence Limits for INdirect effects), written for SAS, SPSS, and R, that computes confidence limits for the product of two normal random variables. 
		The program is important because it can be used to obtain more accurate confidence limits for the indirect effect, as demonstrated in several recent articles (MacKinnon, Lockwood, \& Williams, 2004; Pituch, Whittaker, \& Stapleton, 2005). 
		Tests of the significance of and confidence limits for indirect effects based on the distribution of the product method have more accurate Type I error rates and more power than other, more commonly used tests. 
		Values for the two paths involved in the indirect effect and their standard errors are entered in the PRODCLIN program, and distribution of the product confidence limits are computed. 
		Several examples are used to illustrate the PRODCLIN program. 
		The PRODCLIN programs in rich text format may be downloaded from www.psychonomic.org/archive.
	},
}

@Comment{jabref-meta: databaseType:biblatex;}
% Encoding: US-ASCII

@Article{
	Lib-Mediation-Profile-Likelihood-Cheung-2007
	,
	author       = {
		Cheung, Mike W.-L.
	},
	date         = {
		2007-05
	},
	journaltitle = {
		Structural Equation Modeling: A Multidisciplinary Journal
	},
	title        = {
		Comparison of approaches to constructing confidence intervals for mediating effects using structural equation models
	},
	doi          = {
		10.1080/10705510709336745
	},
	number       = {
		2
	},
	pages        = {
		227--246
	},
	volume       = {
		14
	},
	publisher    = {
		Informa {UK} Limited
	},
	file = {
		references/10.1080%2F10705510709336745.pdf
	},
	library = {},
	keywords = {
		mediation, 
		bootstrapping
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Profile-Likelihood
	},
	abstract     = {
		Mediators are variables that explain the association between an independent variable and a dependent variable. 
		Structural equation modeling (SEM) is widely used to test models with mediating effects. 
		This article illustrates how to construct confidence intervals (CIs) of the mediating effects for a variety of models in SEM. 
		Specifically, mediating models with 1 mediator, 2 intermediate mediators, 2 specific mediators, and 1 mediator in 2 independent groups are illustrated. 
		By using phantom variables (Rindskopf, 1984), 
		a Wald CI, 
		percentile bootstrap CI, 
		bias-corrected bootstrap CI, 
		and a likelihood-based CI on the mediating effect are easily constructed with some existing SEM packages, 
		such as LISREL, Mplus, and Mx. 
		Monte Carlo simulation studies are used to compare the coverage probabilities of these CIs. 
		The results show that the coverage probabilities of these CIs are comparable when the mediating effect is large or when the sample size is large. 
		However, when the mediating effect and the sample size are both small, the bootstrap CI and likelihood-based CI are preferred over the Wald CI. 
		Extensions of this SEM approach for future research are discussed.
	},
}

@Article{
	Lib-Mediation-Profile-Likelihood-Cheung-2009a
	,
	author       = {
		Cheung, Mike W.-L.
	},
	date         = {
		2009-05
	},
	journaltitle = {
		Behavior Research Methods
	},
	title        = {
		Comparison of methods for constructing confidence intervals of standardized indirect effects
	},
	doi          = {
		10.3758/brm.41.2.425
	},
	number       = {
		2
	},
	pages        = {
		425--438
	},
	volume       = {
		41
	},
	publisher    = {
		Springer Science and Business Media {LLC}
	},
	file = {
		references/10.3758%2Fbrm.41.2.425.pdf
	},
	library = {},
	keywords = {
		mediation analysis,
		coverage probability,
		structural equation modeling approach
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Profile-Likelihood
	},
	abstract     = {
		Mediation models are often used as a means to explain the psychological mechanisms between an independent and a dependent variable in the behavioral and social sciences. 
		A major limitation of the unstandardized indirect effect calculated from raw scores is that it cannot be interpreted as an effect-size measure. 
		In contrast, the standardized indirect effect calculated from standardized scores can be a good candidate as a measure of effect size because it is scale invariant. 
		In the present article, 11 methods for constructing the confidence intervals (CIs) of the standardized indirect effects were evaluated via a computer simulation. 
		These included six Wald CIs, three bootstrap CIs, one likelihood-based CI, and the PRODCLIN CI. 
		The results consistently showed that the percentile bootstrap, the bias-corrected bootstrap, and the likelihood-based approaches had the best coverage probability. 
		Mplus, LISREL, and Mx syntax were included to facilitate the use of these preferred methods in applied settings. Future issues on the use of the standardized indirect effects are discussed.
	},
}

@Article{
	Lib-Mediation-Profile-Likelihood-Cheung-2009b
	,
	author       = {
	Cheung, Mike W.-L.
	},
	date         = {
	2009-04
	},
	journaltitle = {
	Structural Equation Modeling: A Multidisciplinary Journal
	},
	title        = {
	Constructing approximate confidence intervals for parameters with structural equation models
	},
	doi          = {
	10.1080/10705510902751291
	},
	number       = {
	2
	},
	pages        = {
	267--294
	},
	volume       = {
	16
	},
	publisher    = {
	Informa {UK} Limited
	},
	file = {
		references/10.1080%2F10705510902751291.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Profile-Likelihood
	},
	abstract     = {
Confidence intervals (CIs) for parameters are usually constructed based on the estimated standard errors. 
These are known as Wald CIs. 
This article argues that likelihood-based CIs (CIs based on likelihood ratio statistics) are often preferred to Wald CIs. 
It shows how the likelihood-based CIs and the Wald CIs for many statistics and psychometric indexes can be constructed with the use of phantom variables (Rindskopf, 1984) in some of the current structural equation modeling (SEM) packages. 
The procedures to form CIs for the differences in correlation coefficients, squared multiple correlations, indirect effects, coefficient alphas, and reliability estimates are illustrated. 
A simulation study on the Pearson correlation is used to demonstrate the advantages of the likelihood-based CI over the Wald CI. 
Issues arising from this SEM approach and extensions of this approach are discussed.
	},
}

@Article{
	Lib-Mediation-Profile-Likelihood-Pesigan-2020
	,
	author       = {
		Pesigan, Ivan Jacob Agaloos
		and
		Cheung, Shu Fai
	},
	date         = {
		2020-12
	},
	journaltitle = {
		Frontiers in Psychology
	},
	title        = {
		{SEM}-based methods to form confidence intervals for indirect effect: Still applicable given nonnormality, under certain conditions
	},
	doi          = {
		10.3389/fpsyg.2020.571928
	},
	volume       = {
		11
	},
	publisher    = {
		Frontiers Media {SA}
	},  
	file = {
		references/10.3389%2Ffpsyg.2020.571928.pdf
	},
	library = {},
	keywords = {
		mediation, 
		nonnormal, 
		confidence interval, 
		structural equation modeling,
		bootstrapping
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Mediation-Profile-Likelihood
	},
	abstract     = {
		A SEM-based approach using likelihood-based confidence interval (LBCI) has been proposed to form confidence intervals for unstandardized and standardized indirect effect in mediation models.
		However,
		when used with the maximum likelihood estimation,
		this approach requires that the variables are multivariate normally distributed.
		This can affect the LBCIs of unstandardized and standardized effect differently.
		In the present study,
		the robustness of this approach when the predictor is not normally distributed but the error terms are conditionally normal,
		which does not violate the distributional assumption of ordinary least squares (OLS) estimation,
		is compared to four other approaches:
		nonparametric bootstrapping,
		two variants of LBCI,
		LBCI assuming the predictor is fixed (LBCI-Fixed-X) and LBCI based on ADF estimation (LBCI-ADF),
		and Monte Carlo.
		A simulation study was conducted using a simple mediation model and a serial mediation model,
		manipulating the distribution of the predictor.
		The Monte Carlo method performed worst among the methods.
		LBCI and LBCI-Fixed-X had suboptimal performance when the distributions had high kurtosis and the population indirect effects were medium to large.
		In some conditions,
		the problem was severe even when the sample size was large.
		LBCI-ADF and nonparametric bootstrapping had coverage probabilities close to the nominal value in nearly all conditions,
		although the coverage probabilities were still suboptimal for the serial mediation model when the sample size was small with respect to the model.
		Implications of these findings in the context of this special case of nonnormal data were discussed.
	},
}

@Comment{jabref-meta: databaseType:biblatex;}
% Encoding: US-ASCII

@Book{
	Lib-Missing-Data-Books-Rubin-1987
	,
	author    = {
		Rubin, Donald B.
	},
	date      = {
		1987-06
	},
	title     = {
		Multiple imputation for nonresponse in surveys
	},
	doi       = {
		10.1002/9780470316696
	},
	isbn      = {
		9780470316696
	},
	location  = {
		New York
	},
	publisher = {
		John Wiley {\&} Sons, Inc.
	},
	file = {
		references/9780470316696.pdf
	},
	library = {
		HA31.2 .R83 1987
	},
	keywords = {
		Multiple imputation (Statistics),
		Nonresponse (Statistics),
		Social surveys--Response rate
	},
	addendum = {
		https://lccn.loc.gov/86028935
	},
	note = {},
	annotation = {
		Lib-Missing-Data-Books
	},
	abstract  = {
		Demonstrates how nonresponse in sample surveys and censuses can be handled by replacing each missing value with two or more multiple imputations. 
		Clearly illustrates the advantages of modern computing to such handle surveys, 
		and demonstrates the benefit of this statistical technique for researchers who must analyze them. 
		Also presents the background for Bayesian and frequentist theory. 
		After establishing that only standard complete-data methods are needed to analyze a multiply-imputed set, 
		the text evaluates procedures in general circumstances, 
		outlining specific procedures for creating imputations in both the ignorable and nonignorable cases. 
		Examples and exercises reinforce ideas, 
		and the interplay of Bayesian and frequentist ideas presents a unified picture of modern statistics.
	},
}

@Book{
	Lib-Missing-Data-Books-Schafer-1997
	,
	author    = {
		Schafer, Joseph L.
	},
	date      = {
		1997-08
	},
	title     = {
		Analysis of Incomplete Multivariate Data
	},
	doi       = {
		10.1201/9780367803025
	},
	isbn      = {
		9780367803025
	},
	publisher = {
		Chapman and Hall/CRC
	},
	file = {
		references/9780367803025.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Books
	},
	abstract  = {
		The last two decades have seen enormous developments in statistical methods for incomplete data. 
		The EM algorithm and its extensions, 
		multiple imputation, 
		and Markov Chain Monte Carlo provide a set of flexible and reliable tools from inference in large classes of missing-data problems. 
		Yet, 
		in practical terms, 
		those developments have had surprisingly little impact on the way most data analysts handle missing values on a routine basis.
				
		Analysis of Incomplete Multivariate Data helps bridge the gap between theory and practice, 
		making these missing-data tools accessible to a broad audience. 
		It presents a unified, 
		Bayesian approach to the analysis of incomplete multivariate data, 
		covering datasets in which the variables are continuous, 
		categorical, 
		or both. 
		The focus is applied, 
		where necessary, 
		to help readers thoroughly understand the statistical properties of those methods, 
		and the behavior of the accompanying algorithms.
				
		All techniques are illustrated with real data examples, 
		with extended discussion and practical advice. 
		All of the algorithms described in this book have been implemented by the author for general use in the statistical languages S and S Plus. 
		The software is available free of charge on the Internet.
	},
}

@Book{
	Lib-Missing-Data-Books-Little-2002
	,
	author    = {
		Little, Roderick J. A. 
		and 
		Rubin, Donald B.
	},
	date      = {
		2002-08
	},
	title     = {
		Statistical analysis with missing data
	},
	doi       = {
		10.1002/9781119013563
	},
	edition   = {
		2
	},
	isbn      = {
		9781119013563
	},
	publisher = {
		John Wiley {\&} Sons, Inc.
	},
	file = {
		references/9781119013563.pdf
	},
	library = {
		QA276 .L57 2002
	},
	keywords = {
		Mathematical statistics,
		Missing observations (Statistics)
	},
	addendum = {
		https://lccn.loc.gov/2002027006
	},
	note = {},
	annotation = {
		Lib-Missing-Data-Books
	},
	abstract  = {
		Statistical analysis of data sets with missing values is a pervasive problem for which standard methods are of limited value. 
		The first edition of Statistical Analysis with Missing Data has been a standard reference on missing-data methods. 
		Now, 
		reflecting extensive developments in Bayesian methods for simulating posterior distributions, 
		this Second Edition by two acknowledged experts on the subject offers a thoroughly up-to-date, 
		reorganized survey of current methodology for handling missing-data problems.
				
		Blending theory and application, 
		authors Roderick Little and Donald Rubin review historical approaches to the subject and describe rigorous yet simple methods for multivariate analysis with missing values. 
		They then provide a coherent theory for analysis of problems based on likelihoods derived from statistical models for the data and the missing-data mechanism and apply the theory to a wide range of important missing-data problems.
				
		The new edition now enlarges its coverage to include:
				
		\begin{itemize}
			\item Expanded coverage of Bayesian methodology, both theoretical and computational, and of multiple imputation
			\item Analysis of data with missing values where inferences are based on likelihoods derived from formal statistical models for the data-generating and missing-data mechanisms
			\item Applications of the approach in a variety of contexts including regression, factor analysis, contingency table analysis, time series, and sample survey inference
			\item Extensive references, examples, and exercises
		\end{itemize}
	},
}

@Book{
	Lib-Missing-Data-Books-Allison-2002
	,
	author    = {
		Allison, Paul
	},
	date      = {
		2002
	},
	title     = {
		Missing data
	},
	doi       = {
		10.4135/9781412985079
	},
	isbn      = {
		9781412985079
	},
	publisher = {
		{SAGE} Publications, Inc.
	},
	file = {
		references/9781412985079*.pdf
	},
	library = {
		QA276 .A55 2002
	},
	keywords = {
		Mathematical statistics,
		Missing observations (Statistics)
	},
	addendum = {
		https://lccn.loc.gov/2001001295
	},
	note = {},
	annotation = {
		Lib-Missing-Data-Books
	},
	abstract  = {
		Sooner or later anyone who does statistical analysis runs into problems with missing data in which information for some variables is missing for some cases. 
		Why is this a problem? Because most statistical methods presume that every case has information on all the variables to be included in the analysis. 
		Using numerous examples and practical tips, 
		this book offers a non-technical explanation of the standard methods for missing data (such as listwise or casewise deletion) as well as two newer (and, better) methods, 
		maximum likelihood and multiple imputation. 
		Anyone who has been relying on ad-hoc methods that are statistically inefficient or biased will find this book a welcome and accessible solution to their problems with handling missing data.
	},
}

@Book{
	Lib-Missing-Data-Books-McKnight-2007,
	author    = {
		McKnight, Patrick E. 
		and 
		McKnight, Katherine M. 
		and 
		Sidani, Souraya 
		and 
		Figueredo, Aurelio Jos{\'e}
	},
	date      = {
		2007-04-30
	},
	title     = {
		Missing data: A gentle introduction
	},
	isbn      = {
		9781593853945
	},
	pagetotal = {
		270
	},
	publisher = {
		Guilford Publications
	},
	file = {
		references/9781593853945.pdf
	},
	library = {
		H62 .M464 2007
	},
	keywords = {
		Social sciences--Research--Methodology,
		Missing observations (Statistics)
	},
	addendum = {
		https://lccn.loc.gov/2007001824
	},
	note = {},
	annotation = {
		Lib-Missing-Data-Books
	},
	abstract  = {
		While most books on missing data focus on applying sophisticated statistical techniques to deal with the problem after it has occurred, 
		this volume provides a methodology for the control and prevention of missing data. 
		In clear, 
		nontechnical language, 
		the authors help the reader understand the different types of missing data and their implications for the reliability, 
		validity, 
		and generalizability of a study’s conclusions. 
		They provide practical recommendations for designing studies that decrease the likelihood of missing data, 
		and for addressing this important issue when reporting study results. 
		When statistical remedies are needed--such as deletion procedures, 
		augmentation methods, 
		and single imputation and multiple imputation procedures--the book also explains how to make sound decisions about their use. 
		Patrick E. McKnight's website offers a periodically updated annotated bibliography on missing data and links to other Web resources that address missing data.
	},
}

@Book{
	Lib-Missing-Data-Books-Enders-2010
	,
	author    = {
		Enders, Craig K.
	},
	date      = {
		2010-05-31
	},
	title     = {
		Applied missing data analysis
	},
	isbn      = {
		9781606236390
	},
	pagetotal = {
		377
	},
	publisher = {
		Guilford Publications
	},
	file = {
		references/9781606236390.pdf
	},
	library = {
		HA29 .E497 2010
	},
	keywords = {
		Social sciences--Statistical methods,
		Missing observations (Statistics),
		Social sciences--Research--Methodology
	},
	addendum = {
		https://lccn.loc.gov/2010008465
	},
	note = {},
	annotation = {
		Lib-Missing-Data-Books
	},
	abstract  = {
		Walking readers step by step through complex concepts, 
		this book translates missing data techniques into something that applied researchers and graduate students can understand and utilize in their own research. 
		Enders explains the rationale and procedural details for maximum likelihood estimation, 
		Bayesian estimation, 
		multiple imputation, 
		and models for handling missing not at random (MNAR) data. 
		Easy-to-follow examples and small simulated data sets illustrate the techniques and clarify the underlying principles. 
		The companion website (www.appliedmissingdata.com) includes data files and syntax for the examples in the book as well as up-to-date information on software. 
		The book is accessible to substantive researchers while providing a level of detail that will satisfy quantitative specialists.
	},
}

@Book{
	Lib-Missing-Data-Books-Graham-2012
	,
	author    = {
		Graham, John W.
	},
	date      = {
		2012
	},
	title     = {
		Missing data
	},
	doi       = {
		10.1007/978-1-4614-4018-5
	},
	isbn      = {
		978-1-4614-4018-5
	},
	publisher = {
		Springer New York
	},
	file = {
		references/9781461440185.pdf
	},
	library = {
		QA276 .G7126 2012
	},
	keywords = {
		Missing observations (Statistics)
	},
	addendum = {
		https://lccn.loc.gov/2012938715
	},
	note = {},
	annotation = {
		Lib-Missing-Data-Books
	},
	abstract  = {
		Missing data have long plagued those conducting applied research in the social,
		behavioral, 
		and health sciences.
		Good missing data analysis solutions are available,
		but practical information about implementation of these solutions has been lacking.
		The objective of Missing Data: Analysis and Design is to enable investigators who are non-statisticians to implement modern missing data procedures properly in their research,
		and reap the benefits in terms of improved accuracy and statistical power.
				
		Missing Data: Analysis and Design contains essential information for both beginners and advanced readers.
		For researchers with limited missing data analysis experience,
		this book offers an easy-to-read introduction to the theoretical underpinnings of analysis of missing data;
		provides clear,
		step-by-step instructions for performing state-of-the-art multiple imputation analyses;
		and offers practical advice, 
		based on over 20 years' experience, for avoiding and troubleshooting problems.
		For more advanced readers, 
		unique discussions of attrition, 
		non-Monte-Carlo techniques for simulations involving missing data, 
		evaluation of the benefits of auxiliary variables, 
		and highly cost-effective planned missing data designs are provided.
				
		The author lays out missing data theory in a plain English style that is accessible and precise.
		Most analyses described in the book are conducted using the well-known statistical software packages SAS and SPSS,
		supplemented by Norm 2.03 and associated Java-based automation utilities.
		A related web site contains free downloads of the supplementary software,
		as well as sample empirical data sets and a variety of practical exercises described in the book to enhance and reinforce the reader's learning experience.
		Missing Data: Analysis and Design and its web site work together to enable beginners to gain confidence in their ability to conduct missing data analysis,
		and more advanced readers to expand their skill set.
	},
}

@Book{
	Lib-Missing-Data-Books-Kim-2013
	,
	author    = {
		Kim, Jae Kwang 
		and 
		Shao, Jun
	},
	date      = {
		2013-07
	},
	title     = {
		Statistical methods for handling incomplete data
	},
	doi       = {
		10.1201/b13981
	},
	isbn      = {
		9780429090653
	},
	publisher = {
		Chapman and Hall/{CRC}
	},
	file = {
		references/9780429090653.pdf
	},
	library = {
		QA276.8
	},
	keywords = {
		Missing observations (Statistics),
		Multiple imputation (Statistics)
	},
	addendum = {
		https://lccn.loc.gov/2021692680
	},
	note = {},
	annotation = {
		Lib-Missing-Data-Books
	},
	abstract  = {
		Along with many examples, 
		this text covers the most up-to-date statistical theories and computational methods for analyzing incomplete data.
		It presents a thorough treatment of statistical theories of likelihood-based inference with missing data.
		It also discusses numerous computational techniques and theories on imputation and extensively covers methods involving propensity score weighting,
		nonignorable missing data,
		longitudinal missing data,
		survey sampling,
		and statistical matching.
		Some of the research ideas introduced can be developed further for specific applications.
	},
}

@Book{
	Lib-Missing-Data-Books-Molenberghs-2014
	,
	author    = {
		Molenberghs, Geert 
		and 
		Fitzmaurice, Garrett 
		and 
		Kenward, Michael G. 
		and 
		Tsiatis, Anastasios
	},
	date      = {
		2014-11
	},
	title     = {
		Handbook of missing data methodology
	},
	doi       = {
		10.1201/b17622
	},
	isbn      = {
		9780429104770
	},
	publisher = {
		Chapman and Hall/{CRC}
	},
	file = {
		references/9780429104770.pdf
	},
	library = {
		QA276 .H3185 2015
	},
	keywords = {
		Missing observations (Statistics).
		Statistics--Methodology
	},
	addendum = {
		https://lccn.loc.gov/2014039480
	},
	note = {},
	annotation = {
		Lib-Missing-Data-Books
	},
	abstract  = {
		Missing data affect nearly every discipline by complicating the statistical analysis of collected data. 
		But since the 1990s, 
		there have been important developments in the statistical methodology for handling missing data. 
		Written by renowned statisticians in this area, 
		Handbook of Missing Data Methodology presents many methodological advances and the latest applications of missing data methods in empirical research.
				
		Divided into six parts, 
		the handbook begins by establishing notation and terminology. 
		It reviews the general taxonomy of missing data mechanisms and their implications for analysis and offers a historical perspective on early methods for handling missing data. 
		The following three parts cover various inference paradigms when data are missing,
		including likelihood and Bayesian methods; 
		semi-parametric methods, 
		with particular emphasis on inverse probability weighting; 
		and multiple imputation methods.
				
		The next part of the book focuses on a range of approaches that assess the sensitivity of inferences to alternative, 
		routinely non-verifiable assumptions about the missing data process. 
		The final part discusses special topics, such as missing data in clinical trials and sample surveys as well as approaches to model diagnostics in the missing data setting. 
		In each part, 
		an introduction provides useful background material and an overview to set the stage for subsequent chapters.
				
		Covering both established and emerging methodologies for missing data, 
		this book sets the scene for future research. 
		It provides the framework for readers to delve into research and practical applications of missing data methods.
	},
}

@Book{
	Lib-Missing-Data-Books-vanBuuren-2018
	,
	author    = {
		van Buuren, Stef
	},
	date      = {
		2018-07
	},
	title     = {
		Flexible imputation of missing data
	},
	doi       = {
		10.1201/9780429492259
	},
	edition   = {
		2
	},
	isbn      = {
		9780429492259
	},
	publisher = {
		Chapman and Hall/{CRC}
	},
	file = {
		references/9780429492259.pdf
	},
	library = {
		QA278
	},
	keywords = {
		Multivariate analysis,
		Multiple imputation (Statistics),
		Missing observations (Statistics)
	},
	addendum = {
		https://lccn.loc.gov/2019719619
	},
	note = {},
	annotation = {
		Lib-Missing-Data-Books
	},
	abstract  = {
		Missing data pose challenges to real-life data analysis. 
		Simple ad-hoc fixes, 
		like deletion or mean imputation, 
		only work under highly restrictive conditions, 
		which are often not met in practice. 
		Multiple imputation replaces each missing value by multiple plausible values. 
		The variability between these replacements reflects our ignorance of the true (but missing) value. 
		Each of the completed data set is then analyzed by standard methods, 
		and the results are pooled to obtain unbiased estimates with correct confidence intervals. 
		Multiple imputation is a general approach that also inspires novel solutions to old problems by reformulating the task at hand as a missing-data problem.
				
		This is the second edition of a popular book on multiple imputation, 
		focused on explaining the application of methods through detailed worked examples using the MICE package as developed by the author. 
		This new edition incorporates the recent developments in this fast-moving field.
				
		This class-tested book avoids mathematical and technical details as much as possible: 
		formulas are accompanied by verbal statements that explain the formula in accessible terms. 
		The book sharpens the reader’s intuition on how to think about missing data, 
		and provides all the tools needed to execute a well-grounded quantitative analysis in the presence of missing data.
	},
}

@Book{
	Lib-Missing-Data-Books-Little-2019
	,
	author    = {
		Little, Roderick J. A. 
		and 
		Rubin, Donald B.
	},
	date      = {
		2019-04
	},
	title     = {
		Statistical analysis with missing data
	},
	doi       = {
		10.1002/9781119482260
	},
	edition   = {
		3
	},
	isbn      = {
		9781119482260
	},
	publisher = {
		Wiley
	},
	file = {
		references/9781119482260.pdf
	},
	library = {
		QA276
	},
	keywords = {
		Mathematical statistics,
		Mathematical statistics--Problems, exercises, etc.,
		Missing observations (Statistics),
		Missing observations (Statistics)--Problems, exercises, etc.
	},
	addendum = {
		https://lccn.loc.gov/2018061330
	},
	note = {},
	annotation = {
		Lib-Missing-Data-Books
	},
	abstract  = {
		An up-to-date, comprehensive treatment of a classic text on missing data in statistics.
				
		The topic of missing data has gained considerable attention in recent decades. 
		This new edition by two acknowledged experts on the subject offers an up-to-date account of practical methodology for handling missing data problems. 
		Blending theory and application, 
		authors Roderick Little and Donald Rubin review historical approaches to the subject and describe simple methods for multivariate analysis with missing values. 
		They then provide a coherent theory for analysis of problems based on likelihoods derived from statistical models for the data and the missing data mechanism, 
		and then they apply the theory to a wide range of important missing data problems.
				
		Statistical Analysis with Missing Data, 
		Third Edition starts by introducing readers to the subject and approaches toward solving it. 
		It looks at the patterns and mechanisms that create the missing data, 
		as well as a taxonomy of missing data. 
		It then goes on to examine missing data in experiments, 
		before discussing complete-case and available-case analysis, 
		including weighting methods. 
		The new edition expands its coverage to include recent work on topics such as nonresponse in sample surveys, 
		causal inference, 
		diagnostic methods, 
		and sensitivity analysis, 
		among a host of other topics.
				
		\begin{itemize}
			\item An updated ``classic'' written by renowned authorities on the subject
			\item Features over 150 exercises (including many new ones)
			\item Covers recent work on important methods like multiple imputation, robust alternatives to weighting, and Bayesian methods
			\item Revises previous topics based on past student feedback and class experience
			\item Contains an updated and expanded bibliography
		\end{itemize}
				
		The authors were awarded The Karl Pearson Prize in 2017 by the International Statistical Institute, 
		for a research contribution that has had profound influence on statistical theory, 
		methodology or applications. 
		Their work ``has been no less than defining and transforming.'' (ISI)
				
		Statistical Analysis with Missing Data, 
		Third Edition is an ideal textbook for upper undergraduate and/or beginning graduate level students of the subject. 
		It is also an excellent source of information for applied statisticians and practitioners in government and industry.
	},
}

@Comment{jabref-meta: databaseType:biblatex;}
% Encoding: US-ASCII

@Book{
	Lib-Missing-Data-Multiple-Imputation-Rubin-1987
	,
	author    = {
		Rubin, Donald B.
	},
	date      = {
		1987-06
	},
	title     = {
		Multiple imputation for nonresponse in surveys
	},
	doi       = {
		10.1002/9780470316696
	},
	isbn      = {
		9780470316696
	},
	publisher = {
		John Wiley {\&} Sons, Inc.
	},
	file = {
		references/9780470316696.pdf
	},
	library = {
		HA31.2 .R83 1987
	},
	keywords = {
		Multiple imputation (Statistics),
		Nonresponse (Statistics),
		Social surveys--Response rate
	},
	addendum = {
		https://lccn.loc.gov/86028935
	},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
	abstract  = {
		Demonstrates how nonresponse in sample surveys and censuses can be handled by replacing each missing value with two or more multiple imputations. 
		Clearly illustrates the advantages of modern computing to such handle surveys, and demonstrates the benefit of this statistical technique for researchers who must analyze them. 
		Also presents the background for Bayesian and frequentist theory. 
		After establishing that only standard complete-data methods are needed to analyze a multiply-imputed set, the text evaluates procedures in general circumstances, outlining specific procedures for creating imputations in both the ignorable and nonignorable cases. 
		Examples and exercises reinforce ideas, and the interplay of Bayesian and frequentist ideas presents a unified picture of modern statistics.
	},
}

@InProceedings{
	Lib-Missing-Data-Multiple-Imputation-Heitjan-1988
	,
	author    = {
		Heitjan, Daniel F.
		and
		Little, Roderick J. A.
	},
	booktitle = {
		JSM Proceedings, Survey Research Methods Section
	},
	date      = {
		1988
	},
	title     = {
		Multiple imputation for the fatal accident reporting system
	},
	location  = {
		Alexandria, VA
	},
	pages     = {
		79--84
	},
	publisher = {
		American Statistical Association
	},
	url       = {http://www.asasrms.org/Proceedings/papers/1988_018.pdf},
	file = {
		references/Heitjan-1988.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
	abstract  = {},
}

@InProceedings{
	Lib-Missing-Data-Multiple-Imputation-Rubin-1988
	,
	author    = {
		Rubin, Donald B.
	},
	booktitle = {
		JSM Proceedings, Survey Research Methods Section
	},
	date      = {
		1988
	},
	title     = {
		An overview of multiple imputation
	},
	location  = {
		Alexandria, VA
	},
	pages     = {
		79--84
	},
	publisher = {
		American Statistical Association
	},
	url       = {http://www.asasrms.org/Proceedings/papers/1988_016.pdf},
	file = {
		references/Rubin-1988.pdf
	},
	library = {},
	keywords  = {
		data-base construction, 
		missing data, 
		nonresponse, 
		public-use files, 
		survey methods
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
	abstract  = {		
		Multiple imputation for nonresponse in public-use files replaces each missing value by two or more plausible values. 
		The values can be chosen to represent both uncertainty about which values to impute assuming the reasons for nonresponse are known and uncertainty about the reasons for nonresponse. 
		The theoretical underpinnings and several examples are given in Rubin (1987). 
		This presentation illustrates the dramatic improvements possible when using multiple rather than single imputation and provides a brief overview of current technology and lacunae that, hopefully, will be addressed and filled by current research efforts. 
		The two important applications of multiple imputation that this overview introduces, demonstrate the substantial improvements that can accrue from the straightforward use of multiple imputation in practice.
	},
}

@InProceedings{
	Lib-Missing-Data-Multiple-Imputation-Schenker-1988
	,
	author    = {
		Schenker, Nathaniel
		and
		Treiman, Donald J.
		and
		Weidman, Lynn
	},
	booktitle = {
		JSM Proceedings, Survey Research Methods Section
	},
	date      = {
		1988
	},
	title     = {
		Multiple imputation of industry and occupation codes for public-use files
	},
	location  = {
		Alexandria, VA
	},
	pages     = {
		79--84
	},
	publisher = {
		American Statistical Association
	},
	url       = {http://www.asasrms.org/Proceedings/papers/1988_017.pdf},
	file = {
		references/Schenker-1988.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
}

@Article{
	Lib-Missing-Data-Multiple-Imputation-Li-1991
	,
	author       = {
		Li, K. H. 
		and 
		Raghunathan, Trivellore Eachambadi
		and 
		Rubin, Donald B. 
	},
	date         = {
		1991-12
	},
	journaltitle = {
		Journal of the American Statistical Association
	},
	title        = {
		Large-sample significance levels from multiply imputed data using moment-based statistics and an {$F$} reference distribution
	},
	doi          = {
		10.1080/01621459.1991.10475152
	},
	number       = {
		416
	},
	pages        = {
		1065--1073
	},
	volume       = {
		86
	},
	publisher    = {
		Informa {UK} Limited
	},
	file = {
		references/10.1080%2F01621459.1991.10475152.pdf
	},
	library = {},
	keywords = {
		Imputation,
		Missing data,
		Nonresponse,
		Tests of significance
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
	abstract     = {
		We present a procedure for computing significance levels from data sets whose missing values have been multiply imputed data. 
		This procedure uses moment-based statistics, $m \leq 3$ repeated imputations, and an F reference distribution. 
		When $m = \infty$, we show first that our procedure is essentially the same as the ideal procedure in cases of practical importance and, second, that its deviations from the ideal are basically a function of the coefficient of variation of the canonical ratios of complete to observed information. 
		For small $m$ our procedure's performance is largely governed by this coefficient of variation and the mean of these ratios. 
		Using simulation techniques with small $m$, we compare our procedure's actual and nominal large-sample significance levels and conclude that it is essentially calibrated and thus represents a definite improvement over previously available procedures. 
		Furthermore, we compare the large-sample power of the procedure as a function of $m$ and other factors, such as the dimensionality of the estimand and fraction of missing information, to provide guidance on the choice of the number of imputations; generally, we find the loss of power due to small $m$ to be quite modest in cases likely to occur in practice.
	},
}

@Article{
	Lib-Missing-Data-Multiple-Imputation-Rubin-1991
	,
	author       = {
		Rubin, Donald B. 
		and 
		Schenker, Nathaniel
	},
	date         = {
		1991-04
	},
	journaltitle = {
		Statistics in Medicine
	},
	title        = {
		Multiple imputation in health-are databases: An overview and some applications
	},
	doi          = {
		10.1002/sim.4780100410
	},
	number       = {
		4
	},
	pages        = {
		585--598
	},
	volume       = {
		10
	},
	publisher    = {
		Wiley
	},
	file = {
		references/10.1002%2Fsim.4780100410.pdf
	},
	library = {},
	keywords = {	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
	abstract     = {
		Multiple imputation for non-response replaces each missing value by two or more plausible values. 
		The values can be chosen to represent both uncertainty about the reasons for non-response and uncertainty about which values to impute assuming the reasons for non-response are known. 
		This paper provides an overview of methods for creating and analysing multiply-imputed data sets, and illustrates the dramatic improvements possible when using multiple rather than single imputation. 
		A major application of multiple imputation to public-use files from the 1970 census is discussed, and several exploratory studies related to health care that have used multiple imputation are described.
	},
}

@Article{
	Lib-Missing-Data-Multiple-Imputation-Meng-1994a
	,
	author       = {
		Meng, Xiao-Li
	},
	date         = {
		1994-11
	},
	journaltitle = {
		Statistical Science
	},
	title        = {
		Multiple-imputation inferences with uncongenial sources of input
	},
	doi          = {
		10.1214/ss/1177010269
	},
	number       = {
		4
	},
	volume       = {
		9
	},
	publisher    = {
		Institute of Mathematical Statistics
	},  
	file = {
		references/10.1214%2Fss%2F1177010269.pdf
	},
	library = {},
	keywords = {
		Congeniality, 
		importance sampling, 
		incomplete data, 
		missing data, 
		nonresponse, 
		Normalizing constants, 
		public-use data file, 
		Randomization, 
		self-efficiency 
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
	abstract     = {
		Conducting sample surveys, imputing incomplete observations, and analyzing the resulting data are three indispensable phases of modern practice with public-use data files and with many other statistical applications.
		Each phase inherits different input, including the information preceding it and the intellectual assessments available, and aims to provide output that is one step closer to arriving at statistical inferences with scientific relevance. 
		However, the role of the imputation phase has often been viewed as merely providing computational convenience for users of data. Although facilitating computation is very important, such a viewpoint ignores the imputer's assessments and information inaccessible to the users. 
		This view underlies the recent controversy over the validity of multiple-imputation inference when a procedure for analyzing multiply imputed data sets cannot be derived from (is ``uncongenial'' to) the model adopted for multiple imputation. Given sensible imputations and complete-data analysis procedures, inferences from standard multiple-imputation combining rules are typically superior to, and thus different from, users' incomplete-data analyses. 
		The latter may suffer from serious nonresponse biases because such analyses often must rely on convenient but unrealistic assumptions about the nonresponse mechanism. 
		When it is desirable to conduct inferences under models for nonresponse other than the original imputation model, a possible alternative to recreating imputations is to incorporate appropriate importance weights into the standard combining rules. 
		These points are reviewed and explored by simple examples and general theory, from both Bayesian and frequentist perspectives, particularly from the randomization perspective. 
		Some convenient terms are suggested for facilitating communication among researchers from different perspectives when evaluating multiple-imputation inferences with uncongenial sources of input.
	},
}

@Article{
	Lib-Missing-Data-Multiple-Imputation-Fay-1994
	,
	author       = {
		Fay, Robert E.
	},
	date         = {
		1994-11
	},
	journaltitle = {
		Statistical Science
	},
	title        = {
		[Multiple-imputation inferences with uncongenial sources of input]: Comment
	},
	doi          = {
		10.1214/ss/1177010270
	},
	number       = {
		4
	},
	volume       = {
		9
	},
	publisher    = {
		Institute of Mathematical Statistics
	},
	file = {
		references/10.1214%2Fss%2F1177010270.pdf
	},
	library = {},
	keywords = {	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
}

@Article{
	Lib-Missing-Data-Multiple-Imputation-Schafer-1994
	,
	author       = {
		Schafer, Joseph L.
	},
	date         = {
		1994-11
	},
	journaltitle = {
		Statistical Science
	},
	title        = {
		[Multiple-imputation inferences with uncongenial sources of input]: Comment
	},
	doi          = {
		10.1214/ss/1177010271
	},
	number       = {
		4
	},
	volume       = {
		9
	},
	publisher    = {
		Institute of Mathematical Statistics
	},
	file = {
		references/10.1214%2Fss%2F1177010271.pdf
	},
	library = {},
	keywords = {	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
}

@Article{
	Lib-Missing-Data-Multiple-Imputation-Skinner-1994
	,
	author       = {
		Skinner, Chris
	},
	date         = {
		1994-11
	},
	journaltitle = {
		Statistical Science
	},
	title        = {
		[Multiple-imputation inferences with uncongenial sources of input]: Comment
	},
	doi          = {
		10.1214/ss/1177010272
	},
	number       = {
		4
	},
	volume       = {
		9
	},
	publisher    = {
		Institute of Mathematical Statistics
	},
	file = {
		references/10.1214%2Fss%2F1177010272.pdf
	},
	library = {},
	keywords = {	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
}

@Article{
	Lib-Missing-Data-Multiple-Imputation-Zaslavsky-1994
	,
	author       = {
		Zaslavsky, Alan M.
	},
	date         = {
		1994-11
	},
	journaltitle = {
		Statistical Science
	},
	title        = {
		[Multiple-imputation inferences with uncongenial sources of input]: Comment: Using the full toolkit
	},
	doi          = {
		10.1214/ss/1177010273
	},
	number       = {
		4
	},
	volume       = {
		9
	},
	publisher    = {
		Institute of Mathematical Statistics
	},
	file = {
		references/10.1214%2Fss%2F1177010273.pdf
	},
	library = {},
	keywords = {	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
}

@Article{
	Lib-Missing-Data-Multiple-Imputation-Meng-1994b
	,
	author       = {
		Meng, Xiao-Li
	},
	date         = {
		1994-11
	},
	journaltitle = {
		Statistical Science
	},
	title        = {
		[Multiple-imputation inferences with uncongenial sources of input]: Rejoinder
	},
	doi          = {
		10.1214/ss/1177010274
	},
	number       = {
		4
	},
	volume       = {
		9
	},
	publisher    = {
		Institute of Mathematical Statistics
	},
	file = {
		references/10.1214%2Fss%2F1177010274.pdf
	},
	library = {},
	keywords = {	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
}

@Article{
	Lib-Missing-Data-Multiple-Imputation-Rubin-1996
	,
	author       = {
		Rubin, Donald B.
	},
	date         = {
		1996-06
	},
	journaltitle = {
		Journal of the American Statistical Association
	},
	title        = {
		Multiple imputation after 18+ years
	},
	doi          = {
		10.1080/01621459.1996.10476908
	},
	number       = {
		434
	},
	pages        = {
		473--489
	},
	volume       = {
		91
	},
	publisher    = {
		Informa {UK} Limited
	},
	file = {
		references/10.1080%2F01621459.1996.10476908.pdf
	},
	library = {},
	keywords = {
		Confidence validity,
		Missing data,
		Nonresponse in surveys,
		Public-use files,
		Sample surveys,
		Superefficient procedures
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
	abstract     = {
		Multiple imputation was designed to handle the problem of missing data in public-use data bases where the data-base constructor and the ultimate user are distinct entities. 
		The objective is valid frequency inference for ultimate users who in general have access only to complete-data software and possess limited knowledge of specific reasons and models for nonresponse. 
		For this situation and objective, I believe that multiple imputation by the data-base constructor is the method of choice. 
		This article first provides a description of the assumed context and objectives, and second, reviews the multiple imputation framework and its standard results. 
		These preliminary discussions are especially important because some recent commentaries on multiple imputation have reflected either misunderstandings of the practical objectives of multiple imputation or misunderstandings of fundamental theoretical results. 
		Then, criticisms of multiple imputation are considered, and, finally, comparisons are made to alternative strategies.
	},
}

@Article{
	Lib-Missing-Data-Multiple-Imputation-Schenker-1996
	,
	author       = {
		Schenker, Nathaniel 
		and 
		Taylor, Jeremy M. G.
	},
	date         = {
		1996-08
	},
	journaltitle = {
		Computational Statistics \& Data Analysis
	},
	title        = {
		Partially parametric techniques for multiple imputation
	},
	doi          = {
		10.1016/0167-9473(95)00057-7
	},
	number       = {
		4
	},
	pages        = {
		425--446
	},
	volume       = {
		22
	},
	publisher    = {
		Elsevier {BV}
	},
	file = {
		references/10.1016%2F0167-9473(95)00057-7.pdf
	},
	library = {},
	keywords = {
		Hot deck,
		Incubation period of AIDS,
		Missing data,
		Nearest neighbor matching,
		Nonresponse,
		Predictive mean matching,
		Robustness
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
	abstract     = {
		Multiple imputation is a technique for handling data sets with missing values. The method fills in the missing values several times, creating several completed data sets for analysis. 
		Each data set is analyzed separately using techniques designed for complete data, and the results are then combined in such a way that the variability due to imputation may be incorporated. 
		Methods of imputing the missing values can vary from fully parametric to nonparametric. 
		In this paper, we compare partially parametric and fully parametric regression-based multiple-imputation methods. 
		The fully parametric method that we consider imputes missing regression outcomes by drawing them from their predictive distribution under the regression model, whereas the partially parametric methods are based on imputing outcomes or residuals for incomplete cases using values drawn from the complete cases. 
		For the partially parametric methods, we suggest a new approach to choosing complete cases from which to draw values. 
		In a Monte Carlo study in the regression setting, we investigate the robustness of the multiple-imputation schemes to misspecification of the underlying model for the data. 
		Sources of model misspecification considered include incorrect modeling of the mean structure as well as incorrect specification of the error distribution with regard to heaviness of the tails and heteroscedasticity. 
		The methods are compared with respect to the bias and efficiency of point estimates and the coverage rates of confidence intervals for the marginal mean and distribution function of the outcome. 
		We find that when the mean structure is specified correctly, all of the methods perform well, even if the error distribution is misspecified. 
		The fully parametric approach, however, produces slightly more efficient estimates of the marginal distribution function of the outcome than do the partially parametric approaches. 
		When the mean structure is misspecified, all of the methods still perform well for estimating the marginal mean, although the fully parametric method shows slight increases in bias and variance. 
		For estimating the marginal distribution function, however, the fully parametric method breaks down in several situations, whereas the partially parametric methods maintain their good performance. 
		In an application to AIDS research in a setting that is similar to although slightly more complicated than that of the Monte Carlo study, we examine how estimates for the distribution of the time from infection with HIV to the onset of AIDS vary with the method used to impute the residual time to AIDS for subjects with right-censored data. 
		The fully parametric and partially parametric techniques produce similar results, suggesting that the model selection used for fully parametric imputation was adequate. 
		Our application provides an example of how multiple imputation can be used to combine information from two cohorts to estimate quantities that cannot be estimated directly from either one of the cohorts separately.
	},
}

@Article{
	Lib-Missing-Data-Multiple-Imputation-Schafer-1998
	,
	author       = {
		Schafer, Joseph L. 
		and 
		Olsen, Maren K.
	},
	date         = {
		1998-10
	},
	journaltitle = {
		Multivariate Behavioral Research
	},
	title        = {
		Multiple imputation for multivariate missing-data Problems: A data analyst's perspective
	},
	doi          = {
		10.1207/s15327906mbr3304_5
	},
	number       = {
		4
	},
	pages        = {
		545--571
	},
	volume       = {
		33
	},
	publisher    = {
		Informa {UK} Limited
	},
	file = {
		references/10.1207%2Fs15327906mbr3304_5.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
	abstract     = {
		Analyses of multivariate data are frequently hampered by missing values. 
		Until recently, the only missing-data methods available to most data analysts have been relatively ad1 hoc practices such as listwise deletion. 
		Recent dramatic advances in theoretical and computational statistics, however, have produced anew generation of flexible procedures with a sound statistical basis. 
		These procedures involve multiple imputation (Rubin, 1987), a simulation technique that replaces each missing datum with a set of m > 1 plausible values. 
		The m versions of the complete data are analyzed by standard complete-data methods, and the results are combined using simple rules to yield estimates, standard errors, and p-values that formally incorporate missing-data uncertainty. 
		New computational algorithms and software described in a recent book (Schafer, 1997a) allow us to create proper multiple imputations in complex multivariate settings. 
		This article reviews the key ideas of multiple imputation, discusses the software programs currently available, and demonstrates their use on data from the Adolescent Alcohol Prevention Trial (Hansen \& Graham, 1991).
	},
}

@Article{
	Lib-Missing-Data-Multiple-Imputation-Schafer-1999
	,
	author       = {
		Schafer, Joseph L.
	},
	date         = {
		1999-02
	},
	journaltitle = {
		Statistical Methods in Medical Research
	},
	title        = {
		Multiple imputation: A primer
	},
	doi          = {
		10.1177/096228029900800102
	},
	number       = {
		1
	},
	pages        = {
		3--15
	},
	volume       = {
		8
	},
	publisher    = {
		{SAGE} Publications
	},
	file = {
		references/10.1177%2F096228029900800102.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
	abstract     = {
		In recent years, multiple imputation has emerged as a convenient and flexible paradigm for analysing data with missing values. 
		Essential features of multiple imputation are reviewed, with answers to frequently asked questions about using the method in practice.
	},
}

@Article{
	Lib-Missing-Data-Multiple-Imputation-Allison-2000
	,
	author       = {
		Allison, Paul D.
	},
	date         = {
		2000-02
	},
	journaltitle = {
		Sociological Methods \& Research
	},
	title        = {
		Multiple imputation for missing data: A cautionary tale
	},
	doi          = {
		10.1177/0049124100028003003
	},
	number       = {
		3
	},
	pages        = {
		301--309
	},
	volume       = {
		28
	},
	publisher    = {
		{SAGE} Publications
	},
	file = {
		references/10.1177%2F0049124100028003003.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
	abstract     = {
		Two algorithms for producing multiple imputations for missing data are evaluated with simulated data. 
		Software using a propensity score classifier with the approximate Bayesian bootstrap produces badly biased estimates of regression coefficients when data on predictor variables are missing at random or missing completely at random. 
		On the other hand, a regression-based method employing the data augmentation algorithm produces estimates with little or no bias.
	},
}

@Article{
	Lib-Missing-Data-Multiple-Imputation-King-2001
	,
	author       = {
		King, Gary 
		and 
		Honaker, James 
		and 
		Joseph, Anne 
		and 
		Scheve, Kenneth
	},
	date         = {
		2001-03
	},
	journaltitle = {
		American Political Science Review
	},
	title        = {
		Analyzing incomplete political science data: An alternative algorithm for multiple imputation
	},
	doi          = {
		10.1017/s0003055401000235
	},
	number       = {
		1
	},
	pages        = {
		49--69
	},
	volume       = {
		95
	},
	publisher    = {
		Cambridge University Press ({CUP})
	},
	file = {
		references/10.1017%2Fs0003055401000235.pdf
	},
	library = {},
	keywords = {
		item nonresponse,
		Missing at random,
		Multiple imputation,
		Nonignorable missing mechanism,
		Regression,
		Sampling properties and simulations
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
	abstract     = {
		We propose a remedy for the discrepancy between the way political scientists analyze data with missing values and the recommendations of the statistics community. 
		Methodologists and statisticians agree that ``multiple imputation'' is a superior approach to the problem of missing data scattered through one's explanatory and dependent variables than the methods currently used in applied data analysis.
		The discrepancy occurs because the computational algorithms used to apply the best multiple imputation models have been slow, difficult to implement, impossible to run with existing commercial statistical packages, and have demanded considerable expertise.
		We adapt an algorithm and use it to implement a general-purpose, multiple imputation model for missing data.
		This algorithm is considerably faster and easier to use than the leading method recommended in the statistics literature.
		We also quantify the risks of current missing data practices, illustrate how to use the new procedure, and evaluate this alternative through simulated data as well as actual empirical examples.
		Finally, we offer easy-to-use software that implements all methods discussed.
	},
}

@Article{
	Lib-Missing-Data-Multiple-Imputation-Raghunathan-2001
	,
	author       = {
		Raghunathan, Trivellore E.
		and
		Lepkowski, James M.
		and
		Hoewyk, John Van
		and
		Solenberger, Peter
	},
	date         = {
		2001
	},
	journaltitle = {
		Survey Methodology
	},
	title        = {
		A multivariate technique for multiply imputing missing values using a sequence of regression models
	},
	doi          = {},
	number       = {
		1
	},
	pages        = {
		85--95
	},
	volume       = {
		27
	},
	publisher    = {},
	file = {
		references/Raghunathan-2001.pdf
	},
	library = {},
	keywords = {
		item nonresponse,
		Missing at random,
		Multiple imputation,
		Nonignorable missing mechanism,
		Regression,
		Sampling properties and simulations
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
	abstract     = {
		This article describes and evaluates a procedure for imputing missing values for a relatively complex data structure when the data are missing at random. 
		The imputations are obtained by fitting a sequence of regression models and drawing values from the corresponding predictive distributions. 
		The types of regression models used are linear, logistic, Poisson, generalized logit or a mixture of these depending on the type of variable being imputed. 
		Two additional common features in the imputation process are incorporated: restriction to a relevant subpopulation for some variables and logical bounds or constraints for the imputed values. 
		The restrictions involve subsetting the sample individuals that satisfy certain criteria while fitting the regression models. 
		The bounds involve drawing values from a truncated predictive distribution. 
		The development of this method was partly motivated by the analysis of two data sets which are used as illustrations. 
		The sequential regression procedure is applied to perform multiple imputation analysis for the two applied problems. 
		The sampling properties of inferences from multiply imputed data sets created using the sequential regression method are
		evaluated through simulated data sets.
	},
}

@Article{
	Lib-Missing-Data-Multiple-Imputation-Sinharay-2001
	,
	author       = {
		Sinharay, Sandip 
		and 
		Stern, Hal S. 
		and 
		Russell, Daniel
	},
	date         = {
		2001
	},
	journaltitle = {
		Psychological Methods
	},
	title        = {
		The use of multiple imputation for the analysis of missing data
	},
	doi          = {
		10.1037/1082-989x.6.4.317
	},
	number       = {
		4
	},
	pages        = {
		317--329
	},
	volume       = {
		6
	},
	publisher    = {
		American Psychological Association ({APA})
	},
	file = {
		references/10.1037%2F1082-989x.6.4.317.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
	abstract     = {
		This article provides a comprehensive review of multiple imputation (MI), a technique for analyzing data sets with missing values. 
		Formally, MI is the process of replacing each missing data point with a set of m > 1 plausible values to generate m complete data sets. 
		These complete data sets are then analyzed by standard statistical software, and the results combined, to give parameter estimates and standard errors that take into account the uncertainty due to the missing data values. 
		This article introduces the idea behind MI, discusses the advantages of MI over existing techniques for addressing missing data, describes how to do MI for real problems, reviews the software available to implement MI, and discusses the results of a simulation study aimed at finding out how assumptions regarding the imputation model affect the parameter estimates provided by MI.
	},
}

@Article{
	Lib-Missing-Data-Multiple-Imputation-Royston-2004
	,
	author       = {
		Royston, Patrick
	},
	date         = {
		2004-08
	},
	journaltitle = {
		The Stata Journal: Promoting communications on statistics and Stata
	},
	title        = {
		Multiple imputation of missing values
	},
	doi          = {
		10.1177/1536867x0400400301
	},
	number       = {
		3
	},
	pages        = {
		227--241
	},
	volume       = {
		4
	},
	publisher    = {
		{SAGE} Publications
	},
	file = {
		references/10.1177%2F1536867x0400400301.pdf
	},
	library = {},
	keywords = {
		st0067, 
		mvis, 
		uvis, 
		micombine, 
		mijoin, 
		misplit, 
		missing data, 
		missing at random,
		multiple imputation, 
		multivariate imputation, 
		regression modeling
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
	abstract     = {
		Following the seminal publications of Rubin about thirty years ago, statisticians have become increasingly aware of the inadequacy of ``complete-case'' analysis of datasets with missing observations. 
		In medicine, for example, observations may be missing in a sporadic way for different covariates, and a complete-case analysis may omit as many as half of the available cases. Hotdeck imputation was implemented in Stata in 1999 by Mander and Clayton. 
		However, this technique may perform poorly when many rows of data have at least one missing value. 
		This article describes an implementation for Stata of the MICE method of multiple multivariate imputation described by van Buuren, Boshuizen, and Knook (1999). 
		MICE stands for multivariate imputation by chained equations. 
		The basic idea of data analysis with multiple imputation is to create a small number (e.g., 5-10) of copies of the data, each of which has the missing values suitably imputed, and analyze each complete dataset independently. 
		Estimates of parameters of interest are averaged across the copies to give a single estimate. 
		Standard errors are computed according to the ``Rubin rules'', devised to allow for the between- and within-imputation components of variation in the parameter estimates. 
		This article describes five ado-files. 
		mvis creates multiple multivariate imputations. 
		uvis imputes missing values for a single variable as a function of several covariates, each with complete data. micombine fits a wide variety of regression models to a multiply imputed dataset, combining the estimates using Rubin's rules, and supports survival analysis models (stcox and streg), categorical data models, generalized linear models, and more. 
		Finally, misplit and mijoin are utilities to intercon-vert datasets created by mvis and by the miset program from John Carlin and colleagues. 
		The use of the routines is illustrated with an example of prognostic modeling in breast cancer.
	},
}

@Article{
	Lib-Missing-Data-Multiple-Imputation-Royston-2005
	,
	author       = {
		Royston, Patrick
	},
	date         = {
		2005-06
	},
	journaltitle = {
		The Stata Journal: Promoting communications on statistics and Stata
	},
	title        = {
		Multiple imputation of missing values: Update
	},
	doi          = {
		10.1177/1536867x0500500204
	},
	number       = {
		2
	},
	pages        = {
		188--201
	},
	volume       = {
		5
	},
	publisher    = {
		{SAGE} Publications
	},
	file = {
		references/10.1177%2F1536867x0500500204.pdf
	},
	library = {},
	keywords = {
		st00671, 
		ice, 
		mvis, 
		uvis, 
		micombine, 
		mijoin, 
		misplit, 
		missing data,
		missing at random, 
		multiple imputation, 
		multivariate imputation, 
		regression modeling
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
	abstract     = {
		This article describes a substantial update to mvis, which brings it more closely in line with the feature set of S. van Buuren and C. G. M. Oudshoorn's implementation of the MICE system in R and S-PLUS (for details, see http://www.multiple-imputation.com). 
		To make a clear distinction from mvis, the principal program of the new Stata release is called ice. 
		I will give details of how to use the new features and a practical illustrative example using real data. 
		All the facilities of mvis are retained by ice. Some improvements to micombine for computing estimates from multiply imputed datasets are also described.
	},
}

@Article{
	Lib-Missing-Data-Multiple-Imputation-vanBuuren-2006
	,
	author       = {
		van Buuren, Stef 
		and 
		Brand, J. P. L. 
		and 
		Groothuis-Oudshoorn, C. G. M. 
		and 
		Rubin, Donald B.
	},
	date         = {
		2006-12
	},
	journaltitle = {
		Journal of Statistical Computation and Simulation
	},
	title        = {
		Fully conditional specification in multivariate imputation
	},
	doi          = {
		10.1080/10629360600810434
	},
	number       = {
		12
	},
	pages        = {
		1049--1064
	},
	volume       = {
		76
	},
	publisher    = {
		Informa {UK} Limited
	},
	file = {
		references/10.1080%2F10629360600810434.pdf
	},
	library = {},
	keywords = {
		Multivariate missing data,
		Multiple imputation,
		Distributional compatibility,
		Gibbs sampling,
		Simulation,
		Proper imputation
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
	abstract     = {
		The use of the Gibbs sampler with fully conditionally specified models, where the distribution of each variable given the other variables is the starting point, has become a popular method to create imputations in incomplete multivariate data. 
		The theoretical weakness of this approach is that the specified conditional densities can be incompatible, and therefore the stationary distribution to which the Gibbs sampler attempts to converge may not exist. 
		This study investigates practical consequences of this problem by means of simulation. Missing data are created under four different missing data mechanisms. 
		Attention is given to the statistical behavior under compatible and incompatible models.
		The results indicate that multiple imputation produces essentially unbiased estimates with appropriate coverage in the simple cases investigated, even for the incompatible models. 
		Of particular interest is that these results were produced using only five Gibbs iterations starting from a simple draw from
		observed marginal distributions. 
		It thus appears that, despite the theoretical weaknesses, the actual performance of conditional model specification for multivariate imputation can be quite good, and therefore deserves further study.
	},
}

@Article{
	Lib-Missing-Data-Multiple-Imputation-Graham-2007
	,
	author       = {
		Graham, John W. 
		and 
		Olchowski, Allison E. 
		and 
		Gilreath, Tamika D.
	},
	date         = {
		2007-06
	},
	journaltitle = {
		Prevention Science
	},
	title        = {
		How many imputations are really needed? Some practical clarifications of multiple imputation theory
	},
	doi          = {
		10.1007/s11121-007-0070-9
	},
	number       = {
		3
	},
	pages        = {
		206--213
	},
	volume       = {
		8
	},
	publisher    = {
		Springer Science and Business Media {LLC}
	},
	file = {
		references/10.1007%2Fs11121-007-0070-9.pdf
	},
	library = {},
	keywords = {
		Multiple imputation,
		Number of imputations,
		Full information maximum likelihood,
		Missing data,
		Statistical power
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
	abstract     = {
		Multiple imputation (MI) and full information maximum likelihood (FIML) are the two most common approaches to missing data analysis. 
		In theory, MI and FIML are equivalent when identical models are tested using the same variables, and when m, the number of imputations performed with MI, approaches infinity. 
		However, it is important to know how many imputations are necessary before MI and FIML are sufficiently equivalent in ways that are important to prevention scientists. 
		MI theory suggests that small values of m, even on the order of three to five imputations, yield excellent results. Previous guidelines for sufficient m are based on relative efficiency, which involves the fraction of missing information ($\gamma$) for the parameter being estimated, and m. 
		In the present study, we used a Monte Carlo simulation to test MI models across several scenarios in which $\gamma$ and m were varied. 
		Standard errors and p-values for the regression coefficient of interest varied as a function of m, but not at the same rate as relative efficiency. 
		Most importantly, statistical power for small effect sizes diminished as m became smaller, and the rate of this power falloff was much greater than predicted by changes in relative efficiency. 
		Based our findings, we recommend that researchers using MI should perform many more imputations than previously considered sufficient. 
		These recommendations are based on $\gamma$, and take into consideration one's tolerance for a preventable power falloff (compared to FIML) due to using too few imputations.
	},
}

@Article{
	Lib-Missing-Data-Multiple-Imputation-Harel-2007
	,
	author       = {
		Harel, Ofer 
		and 
		Zhou, Xiao-Hua
	},
	date         = {
		2007
	},
	journaltitle = {
		Statistics in Medicine
	},
	title        = {
		Multiple imputation: Review of theory, implementation and software
	},
	doi          = {
		10.1002/sim.2787
	},
	number       = {
		16
	},
	pages        = {
		3057--3077
	},
	volume       = {
		26
	},
	publisher    = {
		Wiley
	},
	file = {
		references/10.1002%2Fsim.2787.pdf
	},
	library = {},
	keywords = {
		multiple imputation,
		sensitivity and specificity,
		diagnostic tests
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
	abstract     = {
		Missing data is a common complication in data analysis. 
		In many medical settings missing data can cause difficulties in estimation, precision and inference. 
		Multiple imputation (MI) (Multiple Imputation for Nonresponse in Surveys. Wiley: New York, 1987) is a simulation-based approach to deal with incomplete data. 
		Although there are many different methods to deal with incomplete data, MI has become one of the leading methods. 
		Since the late 1980s we observed a constant increase in the use and publication of MI-related research. 
		This tutorial does not attempt to cover all the material concerning MI, but rather provides an overview and combines together the theory behind MI, the implementation of MI, and discusses increasing possibilities of the use of MI using commercial and free software. 
		We illustrate some of the major points using an example from an Alzheimer disease (AD) study. 
		In this AD study, while clinical data are available for all subjects, postmortem data are only available for the subset of those who died and underwent an autopsy. 
		Analysis of incomplete data requires making unverifiable assumptions.
		These assumptions are discussed in detail in the text. 
		Relevant S-Plus code is provided.
	},
}

@Article{
	Lib-Missing-Data-Multiple-Imputation-Kenward-2007
	,
	author       = {
		Kenward, Michael G. 
		and 
		Carpenter, James
	},
	date         = {
		2007-06
	},
	journaltitle = {
		Statistical Methods in Medical Research
	},
	title        = {
		Multiple imputation: Current perspectives
	},
	doi          = {
		10.1177/0962280206075304
	},
	number       = {
		3
	},
	pages        = {
		199--218
	},
	volume       = {
		16
	},
	publisher    = {
		{SAGE} Publications
	},
	file = {
		references/10.1177%2F0962280206075304.pdf
	},
	library = {},
	keywords = {
		Missing data,
		Multiple imputation,
		Congeniality,
		Efficiency
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
	abstract     = {
		This paper provides an overview of multiple imputation and current perspectives on its use in medical research. 
		We begin with a brief review of the problem of handling missing data in general and place multiple imputation in this context, emphasizing its relevance for longitudinal clinical trials and observational studies with missing covariates. 
		We outline how multiple imputation proceeds in practice and then sketch its rationale. 
		We explore the problem of obtaining proper imputations in some detail and distinguish two main classes of approach, methods based on fully multivariate models, and those that iterate conditional univariate models. 
		We show how the use of so-called uncongenial imputation models are particularly valuable for sensitivity analyses and also for certain analyses in clinical trial settings. 
		We also touch upon other forms of sensitivity analysis that use multiple imputation. 
		Finally, we give some open questions that the increasing use of multiple imputation has thrown up, which we believe are useful directions for future research.
	},
}

@Article{
	Lib-Missing-Data-Multiple-Imputation-Nielsen-2007
	,
	author       = {
		Nielsen, S{\o}ren Feodor
	},
	date         = {
		2007-01
	},
	journaltitle = {
		International Statistical Review
	},
	title        = {
		Proper and improper multiple imputation
	},
	doi          = {
		10.1111/j.1751-5823.2003.tb00214.x
	},
	number       = {
		3
	},
	pages        = {
		593--607
	},
	volume       = {
		71
	},
	publisher    = {
		Wiley
	},
	file = {
		references/10.1111%2Fj.1751-5823.2003.tb00214.x.pdf
	},
	library = {},
	keywords = {
		Missing data,
		Multiple imputation,
		Congeniality,
		Efficiency
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
	abstract     = {
		Multiple imputation has become viewed as a general solution to missing data problems in statistics. 
		However, in order to lead to consistent asymptotically normal estimators, correct variance estimators and valid tests, the imputations must be proper. 
		So far it seems that only Bayesian multiple imputation, i.e. using a Bayesian predictive distribution to generate the imputations, or approximately Bayesian multiple imputations has been shown to lead to proper imputations in some settings. 
		In this paper, we shall see that Bayesian multiple imputation does not generally lead to proper multiple imputations. 
		Furthermore, it will be argued that for general statistical use, Bayesian multiple imputation is inefficient even when it is proper.
	},
}

@Article{
	Lib-Missing-Data-Multiple-Imputation-Reiter-2007
	,
	author       = {
		Reiter, Jerome P. 
		and 
		Raghunathan, Trivellore E.
	},
	date         = {
		2007-12
	},
	journaltitle = {
		Journal of the American Statistical Association
	},
	title        = {
		The multiple adaptations of multiple imputation
	},
	doi          = {
		10.1198/016214507000000932
	},
	number       = {
		480
	},
	pages        = {
		1462--1471
	},
	volume       = {
		102
	},
	publisher    = {
		Informa {UK} Limited
	},
	file = {
		references/10.1198%2F016214507000000932.pdf
	},
	library = {},
	keywords = {
		Confidentiality,
		Measurement error,
		Missing data,
		Synthetic
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
	abstract     = {
		Multiple imputation was first conceived as a tool that statistical agencies could use to handle nonresponse in large-sample public use surveys. 
		In the last two decades, the multiple-imputation framework has been adapted for other statistical contexts. 
		For example, individual researchers use multiple imputation to handle missing data in small samples, statistical agencies disseminate multiply-imputed data sets for purposes of protecting data confidentiality, and survey methodologists and epidemiologists use multiple imputation to correct for measurement errors. 
		In some of these settings, Rubin's original rules for combining the point and variance estimates from the multiply-imputed data sets are not appropriate, because what is known--and thus the conditional expectations and variances used to derive inferential methods--differs from that in the missing-data context. 
		These applications require new combining rules and methods of inference.
		In fact, more than 10 combining rules exist in the published literature.
		This article describes some of the main adaptations of the multiple-imputation framework, including missing data in large and small samples, data confidentiality, and measurement error. 
		It reviews the combining rules for each setting and explains why they differ. 
		Finally, it highlights research topics in extending the multiple-imputation framework.
	},
}

@Article{
	Lib-Missing-Data-Multiple-Imputation-Zhang-2007
	,
	author       = {
		Zhang, Paul
	},
	date         = {
		2007-01
	},
	journaltitle = {
		International Statistical Review
	},
	title        = {
		Multiple imputation: Theory and method
	},
	doi          = {
		10.1111/j.1751-5823.2003.tb00213.x
	},
	number       = {
		3
	},
	pages        = {
		581--592
	},
	volume       = {
		71
	},
	publisher    = {
		Wiley
	},
	file = {
		references/10.1111%2Fj.1751-5823.2003.tb00213.x.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
	abstract     = {
		In this review paper, we discuss the theoretical background of multiple imputation, describe how to build an imputation model and how to create proper imputations. 
		We also present the rules for making repeated imputation inferences. 
		Three widely used multiple imputation methods, the propensity score method, the predictive model method and the Markov chain Monte Carlo (MCMC) method, are presented and discussed.
	},
}

@Article{
	Lib-Missing-Data-Multiple-Imputation-Sterne-2009
	,
	author       = {
		Sterne, Jonathan A. C. 
		and 
		White, Ian R. 
		and 
		Carlin, John B. 
		and 
		Spratt, Michael 
		and 
		Royston, Patrick 
		and 
		Kenward, Michael G. 
		and 
		Wood, Angela M. 
		and 
		Carpenter, James R.
	},
	date         = {
		2009-06
	},
	journaltitle = {
		{BMJ}
	},
	title        = {
		Multiple imputation for missing data in epidemiological and clinical research: potential and pitfalls
	},
	doi          = {
		10.1136/bmj.b2393
	},
	number       = {
		jun29 1
	},
	pages        = {
		b2393--b2393
	},
	volume       = {
		338
	},
	publisher    = {
		{BMJ}
	},
	file = {
		references/10.1136%2Fbmj.b2393.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
	abstract = {
		Most studies have some missing data. Jonathan Sterne and colleagues describe the appropriate use and reporting of the multiple imputation approach to dealing with them.
	},
}

@Article{
	Lib-Missing-Data-Multiple-Imputation-White-2010
	,
	author       = {
		White, Ian R. 
		and 
		Royston, Patrick 
		and 
		Wood, Angela M.
	},
	date         = {
		2010-11
	},
	journaltitle = {
		Statistics in Medicine
	},
	title        = {
		Multiple imputation using chained equations: Issues and guidance for practice
	},
	doi          = {
		10.1002/sim.4067
	},
	number       = {
		4
	},
	pages        = {
		377--399
	},
	volume       = {
		30
	},
	publisher    = {
		Wiley
	},
	file = {
		references/10.1002%2Fsim.4067.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
	abstract     = {
		Multiple imputation by chained equations is a flexible and practical approach to handling missing data. 
		We describe the principles of the method and show how to impute categorical and quantitative variables, including skewed variables. 
		We give guidance on how to specify the imputation model and how many imputations are needed. 
		We describe the practical analysis of multiply imputed data, including model building and model checking. 
		We stress the limitations of the method and discuss the possible pitfalls. 
		We illustrate the ideas using a data set in mental health, giving Stata code fragments.
	},
}

@Article{
	Lib-Missing-Data-Multiple-Imputation-Azur-2011
	,
	author       = {
		Azur, Melissa J. 
		and 
		Stuart, Elizabeth A. 
		and 
		Frangakis, Constantine 
		and 
		Leaf, Philip J.
	},
	date         = {
		2011-02
	},
	journaltitle = {
		International Journal of Methods in Psychiatric Research
	},
	title        = {
		Multiple imputation by chained equations: What is it and how does it work?
	},
	doi          = {
		10.1002/mpr.329
	},
	number       = {
		1
	},
	pages        = {
		40--49
	},
	volume       = {
		20
	},
	publisher    = {
		Wiley
	},
	file = {
		references/10.1002%2Fmpr.329.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
	abstract     = {
		Multivariate imputation by chained equations (MICE) has emerged as a principled method of dealing with missing data. 
		Despite properties that make MICE particularly useful for large imputation procedures and advances in software development that now make it accessible to many researchers, many psychiatric researchers have not been trained in these methods and few practical resources exist to guide researchers in the implementation of this technique. 
		This paper provides an introduction to the MICE method with a focus on practical aspects and challenges in using this method. 
		A brief review of software programs available to implement MICE and then analyze multiply imputed data is also provided.
	},
}

@Article{
	Lib-Missing-Data-Multiple-Imputation-Kropko-2014
	,
	author       = {
		Kropko, Jonathan 
		and 
		Goodrich, Ben 
		and 
		Gelman, Andrew 
		and 
		Hill, Jennifer
	},
	date         = {
		2014
	},
	journaltitle = {
		Political Analysis
	},
	title        = {
		Multiple imputation for continuous and categorical data: Comparing joint multivariate normal and conditional approaches
	},
	doi          = {
		10.1093/pan/mpu007
	},
	number       = {
		4
	},
	pages        = {
		497--519
	},
	volume       = {
		22
	},
	publisher    = {
		Cambridge University Press ({CUP})
	},
	file = {
		references/10.1093%2Fpan%2Fmpu007.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
	abstract     = {
		We consider the relative performance of two common approaches to multiple imputation (MI): joint multivariate normal (MVN) MI, in which the data are modeled as a sample from a joint MVN distribution; and conditional MI, in which each variable is modeled conditionally on all the others. 
		In order to use the multivariate normal distribution, implementations of joint MVN MI typically assume that categories of discrete variables are probabilistically constructed from continuous values. 
		We use simulations to examine the implications of these assumptions. For each approach, we assess (1) the accuracy of the imputed values; and (2) the accuracy of coefficients and fitted values from a model fit to completed data sets. 
		These simulations consider continuous, binary, ordinal, and unordered-categorical variables. One set of simulations uses multivariate normal data, and one set uses data from the 2008 American National Election Studies.
		We implement a less restrictive approach than is typical when evaluating methods using simulations in the missing data literature: in each case, missing values are generated by carefully following the conditions necessary for missingness to be ``missing at random'' (MAR). 
		We find that in these situations conditional MI is more accurate than joint MVN MI whenever the data include categorical variables.
	},
}

@Article{
	Lib-Missing-Data-Multiple-Imputation-Lall-2016
	,
	author       = {
		Lall, Ranjit
	},
	date         = {
		2016
	},
	journaltitle = {
		Political Analysis
	},
	title        = {
		How multiple imputation makes a difference
	},
	doi          = {
		10.1093/pan/mpw020
	},
	number       = {
		4
	},
	pages        = {
		414--433
	},
	volume       = {
		24
	},
	publisher    = {
		Cambridge University Press ({CUP})
	},
	file = {
		references/10.1093%2Fpan%2Fmpw020.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
	abstract     = {
		Political scientists increasingly recognize that multiple imputation represents a superior strategy for analyzing missing data to the widely used method of listwise deletion. 
		However, there has been little systematic investigation of how multiple imputation affects existing empirical knowledge in the discipline. 
		This article presents the first large-scale examination of the empirical effects of substituting multiple imputation for listwise deletion in political science. 
		The examination focuses on research in the major subfield of comparative and international political economy (CIPE) as an illustrative example.
		Specifically, I use multiple imputation to reanalyze the results of almost every quantitative CIPE study published during a recent five-year period in International Organization and World Politics, two of the leading subfield journals in CIPE. 
		The outcome is striking: in almost half of the studies, key results ``disappear'' (by conventional statistical standards) when reanalyzed.
	},
}

@Article{
	Lib-Missing-Data-Multiple-Imputation-Beesley-2020
	,
	author       = {
		Beesley, Lauren J. 
		and 
		Taylor, Jeremy M. G.
	},
	date         = {
		2020-10
	},
	journaltitle = {
		Biometrics
	},
	title        = {
		A stacked approach for chained equations multiple imputation incorporating the substantive model
	},
	doi          = {
		10.1111/biom.13372
	},
	number       = {
		4
	},
	pages        = {
		1342--1354
	},
	volume       = {
		77
	},
	publisher    = {
		Wiley
	},
	file = {
		references/10.1111%2Fbiom.13372.pdf
	},
	library = {},
	keywords = {
		chained equations, 
		multiple imputation, 
		stacked imputation, 
		substantive model compatible imputation
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
	abstract     = {
		Multiple imputation by chained equations (MICE) has emerged as a popular approach for handling missing data. 
		A central challenge for applying MICE is determining how to incorporate outcome information into covariate imputation models, particularly for complicated outcomes. 
		Often, we have a particular analysis model in mind, and we would like to ensure congeniality between the imputation and analysis models. 
		We propose a novel strategy for directly incorporating the analysis model into the handling of missing data. 
		In our proposed approach, multiple imputations of missing covariates are obtained without using outcome information. 
		We then utilize the strategy of imputation stacking, where multiple imputations are stacked on top of each other to create a large data set.
		The analysis model is then incorporated through weights. 
		Instead of applying Rubin's combining rules, we obtain parameter estimates by fitting a weighted version of the analysis model on the stacked data set. 
		We propose a novel estimator for obtaining standard errors for this stacked and weighted analysis. 
		Our estimator is based on the observed data information principle in Louis' work and can be applied for analyzing stacked multiple imputations more generally. 
		Our approach for analyzing stacked multiple imputations is the first method that can be easily applied (using R package StackImpute) for a wide variety of standard analysis models and missing data settings.
	},
}

@Report{
	Lib-Missing-Data-Multiple-Imputation-Asparouhov-2022
	,
	author = {
		Asparouhov, Tihomir 
		and 
		Muthen, Bengt
	},
	date   = {
		2022
	},
	title  = {
		Multiple imputation with {Mplus}
	},
	type   = {
		techreport
	},
	url    = {
		http://www.statmodel.com/download/Imputations7.pdf
	},
	file = {
		references/Asparouhov-2022.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data-Multiple-Imputation
	},
}

@Comment{jabref-meta: databaseType:biblatex;}
% Encoding: US-ASCII

@Article{
	Lib-Missing-Data-Rubin-1976
	,
	author       = {
		Rubin, Donald B.
	},
	date         = {
		1976
	},
	journaltitle = {
		Biometrika
	},
	title        = {
		Inference and missing data
	},
	doi          = {
		10.1093/biomet/63.3.581
	},
	number       = {
		3
	},
	pages        = {
		581--592
	},
	volume       = {
		63
	},
	publisher    = {Oxford University Press ({OUP})},
	file = {
		references/10.1093%2Fbiomet%2F63.3.581.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data
	},
	abstract     = {
		When making sampling distribution inferences about the parameter of the data, 
		$\theta$, 
		it is appropriate to ignore the process that causes missing data if the missing data are `missing at random' and the observed data are `observed at random', 
		but these inferences are generally conditional on the observed pattern of missing data. 
		When making direct-likelihood or Bayesian inferences about $\theta$, 
		it is appropriate to ignore the process that causes missing data if the missing data are missing at random and the parameter of the missing data process is `distinct' from $\theta$. 
		These conditions are the weakest general conditions under which ignoring the process that causes missing data always leads to correct inferences.
	},
}

@InBook{
	Lib-Missing-Data-Arbuckle-1996
	,
	author    = {
		Arbuckle, James L.
	},
	booktitle = {
		Advanced structural equation modeling
	},
	date      = {
		1996
	},
	title     = {
		Full information estimation in the presence of incomplete data
	},
	doi       = {
		10.4324/9781315827414
	},
	editor    = {
		Marcoulides, George A. and Schumacker, Randall E.
	},
	publisher = {
		Psychology Press
	},
	file = {},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data
	},
}

@Article{
	Lib-Missing-Data-Schafer-1998,
	author       = {
		Schafer, Joseph L. 
		and 
		Olsen, Maren K.
	},
	date         = {
		1998-10
	},
	journaltitle = {
		Multivariate Behavioral Research
	},
	title        = {
		Multiple imputation for multivariate missing-data problems: A data analyst's perspective
	},
	doi          = {
		10.1207/s15327906mbr3304_5
	},
	number       = {
		4
	},
	pages        = {
		545--571
	},
	volume       = {
		33
	},
	publisher    = {
		Informa {UK} Limited
	},
	file = {
		references/10.1207%2Fs15327906mbr3304_5.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data
	},
	abstract     = {
		Analyses of multivariate data are frequently hampered by missing values. 
		Until recently, 
		the only missing-data methods available to most data analysts have been relatively ad hoc practices such as listwise deletion. 
		Recent dramatic advances in theoretical and computational statistics, 
		however, 
		have produced anew generation of flexible procedures with a sound statistical basis.
		These procedures involve multiple imputation (Rubin, 1987), 
		a simulation technique that replaces each missing datum with a set of $m > 1$ plausible values. 
		The $m$ versions of the complete data are analyzed by standard complete-data methods,
		and the results are combined using simple rules to yield estimates, 
		standard errors, 
		and $p$-values that formally incorporate missing-data uncertainty. 
		New computational algorithms and software described in a recent book (Schafer, 1997a) allow us to create proper multiple imputations in complex multivariate settings. 
		This article reviews the key ideas of multiple imputation, 
		discusses the software programs currently available, 
		and demonstrates their use on data from the Adolescent Alcohol Prevention Trial (Hansen \& Graham, 1991).
	},
}

@Article{
	Lib-Missing-Data-Yuan-2000
	,
	author       = {
		Yuan, Ke-Hai 
		and 
		Bentler, Peter M.
	},
	date         = {
		2000-08
	},
	journaltitle = {
		Sociological Methodology
	},
	title        = {
		Three likelihood-based methods for mean and covariance structure analysis with nonnormal missing data
	},
	doi          = {
		10.1111/0081-1750.00078
	},
	number       = {
		1
	},
	pages        = {
		165--200
	},
	volume       = {
		30
	},
	publisher    = {
		{SAGE} Publications
	},
	file = {
		references/10.1111%2F0081-1750.00078.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data
	},
	abstract     = {
		Survey and longitudinal studies in the social and behavioral sciences generally contain missing data. 
		Mean and covariance structure models play an important role in analyzing such data. 
		Two promising methods for dealing with missing data are a direct maximum-likelihood and a two-stage approach based on the unstructured mean and covariance estimates obtained by the EM-algorithm. 
		Typical assumptions under these two methods are ignorable nonresponse and normality of data. 
		However, 
		data sets in social and behavioral sciences are seldom normal,
		and experience with these procedures indicates that normal theory based methods for nonnormal data very often lead to incorrect model evaluations. 
		By dropping the normal distribution assumption, 
		we develop more accurate procedures for model inference. 
		Based on the theory of generalized estimating equations, 
		a way to obtain consistent standard errors of the two-stage estimates is given. 
		The asymptotic efficiencies of different estimators are compared under various assumptions. 
		We also propose a minimum chi-square approach and show that the estimator obtained by this approach is asymptotically at least as efficient as the two likelihood-based estimators for either normal or nonnormal data. 
		The major contribution of this paper is that for each estimator, 
		we give a test statistic whose asymptotic distribution is chisquare as long as the underlying sampling distribution enjoys finite fourth-order moments. 
		We also give a characterization for each of the two likelihood ratio test statistics when the underlying distribution is nonnormal. 
		Modifications to the likelihood ratio statistics are also given. 
		Our working assumption is that the missing data mechanism is missing completely at random. 
		Examples and Monte Carlo studies indicate that, 
		for commonly encountered nonnormal distributions, 
		the procedures developed in this paper are quite reliable even for samples with missing data that are missing at random.
	},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Psychological Methods
% Special Issue

@Article{
	Lib-Missing-Data-Collins-2001
	,
	author       = {
		Collins, Linda M. 
		and 
		Schafer, Joseph L. 
		and 
		Kam, Chi-Ming
	},
	date         = {
		2001
	},
	journaltitle = {
		Psychological Methods
	},
	title        = {
		A comparison of inclusive and restrictive strategies in modern missing data procedures
	},
	doi          = {
		10.1037/1082-989x.6.4.330
	},
	number       = {
		4
	},
	pages        = {
		330--351
	},
	volume       = {
		6
	},
	publisher    = {
		American Psychological Association ({APA})
	},
	file = {
		references/10.1037%2F1082-989x.6.4.330.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data
	},
	abstract     = {
		Two classes of modem missing data procedures,
		maximum likelihood (ML) and multiple imputation (MI),
		tend to yield similar results when implemented in comparable ways.
		In either approach,
		it is possible to include auxiliary variables solely for the purpose of improving the missing data procedure.
		A simulation was presented to assess the potential costs and benefits of a restrictive strategy,
		which makes minimal use of auxiliary variables,
		versus an inclusive strategy,
		which makes liberal use of such variables.
		The simulation showed that the inclusive strategy is to be greatly preferred.
		With an inclusive strategy not only is there a reduced chance of inadvertently omitting an important cause of missingness,
		there is also the possibility of noticeable gains in terms of increased efficiency and reduced bias,
		with only minor costs.
		As implemented in currently available software,
		the ML approach tends to encourage the use of a restrictive strategy,
		whereas the MI approach makes it relatively simple to use an inclusive strategy.
	},
}

@Article{
	Lib-Missing-Data-Enders-2001a
	,
	author       = {
		Craig K. Enders
	},
	date         = {
		2001
	},
	journaltitle = {
		Psychological Methods
	},
	title        = {
		The impact of nonnormality on full information maximum-likelihood estimation for structural equation models with missing data
	},
	doi          = {
		10.1037/1082-989x.6.4.352
	},
	number       = {
		4
	},
	pages        = {
		352--370
	},
	volume       = {
		6
	},
	publisher    = {
		American Psychological Association ({APA})
	},
	file = {
		references/10.1037%2F1082-989x.6.4.352.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data
	},
	abstract     = {
		A Monte Carlo simulation examined full information maximum-likelihood estimation (FIML) in structural equation models with nonnormal indicator variables. 
		The impacts of 4 independent variables were examined (missing data algorithm, 
		missing data rate, 
		sample size, 
		and distribution shape) on 4 outcome measures (parameter estimate bias, 
		parameter estimate efficiency, 
		standard error coverage, 
		and model rejection rates). 
		Across missing completely at random and missing at random patterns, 
		FIML parameter estimates involved less bias and were generally more efficient than those of ad hoc missing data techniques. 
		However, 
		similar to complete-data maximum-likelihood estimation in structural equation modeling, 
		standard errors were negatively biased and model rejection rates were inflated. 
		Simulation results suggest that recently developed correctives for missing data (e.g., rescaled statistics and the bootstrap) can mitigate problems that stem from nonnormal data.
	},
}

@Article{
	Lib-Missing-Data-Sinharay-2001
	,
	author       = {
		Sinharay, Sandip 
		and 
		Stern, Hal S. 
		and 
		Russell, Daniel
	},
	date         = {
		2001
	},
	journaltitle = {
		Psychological Methods
	},
	title        = {
		The use of multiple imputation for the analysis of missing data
	},
	doi          = {
		10.1037/1082-989x.6.4.317
	},
	number       = {
		4
	},
	pages        = {
		317--329
	},
	volume       = {
		6
	},
	publisher    = {
		American Psychological Association ({APA})
	},
	file = {
		references/10.1037%2F1082-989x.6.4.317.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data
	},
	abstract     = {
		This article provides a comprehensive review of multiple imputation (MI), 
		a technique for analyzing data sets with missing values.
		Formally, MI is the process of replacing each missing data point with a set of $m > 1$ plausible values to generate $m$ complete data sets. 
		These complete data sets are then analyzed by standard statistical software, 
		and the results combined, 
		to give parameter estimates and standard errors that take into account the uncertainty due to the missing data values. 
		This article introduces the idea behind MI, 
		discusses the advantages of MI over existing techniques for addressing missing data, 
		describes how to do MI for real problems, 
		reviews the software available to implement MI, 
		and discusses the results of a simulation study aimed at finding out how assumptions regarding the imputation model affect the parameter estimates provided by MI.
	},
}

@Article{
	Lib-Missing-Data-West-2001
	,
	author       = {
		West, Stephen G.
	},
	date         = {
		2001
	},
	journaltitle = {
		Psychological Methods
	},
	title        = {
		New approaches to missing data in psychological research: Introduction to the special section
	},
	doi          = {
		10.1037/1082-989x.6.4.315
	},
	number       = {
		4
	},
	pages        = {
		315--316
	},
	volume       = {
		6
	},
	publisher    = {
		American Psychological Association ({APA})
	},
	file = {
		references/10.1037%2F1082-989x.6.4.315.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data
	},
	abstract     = {
		Traditional approaches to missing data (e.g., listwise deletion) can lead to less than optimal results in terms of bias,
		statistical power,
		or both.
		This article introduces the 3 articles in the special section of Psychological Methods,
		which consider multiple imputation and maximum-likelihood methods,
		new approaches to missing data that can often yield improved results.
		Computer software is now available to implement these new methods.
	},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@Article{
	Lib-Missing-Data-Enders-2001b
	,
	author       = {
		Enders, Craig K.
	},
	date         = {
		2001-01
	},
	journaltitle = {
		Structural Equation Modeling: A Multidisciplinary Journal
	},
	title        = {
		A primer on maximum likelihood algorithms available for use with missing data
	},
	doi          = {
		10.1207/s15328007sem0801_7
	},
	number       = {
		1
	},
	pages        = {
		128--141
	},
	volume       = {
		8
	},
	publisher    = {
		Informa {UK} Limited
	},
	file = {
		references/10.1207%2Fs15328007sem0801_7.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data
	},
	abstract     = {
		Maximum likelihood algorithms for use with missing data are becoming commonplace in microcomputer packages. 
		Specifically, 
		3 maximum likelihood algorithms are currently available in existing software packages: 
		the multiple-group approach, 
		full information maximum likelihood estimation, 
		and the EM algorithm. 
		Although they belong to the same family of estimator,
		confusion appears to exist over the differences among the 3 algorithms. 
		This article provides a comprehensive, 
		nontechnical overview of the 3 maximum likelihood algorithms. 
		Multiple imputation, 
		which is frequently used in conjunction with the EM algorithm,
		is also discussed.
	},
}

@Article{
	Lib-Missing-Data-Schafer-2002,
	author       = {
		Schafer, Joseph L. 
		and 
		Graham, John W.
	},
	date         = {
		2002
	},
	journaltitle = {
		Psychological Methods
	},
	title        = {
		Missing data: Our view of the state of the art
	},
	doi          = {
		10.1037/1082-989x.7.2.147
	},
	number       = {
		2
	},
	pages        = {
		147--177
	},
	volume       = {
		7
	},
	publisher    = {
		American Psychological Association ({APA})
	},
	file = {
		references/10.1037%2F1082-989x.7.2.147.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data
	},
	abstract     = {
		Statistical procedures for missing data have vastly improved, 
		yet misconception and unsound practice still abound. 
		The authors frame the missing-data problem, 
		review methods, 
		offer advice, 
		and raise issues that remain unresolved. 
		They clear up common misunderstandings regarding the missing at random (MAR) concept.
		They summarize the evidence against older procedures and, 
		with few exceptions, 
		discourage their use. 
		They present, 
		in both technical and practical language, 
		2 general approaches that come highly recommended: 
		maximum likelihood (ML) and Bayesian multiple imputation (MI). 
		Newer developments are discussed, 
		including some for dealing with missing data that are not MAR. 
		Although not yet in the mainstream, 
		these procedures may eventually extend the ML and MI methods that currently represent the state of the art.
	},
}

@Article{
	Lib-Missing-Data-Peugh-2004
	,
	author       = {
		Peugh, James L. 
		and 
		Enders, Craig K.
	},
	date         = {
		2004-12
	},
	journaltitle = {
		Review of Educational Research
	},
	title        = {
		Missing data in educational research: A review of reporting practices and suggestions for improvement
	},
	doi          = {
		10.3102/00346543074004525
	},
	number       = {
		4
	},
	pages        = {
		525--556
	},
	volume       = {
		74
	},
	publisher    = {
		American Educational Research Association ({AERA})
	},
	file = {
		references/10.3102%2F00346543074004525.pdf
	},
	library = {},
	keywords = {
		EM algorithm, 
		maximum likelihood estimation, 
		missing data, 
		multiple imputation, 
		NORM
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data
	},
	abstract     = {
		Missing data analyses have received considerable recent attention in the methodological literature, and two ``modern'' methods, 
		multiple imputation and maximum likelihood estimation, 
		are recommended. 
		The goals of this article are to (a) provide an overview of missing-data theory, 
		maximum likelihood estimation, 
		and multiple imputation; 
		(b) conduct a methodological review of missing-data reporting practices in 23 applied research journals; 
		and (c) provide a demonstration of multiple imputation and maximum likelihood estimation using the Longitudinal Study of American Youth data. 
		The results indicated that explicit discussions of missing data increased substantially between 1999 and 2003, 
		but the use of maximum likelihood estimation or multiple imputation was rare; 
		the studies relied almost exclusively on listwise and pairwise deletion.
	},
}

@Article{
	Lib-Missing-Data-Graham-2009
	,
	author       = {
		Graham, John W.
	},
	date         = {
		2009-01
	},
	journaltitle = {
		Annual Review of Psychology
	},
	title        = {
		Missing data analysis: Making it work in the real world
	},
	doi          = {
		10.1146/annurev.psych.58.110405.085530
	},
	number       = {
		1
	},
	pages        = {
		549--576
	},
	volume       = {
		60
	},
	publisher    = {
		Annual Reviews
	},
	file = {
		references/10.1146%2Fannurev.psych.58.110405.085530.pdf
	},
	library = {},
	keywords = {
		multiple imputation, 
		maximum likelihood, 
		attrition, 
		nonignorable missingness, 
		planned missingness 
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data
	},
	abstract     = {
		This review presents a practical summary of the missing data literature, 
		including a sketch of missing data theory and descriptions of normal-model multiple imputation (MI) and maximum likelihood methods. 
		Practical missing data analysis issues are discussed, 
		most notably the inclusion of auxiliary variables for improving power and reducing bias. 
		Solutions are given for missing data challenges such as handling longitudinal, 
		categorical, 
		and clustered data with normal-model MI; 
		including interactions in the missing data model; 
		and handling large numbers of variables. 
		The discussion of attrition and nonignorable missingness emphasizes the need for longitudinal diagnostics and for reducing the uncertainty about the missing data mechanism under attrition. 
		Strategies suggested for reducing attrition bias include using auxiliary variables, 
		collecting follow-up data on a sample of those initially missing, 
		and collecting data on intent to drop out. Suggestions are given for moving forward with research on missing data and attrition.
	},
}

@Article{
	Lib-Missing-Data-Cheema-2014
	,
	author       = {
		Cheema, Jehanzeb R.
	},
	date         = {
		2014-12
	},
	journaltitle = {
		Review of Educational Research
	},
	title        = {
		A review of missing data handling methods in education research
	},
	doi          = {
		10.3102/0034654314532697
	},
	number       = {
		4
	},
	pages        = {
		487--508
	},
	volume       = {84},
	publisher    = {
		American Educational Research Association ({AERA})
	},
	file = {
		references/10.3102%2F0034654314532697.pdf
	},
	library = {},
	keywords = {
		missing data, 
		imputation, 
		education research, 
		listwise deletion, 
		missing value analysis
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data
	},
	abstract     = {
		Missing data are a common occurrence in survey-based research studies in education, and the way missing values are handled can significantly affect the results of analyses based on such data. 
		Despite known problems with performance of some missing data handling methods, 
		such as mean imputation, 
		many researchers in education continue to use those methods as a quick fix. 
		This study reviews the current literature on missing data handling methods within the special context of education research to summarize the pros and cons of various methods and provides guidelines for future research in this area.
	},
}

@Article{
	Lib-Missing-Data-Schouten-2018a
	,
	author       = {
		Schouten, Rianne Margaretha 
		and 
		Lugtig, Peter 
		and 
		Vink, Gerko
	},
	date         = {
		2018-07
	},
	journaltitle = {
		Journal of Statistical Computation and Simulation
	},
	title        = {
		Generating missing values for simulation purposes: A multivariate amputation procedure
	},
	doi          = {
		10.1080/00949655.2018.1491577
	},
	number       = {
		15
	},
	pages        = {
		2909--2930
	},
	volume       = {
		88
	},
	publisher    = {
		Informa {UK} Limited
	},
	file = {
		references/10.1080%2F00949655.2018.1491577.pdf
	},
	library = {},
	keywords = {
		missing data,
		multiple imputation,
		multivariate amputation,
		evaluation
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data
	},
	abstract     = {
		Missing data form a ubiquitous problem in scientific research, 
		especially since most statistical analyses require complete data. 
		To evaluate the performance of methods dealing with missing data, 
		researchers perform simulation studies. 
		An important aspect of these studies is the generation of missing values in a simulated, 
		complete data set: 
		the amputation procedure. 
		We investigated the methodological validity and statistical nature of both the current amputation practice and a newly developed and implemented multivariate amputation procedure. 
		We found that the current way of practice may not be appropriate for the generation of intuitive and reliable missing data problems. 
		The multivariate amputation procedure, 
		on the other hand, 
		generates reliable amputations and allows for a proper regulation of missing data problems. 
		The procedure has additional features to generate any missing data scenario precisely as intended. 
		Hence, 
		the multivariate amputation procedure is an efficient method to accurately evaluate missing data methodology.
	},
}

@Article{
	Lib-Missing-Data-Schouten-2018b
	,
	author       = {
		Schouten, Rianne Margaretha 
		and 
		Vink, Gerko
	},
	date         = {
		2018-10
	},
	journaltitle = {
		Sociological Methods {\&} Research
	},
	title        = {
		The dance of the mechanisms: How observed information influences the validity of missingness assumptions
	},
	doi          = {
		10.1177/0049124118799376
	},
	number       = {
		3
	},
	pages        = {
		1243--1258
	},
	volume       = {
		50
	},
	publisher    = {
		{SAGE} Publications
	},
	file = {
		references/10.1177%2F0049124118799376.pdf
	},
	library = {},
	keywords = {
		missing data methodology, 
		missingness assumptions, 
		multivariate amputation
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data
	},
	abstract     = {
		Missing data in scientific research go hand in hand with assumptions about the nature of the missingness.
		When dealing with missing values,
		a set of beliefs has to be formulated about the extent to which the observed data may also hold for the missing parts of the data.
		It is vital that the validity of these missingness assumptions is verified, tested, and that assumptions are adjusted when necessary.
		In this article,
		we demonstrate how observed data structures could a priori indicate whether it is likely that our beliefs about the missingness can be trusted.
		To this end,
		we simulate complete data and generate missing values according several types of MCAR, MAR, and MNAR mechanisms.
		We demonstrate that in scenarios where the data correlations are either low or very substantial,
		strictly different mechanisms yield equivalent statistical inferences.
		In addition,
		we show that the choice of quantity of scientific interest together with the distribution of the nonresponse govern the validity of the missingness assumptions.
	},
}

@Article{
	Lib-Missing-Data-Savalei-2021
	,
	author       = {
		Savalei, Victoria 
		and 
		Rosseel, Yves
	},
	date         = {
		2021-10
	},
	journaltitle = {
		Structural Equation Modeling: A Multidisciplinary Journal
	},
	title        = {
		Computational options for standard errors and test statistics with incomplete normal and nonnormal data in {SEM}
	},
	doi          = {
		10.1080/10705511.2021.1877548
	},
	number       = {
		2
	},
	pages        = {
		163--181
	},
	volume       = {
		29
	},
	publisher    = {
		Informa {UK} Limited
	},
	file = {
		references/10.1080%2F10705511.2021.1877548.pdf
	},
	library = {},
	keywords = {
		incomplete data,
		nonnormal data,
		robust corrections,
		software implementation
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Missing-Data
	},
	abstract     = {
		This article provides an overview of different computational options for inference following normal theory maximum likelihood (ML) estimation in structural equation modeling (SEM) with incomplete normal and nonnormal data. 
		Complete data are covered as a special case. 
		These computational options include whether the information matrix is observed or expected, 
		whether the observed information matrix is estimated numerically or using an analytic asymptotic approximation, 
		and whether the information matrix and the outer product matrix of the score vector are evaluated at the saturated or at the structured estimates. 
		A variety of different standard errors and robust test statistics become possible by varying these options. 
		We review the asymptotic properties of these computational variations, 
		and we show how to obtain them using lavaan in R. 
		We hope that this article will encourage methodologists to study the impact of the available computational options on the performance of standard errors and test statistics in SEM.
	},
}

@Comment{jabref-meta: databaseType:biblatex;}

% https://bookdown.org/mwheymans/bookmi/
% Encoding: US-ASCII

@Article{
	Lib-Multilevel-Modeling-Mediation-Bauer-2006
	,
	author       = {
		Bauer, Daniel J. 
		and 
		Preacher, Kristopher J. 
		and 
		Gil, Karen M.
	},
	date         = {
		2006
	},
	journaltitle = {
		Psychological Methods
	},
	title        = {
		Conceptualizing and testing random indirect effects and moderated mediation in multilevel models: New procedures and recommendations
	},
	doi          = {
		10.1037/1082-989x.11.2.142
	},
	number       = {
		2
	},
	pages        = {
		142--163
	},
	volume       = {
		11
	},
	publisher    = {
		American Psychological Association ({APA})
	},
	file = {
		references/10.1037%2F1082-989x.11.2.142.pdf
	},
	library = {},
	keywords = {
		multilevel model, 
		hierarchical linear model, 
		indirect effect, 
		mediation, 
		moderated mediation
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Multilevel-Modeling-Mediation
	},
	abstracts = {
		The authors propose new procedures for evaluating direct, indirect, and total effects in multilevel models when all relevant variables are measured at Level 1 and all effects are random. 
		Formulas are provided for the mean and variance of the indirect and total effects and for the sampling variances of the average indirect and total effects. 
		Simulations show that the estimates are unbiased under most conditions. 
		Confidence intervals based on a normal approximation or a simulated sampling distribution perform well when the random effects are normally distributed but less so when they are nonnormally distributed. 
		These methods are further developed to address hypotheses of moderated mediation in the multilevel context. 
		An example demonstrates the feasibility and usefulness of the proposed methods.
	},
}

@Comment{jabref-meta: databaseType:biblatex;}
% Encoding: US-ASCII

@Book{
	Lib-NHST-Power-Books-Cohen-1988
	,
	author    = {
		Cohen, Jacob
	},
	date      = {
		1988
	},
	title     = {
		Statistical power analysis for the behavioral sciences
	},
	doi       = {
		10.4324/9780203771587
	},
	edition   = {
		2
	},
	isbn      = {
		9780203771587
	},
	publisher = {
		Routledge
	},
	file = {
		references/9780203771587.pdf
	},
	library = {
		HA29 .C66 1988
	},
	keywords = {
		Social sciences--Statistical methods,
		Probabilities,
		Statistical power analysis
	},
	addendum = {
		https://lccn.loc.gov/88012110
	},
	note = {},
	annotation = {
		Lib-NHST-Power-Books
	},
	abstract  = {
		Statistical Power Analysis is a nontechnical guide to power analysis in research planning that provides users of applied statistics with the tools they need for more effective analysis.
		The Second Edition includes:
		\begin{itemize}
			\item a chapter covering power analysis in set correlation and multivariate methods;
			\item a chapter considering effect size, psychometric reliability, and the efficacy of ``qualifying'' dependent variables and;
			\item expanded power and sample size tables for multiple regression/correlation.
		\end{itemize}
	},
}

@Book{
	Lib-NHST-Power-Books-Liu-2013
	,
	author    = {
		Liu, Xiaofeng Steven
	},
	date      = {
		2013-11
	},
	title     = {
		Statistical power analysis for the social and behavioral sciences
	},
	doi       = {
		10.4324/9780203127698
	},
	isbn      = {
		9780203127698
	},
	publisher = {
		Routledge
	},
	file = {
		references/9780203127698.pdf
	},
	library = {
		HA29 .L83185 2014
	},
	keywords = {
		Social sciences--Statistical methods,
		Probabilities,
		Statistical power analysis
	},
	addendum = {
		https://lccn.loc.gov/2013025364
	},
	note = {},
	annotation = {
		Lib-NHST-Power-Books
	},
	abstract  = {
		This is the first book to demonstrate the application of power analysis to the newer more advanced statistical techniques that are increasingly used in the social and behavioral sciences. 
		Both basic and advanced designs are covered. Readers are shown how to apply power analysis to techniques such as hierarchical linear modeling, 
		meta-analysis, 
		and structural equation modeling. 
		Each chapter opens with a review of the statistical procedure and then proceeds to derive the power functions. 
		This is followed by examples that demonstrate how to produce power tables and charts. 
		The book clearly shows how to calculate power by providing open code for every design and procedure in R,
		SAS, 
		and SPSS. 
		Readers can verify the power computation using the computer programs on the book's website. 
		There is a growing requirement to include power analysis to justify sample sizes in grant proposals. 
		Most chapters are self-standing and can be read in any order without much disruption.
		This book will help readers do just that. 
		Sample computer code in R, 
		SPSS, 
		and SAS at www.routledge.com/9781848729810 are written to tabulate power values and produce power curves that can be included in a grant proposal.
				
		Organized according to various techniques, chapters 1 – 3 introduce the basics of statistical power and sample size issues including the historical origin, 
		hypothesis testing, 
		and the use of statistical power in t tests and confidence intervals. 
		Chapters 4 - 6 cover common statistical procedures -- analysis of variance, 
		linear regression (both simple regression and multiple regression), 
		correlation, 
		analysis of covariance, 
		and multivariate analysis. 
		Chapters 7 - 11 review the new statistical procedures -- multi-level models, 
		meta-analysis, 
		structural equation models, 
		and longitudinal studies. 
		The appendixes contain a tutorial about R and show the statistical theory of power analysis.
				
		Intended as a supplement for graduate courses on quantitative methods, 
		multivariate statistics, 
		hierarchical linear modeling (HLM) and/or multilevel modeling and SEM taught in psychology, 
		education, 
		human development, 
		nursing, 
		and social and life sciences, 
		this is the first text on statistical power for advanced procedures. 
		Researchers and practitioners in these fields also appreciate the book's unique coverage of the use of statistical power analysis to determine sample size in planning a study. 
		A prerequisite of basic through multivariate statistics is assumed.
	},
}

@Book{
	Lib-NHST-Power-Books-Murphy-2014
	,
	author    = {
		Murphy, Kevin R. 
		and 
		Myors, Brett and Wolach, Allen
	},
	date      = {
		2014-05
	},
	title     = {
		Statistical power analysis: A simple and general model for traditional and modern hypothesis tests
	},
	doi       = {
		10.4324/9781315773155
	},
	edition   = {
		4
	},
	isbn      = {
		9781315773155
	},
	publisher = {
		Routledge
	},
	file = {
		references/9781315773155.pdf
	},
	library = {
		QA277 .M87 2014
	},
	keywords = {
		Statistical hypothesis testing,
		Statistical power analysis
	},
	addendum = {
		https://lccn.loc.gov/2013042607
	},
	note = {},
	annotation = {
		Lib-NHST-Power-Books
	},
	abstract  = {
		Noted for its accessible approach, 
		this text applies the latest approaches of power analysis to both null hypothesis and minimum-effect testing using the same basic unified model. 
		Through the use of a few simple procedures and examples,
		the authors show readers with little expertise in statistical analysis how to obtain the values needed to carry out the power analysis for their research.
		Illustrations of how these analyses work and how they can be used to choose the appropriate criterion for defining statistically significant outcomes are sprinkled throughout.
		The book presents a simple and general model for statistical power analysis based on the F statistic and reviews how to determine:
		the sample size needed to achieve desired levels of power; 
		the level of power needed in a study; 
		the size of effect that can be reliably detected by a study; 
		and sensible criteria for statistical significance. 
		The book helps readers design studies, 
		diagnose existing studies, 
		and understand why hypothesis tests come out out the way they do.
				
		The fourth edition features:
				
		\begin{itemize}
			\item New Boxed Material sections provide examples of power analysis in action and discuss unique issues that arise as a result of applying power analyses in different designs.
			\item Many more worked examples help readers apply the concepts presented.
			\item Expanded coverage of power analysis for multifactor analysis of variance (ANOVA) to show readers how to analyze up to four factors with repeated measures on any or all of the factors.
			\item Re-designed and expanded web based One Stop F Calculator software and data sets that allow users to perform all of the book's analyses and conduct significance tests, power analyses, and assessments of N and alpha needed for traditional and minimum-effects tests.
			\item Easy to apply formulas for approximating the number of subjects required to reach adequate levels of power in a wide range of studies.
		\end{itemize}
				
		Intended as a supplement for graduate/advanced undergraduate courses in research methods or experimental design, 
		intermediate, 
		advanced, 
		or multivariate statistics, 
		statistics II, 
		or psychometrics, 
		taught in psychology, 
		education, 
		business, 
		and other social and health sciences,
		researchers also appreciate the book's applied approach.
	},
}

@Book{
	Lib-NHST-Power-Books-Verma-2020
	,
	author    = {
		Verma, J. P. 
		and 
		Verma, Priyam
	},
	date      = {
		2020
	},
	title     = {
		Determining sample size and power in research studies
	},
	doi       = {
		10.1007/978-981-15-5204-5
	},
	isbn      = {
		978-981-15-5204-5
	},
	publisher = {
		Springer Singapore
	},
	file = {
		references/9789811552045.pdf
	},
	library = {
	},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-NHST-Power-Books
	},
	abstract  = {
		This book addresses sample size and power in the context of research, 
		offering valuable insights for graduate and doctoral students as well as researchers in any discipline where data is generated to investigate research questions. 
		It explains how to enhance the authenticity of research by estimating the sample size and reporting the power of the tests used. 
		Further, 
		it discusses the issue of sample size determination in survey studies as well as in hypothesis testing experiments so that readers can grasp the concept of statistical errors, 
		minimum detectable difference, 
		effect size, 
		one-tail and two-tail tests and the power of the test. 
		The book also highlights the importance of fixing these boundary conditions in enhancing the authenticity of research findings and improving the chances of research papers being accepted by respected journals. 
				
		Further, 
		it explores the significance of sample size by showing the power achieved in selected doctoral studies. 
		Procedure has been discussed to fix power in the hypothesis testing experiment. 
		One should usually have power at least 0.8 in the study because having power less than this will have the issue of practical significance of findings. 
		If the power in any study is less than 0.5 then it would be better to test the hypothesis by tossing a coin instead of organizing the experiment. 
		It also discusses determining sample size and power using the freeware G*Power software, 
		based on twenty-one examples using different analyses, 
		like t-test, parametric and non-parametric correlations,
		multivariate regression,
		logistic regression,
		independent and repeated measures ANOVA, 
		mixed design, MANOVA and chi-square.
	},
}

@Comment{jabref-meta: databaseType:biblatex;}
% Encoding: US-ASCII

@Book{
	Lib-Nonnormality-Bradley-1968,
	author    = {
		Bradley, James V.
	},
	date      = {
		1968
	},
	title     = {
		Distribution free statistical tests
	},
	location  = {
		Englewood Cliffs, NJ
	},
	publisher = {
		Prentice-Hall
	},
}

@Article{
	Lib-Nonnormality-Glass-1972
	,
	author       = {
		Glass, Gene V.
		and
		Peckham, Percy D.
		and
		Sanders, James R.
	},
	date         = {
		1972-09
	},
	journaltitle = {
		Review of Educational Research
	},
	title        = {
		Consequences of failure to meet assumptions underlying the fixed effects analyses of variance and covariance
	},
	doi          = {
		10.3102/00346543042003237
	},
	number       = {
		3
	},
	pages        = {
		237--288
	},
	volume       = {
		42
	},
	publisher    = {
		American Educational Research Association ({AERA})
	},
}

@Article{
	Lib-Nonnormality-Pearson-1975
	,
	author       = {
		Pearson, E. S.
		and
		Please, N. W.
	},
	date         = {
		1975-08
	},
	journaltitle = {
		Biometrika
	},
	title        = {
		Relation between the shape of population distribution and the robustness of four simple test statistics
	},
	doi          = {
		10.1093/biomet/62.2.223
	},
	number       = {
		2
	},
	pages        = {
		223--241
	},
	volume       = {
		62
	},
	publisher    = {
		Oxford University Press ({OUP})
	},
	abstract     = {
		The underlying theory upon which a great number of statistical procedures are based assumes that the variable or variables sampled are normally distributed.
		While there has been a good deal of theoretical research on the robustness of these procedures,
		the results seem not to have been set out in terms which the unsophisticated user of statistical methods can easily assimilate.
		The present paper, based on extensive computer simulation, aims at relating diagrammatically the shape of population to the robustness of the distribution of four simple statistics.
		A set of 29 Johnson SB and SU curves, several Pearson and Weibull curves and some large-sample histograms have been used as populations. The results indicate the extent to which the population moment ratios $\sqrt{\beta 1}$ and $\sqrt{\beta 2}$ determine the degree of robustness; charts are provided which should help the user, if he has some knowledge of these two parameters, to decide whether the lack of robustness matters from the practical aspect with which he is concerned.
	},
}

@Article{
	Lib-Nonnormality-Blair-1981
	,
	author       = {
		Blair, R. Clifford
	},
	date         = {
		1981-12
	},
	journaltitle = {
		Review of Educational Research
	},
	title        = {
		A reaction to ``Consequences of failure to meet assumptions underlying the fixed effects analysis of variance and covariance''
	},
	doi          = {
		10.3102/00346543051004499
	},
	number       = {
		4
	},
	pages        = {
		499--507
	},
	volume       = {
		51
	},
	publisher    = {
		American Educational Research Association ({AERA})
	},
	abstract     = {
		Glass, Peckham, and Sanders discourage the use of nonparametric counterparts of the $t$-test even when it is known that data were sampled from non-normal (e.g., highly skewed) distributions.
		This paper contends that Glass et al. erred in taking this position and that their error was due,
		at least in part,
		to their failure to consider the relative power of the $t$-test and its nonparametric counterparts under various population shapes.
		It is further contended that Wilcoxon’s rank-sum test has power properties that make it preferable to the $t$-test in many, perhaps even most,
		non-normal population situations.
	},
}

@Article{
	Lib-Nonnormality-Bradley-1982
	,
	author       = {
		Bradley, James V.
	},
	date         = {
		1982-08
	},
	journaltitle = {
		Bulletin of the Psychonomic Society
	},
	title        = {
		The insidious {L}-shaped distribution
	},
	doi          = {
		10.3758/bf03330089
	},
	number       = {
		2
	},
	pages        = {
		85--88
	},
	volume       = {
		20
	},
	publisher    = {
		Springer Science and Business Media {LLC}
	},
	abstract     = {
		L-shaped distributions are not rare and are probably far more prevalent than is generally realized.
		They are highly conducive to nonrobustness of normality-assuming statistical tests, and they strongly resist transformation to normality.
		The thinner the tail of the distribution, the more unlikely it is that its L-shapedness will be detected by inspecting a sample drawn from it.
		Yet, as the tail of an L-shaped distribution becomes increasingly shallow, its skewness and kurtosis depart increasingly from their ``normal-distribution'' values, and the distribution becomes increasingly conducive to drastic nonrobustness.
		Worse, a fairly common type of experimental situation in psychological research produces shallow-tailed L-shaped distributions.
	},
}

@Article{
	Lib-Nonnormality-Micceri-1989
	,
	author       = {
		Micceri, Theodore
	},
	date         = {
		1989
	},
	journaltitle = {
		Psychological Bulletin
	},
	title        = {
		The unicorn, the normal curve, and other improbable creatures
	},
	doi          = {
		10.1037/0033-2909.105.1.156
	},
	number       = {
		1
	},
	pages        = {
		156--166
	},
	volume       = {
		105
	},
	publisher    = {
		American Psychological Association ({APA})
	},
	abtsract = {
		An investigation of the distributional characteristics of 440 large-sample achievement and psychometric measures found all to be significantly nonnormal at the alpha .01 significance level.
		Several classes of contamination were found,
		including tail weights from the uniform to the double exponential,
		exponential-level asymmetry,
		severe digit preferences,
		multimodalities,
		and modes external to the mean/median interval.
		Thus,
		the underlying tenets of normality-assuming statistics appear fallacious for these commonly used types of data.
		However,
		findings here also fail to support the types of distributions used in most prior robustness research suggesting the failure of such statistics under nonnormal conditions.
		A reevaluation of the statistical robustness literature appears appropriate in light of these findings.
	},
}

@Article{
	Lib-Nonnormality-Sawilowsky-1992
	,
	author       = {
		Sawilowsky, Shlomo S. 
		and 
		Blair, R. Clifford
	},
	date         = {
		1992
	},
	journaltitle = {
		Psychological Bulletin
	},
	title        = {
		A more realistic look at the robustness and {Type II} error properties of the t test to departures from population normality
	},
	doi          = {
		10.1037/0033-2909.111.2.352
	},
	number       = {
		2
	},
	pages        = {
		352--360
	},
	volume       = {
		111
	},
	publisher    = {
		American Psychological Association ({APA})
	},
	abstract     = {
		The Type I and II error properties of the t test were evaluated by means of a Monte Carlo study that sampled 8 real distribution shapes identified by T. Micceri (1986, 1989) as being representative of types encountered in psychology and education research.
		Results showed the independent-samples t tests to be reasonably robust to Type I error when (1) sample sizes are equal, (2) sample sizes are fairly large, and (3) tests are 2-tailed rather than 1-tailed. 
		Nonrobust results were obtained primarily under distributions with extreme skew.
		The t test was robust to Type II error under these nonnormal distributions,
		but researchers should not overlook robust nonparametric competitors that are often more powerful than the t test when its underlying assumptions are violated.
	},
}

@Article{
	Lib-Nonnormality-Blanca-2013
	,
	author       = {
		Blanca, Mar{\'{\i}}a J. 
		and 
		Arnau, Jaume 
		and 
		L{\'{o}}pez-Montiel, Dolores 
		and 
		Bono, Roser 
		and 
		Bendayan, Rebecca
	},
	date         = {
		2013-05
	},
	journaltitle = {
		Methodology
	},
	title        = {
		Skewness and kurtosis in real data samples
	},
	doi          = {
		10.1027/1614-2241/a000057
	},
	number       = {
		2
	},
	pages        = {
		78--84
	},
	volume       = {
		9
	},
	publisher    = {
		Hogrefe Publishing Group
	},
	abstract     = {
		Parametric statistics are based on the assumption of normality.
		Recent findings suggest that Type I error and power can be adversely affected when data are non-normal.
		This paper aims to assess the distributional shape of real data by examining the values of the third and fourth central moments as a measurement of skewness and kurtosis in small samples.
		The analysis concerned 693 distributions with a sample size ranging from 10 to 30.
		Measures of cognitive ability and of other psychological variables were included.
		The results showed that skewness ranged between -2.49 and 2.33.
		The values of kurtosis ranged between -1.92 and 7.41.
		Considering skewness and kurtosis together the results indicated that only 5.5\% of distributions were close to expected values under normality.
		Although extreme contamination does not seem to be very frequent,
		the findings are consistent with previous research suggesting that normality is not the rule with real data.
	},
}

@Article{
	Lib-Nonnormality-Bono-2017,
	author       = {
		Bono, Roser 
		and 
		Blanca, Mar{\'{\i}}a J. 
		and 
		Arnau, Jaume 
		and 
		G{\'{o}}mez-Benito, 
		Juana
	},
	date         = {
		2017-09
	},
	journaltitle = {
		Frontiers in Psychology
	},
	title        = {
		Non-normal distributions commonly used in health, education, and social sciences: A systematic review
	},
	doi          = {
		10.3389/fpsyg.2017.01602
	},
	volume       = {
		8
	},
	publisher    = {
		Frontiers Media {SA}
	},
	abstract = {
		Statistical analysis is crucial for research and the choice of analytical technique should take into account the specific distribution of data.
		Although the data obtained from health,
		educational,
		and social sciences research are often not normally distributed,
		there are very few studies detailing which distributions are most likely to represent data in these disciplines.
		The aim of this systematic review was to determine the frequency of appearance of the most common non-normal distributions in the health, 
		educational,
		and social sciences.
		The search was carried out in the Web of Science database,
		from which we retrieved the abstracts of papers published between 2010 and 2015.
		The selection was made on the basis of the title and the abstract,
		and was performed independently by two reviewers.
		The inter-rater reliability for article selection was high (Cohen’s kappa = 0.84),
		and agreement regarding the type of distribution reached 96.5\%.
		A total of 262 abstracts were included in the final review.
		The distribution of the response variable was reported in 231 of these abstracts,
		while in the remaining 31 it was merely stated that the distribution was non-normal.
		In terms of their frequency of appearance,
		the most-common non-normal distributions can be ranked in descending order as follows:
		gamma,
		negative binomial,
		multinomial,
		binomial,
		lognormal,
		and exponential.
		In addition to identifying the distributions most commonly used in empirical studies these results will help researchers to decide which distributions should be included in simulation studies examining statistical procedures.
	},
}

@Comment{jabref-meta: databaseType:biblatex;}
% Encoding: US-ASCII

@Manual{
	Lib-R-Manual-2021
	,
	title = {
		{R}: A language and environment for statistical computing
	},
	author = {
		{R Core Team}
	},
	organization = {
		R Foundation for Statistical Computing
	},
	date = {
		2021
	},
	location = {
		Vienna, Austria
	},
	url = {
		https://www.R-project.org/
	},
	annotation = {
		Lib-R-Manual
	}
}

@Manual{
	Lib-R-Manual-2022
	,
	title = {
		{R}: A language and environment for statistical computing
	},
	author = {
		{R Core Team}
	},
	organization = {
		R Foundation for Statistical Computing
	},
	date = {
		2022
	},
	location = {
		Vienna, Austria
	},
	url = {
		https://www.R-project.org/
	},
	annotation = {
		Lib-R-Manual
	}
}

@Manual{
	Lib-R-Manual-2023
	,
	title = {
		{R}: A language and environment for statistical computing
	},
	author = {
		{R Core Team}
	},
	organization = {
		R Foundation for Statistical Computing
	},
	date = {
		2023
	},
	location = {
		Vienna, Austria
	},
	url = {
		https://www.R-project.org/
	},
	annotation = {
		Lib-R-Manual
	}
}

@Comment{jabref-meta: databaseType:biblatex;}
% Encoding: US-ASCII

@Book{
	Lib-R-Packages-MASS-2002
	,
	author    = {
		Venables, W. N. 
		and 
		Ripley, B. D.
	},
	date      = {
		2002
	},
	title     = {
		Modern applied statistics with {S}
	},
	doi       = {
		10.1007/978-0-387-21706-2
	},
	publisher = {
		Springer New York
	},
	annotation = {
		Lib-R-Packages
	},
}

@Article{
	Lib-R-Packages-mice-2011
	,
	author       = {
		Stef van Buuren 
		and 
		Karin Groothuis-Oudshoorn
	},
	date         = {
		2011
	},
	journaltitle = {
		Journal of Statistical Software
	},
	title        = {
		{mice}: Multivariate Imputation by Chained Equations in {R}
	},
	doi          = {
		10.18637/jss.v045.i03
	},
	number       = {
		3
	},
	volume       = {
		45
	},
	publisher    = {
		Foundation for Open Access Statistic
	},
	file = {
		references/10.18637%2Fjss.v045.i03.pdf
	},
	library = {},
	keywords = {
		MICE, 
		multiple imputation, 
		chained equations, 
		fully conditional specification,
		Gibbs sampler, 
		predictor selection, 
		passive imputation, 
		R.
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-R-Packages
	},
	abstract = {
		The R package mice imputes incomplete multivariate data by chained equations. 
		The software mice 1.0 appeared in the year 2000 as an S-PLUS library, and in 2001 as an R package. 
		mice 1.0 introduced predictor selection, passive imputation and automatic pooling. 
		This article documents mice, which extends the functionality of mice 1.0 in several ways. 
		In mice, the analysis of imputed data is made completely general, whereas the range of models under which pooling works is substantially extended. 
		mice adds new functionality for imputing multilevel data, automatic predictor selection, data handling, post-processing imputed values, specialized pooling routines, model selection tools, and diagnostic graphs.
		Imputation of categorical data is improved in order to bypass problems caused by perfect prediction. 
		Special attention is paid to transformations, sum scores, indices and interactions using passive imputation, and to the proper setup of the predictor matrix. 
		mice can be downloaded from the Comprehensive R Archive Network. 
		This article provides a hands-on, stepwise approach to solve applied incomplete data problems.
	},
}

@Manual{
	Lib-R-Packages-bootstrap-2019
	,
	title = {
		{bootstrap}: Functions for the Book ``{An introduction to the bootstrap}''
	},
	author = {
		Tibshirani, Rob
		and
		Leisch, Friedrich
	},
	date = {
		2019
	},
	note = {
		R package version 2019.6
	},
	addendum = {
		(S original and from StatLib and by Rob Tibshirani.
		R port by Friedrich Leisch.)
	},
	url = {
		"https://CRAN.R-project.org/package=bootstrap"
	},
	annotation = {
		Lib-R-Packages
	},
	abstract = {
		Software (bootstrap, cross-validation, jackknife) and data
		for the book ``An Introduction to the Bootstrap''
		by B. Efron and R. Tibshirani, 1993, Chapman and Hall.
		This package is primarily provided for projects already based on it,
		and for support of the book.
		New projects should preferentially use the recommended package ``boot''.
	}
}
  
@Manual{
	Lib-R-Packages-boot-2020
	,
	title = {
		{boot}: Bootstrap {R (S-Plus)} Functions
	},
	author = {
		Canty, Angelo
		and
	Ripley, B. D.},
	date = {2020},
	note = {
		R package version 1.3-25
	},
	url = {
		"https://CRAN.R-project.org/package=boot"
	},
	annotation = {
		Lib-R-Packages
	},
	abstract = {
		Functions and datasets for bootstrapping
		from the book ``Bootstrap Methods and Their Application''
		by A. C. Davison and D. V. Hinkley (1997, CUP),
		originally written by Angelo Canty for \texttt{S}.
	}
}

@Manual{
	Lib-R-Packages-lm.beta-2014
	,
	title  = {
		{lm.beta}: Add standardized regression coefficients to lm-objects
	},
	author = {
		Behrendt, Stefan
	},
	date   = {
		2014
	},
	note = {
		R package version 1.5-1
	},
	url    = {
		"https://CRAN.R-project.org/package=lm.beta"
	},
	annotation = {
		Lib-R-Packages
	},
}

@Manual{
	Lib-R-Packages-jmv-2020
	,
	title = {
		{jmv}: The 'jamovi' Analyses
	},
	author = {
		Selker, Ravi 
		and 
		Love, Jonathon 
		and 
		Dropmann, Damian
	},
	date = {
		2020
	},
	note = {
		R package version 1.2.23
	},
	url = {
		"https://CRAN.R-project.org/package=jmv"
	},
	annotation = {
		Lib-R-Packages
	},
}

@Manual{
	Lib-R-Packages-jasp-2022
	,
	author = {
		{JASP Team}
	},
	title = {
		{JASP (Version 0.16.1)[Computer software]}
	},
	date = {
		2022
	},
	url = {
		"https://jasp-stats.org/"
	},
	annotation = {
		Lib-R-Packages
	},
}

% Rocker

@Article{
	Lib-R-Packages-Boettiger-2017
	,
	author       = {
		Boettiger, Carl 
		and 
		Eddelbuettel, Dirk
	},
	date         = {
		2017
	},
	journaltitle = {
		The R Journal
	},
	title        = {
		An introduction to {Rocker}: Docker containers for {R}
	},
	doi          = {
		10.32614/rj-2017-065
	},
	number       = {
		2
	},
	pages        = {
		527
	},
	volume       = {
		9
	},
	abstract     = {
		We describe the Rocker project, 
		which provides a widely-used suite of Docker images with customized R environments for particular tasks. 
		We discuss how this suite is organized, 
		and how these tools can increase portability, 
		scaling, 
		reproducibility, 
		and convenience of R users and developers.
	},
	publisher    = {
		The R Foundation
	},
	annotation = {
		Lib-R-Packages
	},
}

@Article{
	Lib-R-Packages-Nuest-2020
	,
	author = {
		Nüst, Daniel
		and
		Eddelbuettel, Dirk
		and
		Bennett, Dom
		and
		Cannoodt, Robrecht 
		and 
		Clark, Dav 
		and 
		Daróczi, Gergely 
		and
		Edmondson, Mark 
		and 
		Fay, Colin 
		and 
		Hughes, Ellis 
		and 
		Kjeldgaard, Lars 
		and 
		Lopp, Sean 
		and 
		Marwick, Ben 
		and 
		Nolis, Heather
		and 
		Nolis, Jacqueline 
		and 
		Ooi, Hong 
		and 
		Ram, Karthik 
		and 
		Ross, Noam 
		and 
		Shepherd, Lori 
		and 
		Sólymos, Péter 
		and 
										
		Swetnam, Tyson Lee
		and 
		Turaga, Nitesh 
		and 
		Van Petegem, Charlotte
		and
		Williams, Jason
		and
		Willis, Craig
		and
		Xiao, Nan
	},
	date         = {
		2020
	},
	journaltitle = {
		The R Journal
	},
	title        = {
		The {Rockerverse}: Packages and applications for containerisation with {R}
	},
	doi          = {
		10.32614/rj-2020-007
	},
	number       = {
		1
	},
	pages        = {
		437
	},
	volume       = {
		12
	},
	abstract     = {
		The Rocker Project provides widely used Docker images for R across different application scenarios. 
		This article surveys downstream projects that build upon the Rocker Project images and presents the current state of R packages for managing Docker images and controlling containers. 
		These use cases cover diverse topics such as package development, 
		reproducible research, 
		collaborative work, 
		cloud-based data processing, 
		and production deployment of services. 
		The variety of applications demonstrates the power of the Rocker Project specifically and containerisation in general. 
		Across the diverse ways to use containers, we identified common themes: 
		reproducible environments, 
		scalability and efficiency, 
		and portability across clouds. 
		We conclude that the current growth and diversification of use cases is likely to continue its positive impact, 
		but see the need for consolidating the Rockerverse ecosystem of packages, 
		developing common practices for applications, and exploring alternative containerisation software.
	},
	publisher    = {
		The R Foundation
	},
	annotation = {
		Lib-R-Packages
	},
}

@Manual{
	Lib-R-Packages-shiny-2020
	,
	author = {
		Chang, Winston 
		and 
		Cheng, Joe 
		and 
		Allaire, J. J. 
		and 
		Xie, Yihui 
		and 
		McPherson, Jonathan
	},
	title = {
		{shiny}: Web application framework for {R}
	}
	,
	year = {
		2020
	},
	note = {
		R package version 1.5.0
	},
	url = {
		"https://CRAN.R-project.org/package=shiny"
	},
	publisher    = {
		The R Foundation
	},
	annotation = {
		Lib-R-Packages
	},
}

@Manual{
	Lib-R-Packages-fungible-2022
	,
	author = {
		Waller, Niels G.
	},
	title = {
		{fungible}: Psychometric functions from the {Waller Lab}
	}
	,
	year = {
		2022
	},
	note = {
		R package version 2.2.1
	},
	url = {
		"https://CRAN.R-project.org/package=fungible"
	},
	publisher    = {
		The R Foundation
	},
	annotation = {
		Lib-R-Packages
	},
}
  
@Comment{jabref-meta: databaseType:biblatex;}
% Encoding: US-ASCII

@Article{
	Lib-Simulation-Robustness-Cochran-1952
	,
	author       = {
		Cochran, William G.
	},
	date         = {
		1952-09
	},
	journaltitle = {
		The Annals of Mathematical Statistics
	},
	title        = {
		The $\chi^{2}$ test of goodness of fit
	},
	doi          = {
		10.1214/aoms/1177729380
	},
	number       = {
		3
	},
	pages        = {
		315--345
	},
	volume       = {
		23
	},
	publisher    = {
		Institute of Mathematical Statistics
	},
	file = {
		references/10.1214%2Faoms%2F1177729380.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Simulation-Robustness
	},
	abstract     = {
		This paper contains an expository discussion of the chi square test of goodness of fit, 
		intended for the student and user of statistical theory rather than for the expert.
		Part I describes the historical development of the distribution theory on which the test rests.
		Research bearing on the practical application of the test--in particular on the minimum expected number per class and the construction of classes--is discussed in Part II.
		Some varied opinions about the extent to which the test actually is useful to the scientist are presented in Part III. 
		Part IV outlines a number of tests that have been proposed as substitutes for the chi square test (the $\omega^2$ test, 
		the smooth test, 
		the likelihood ratio test) and Part V a number of supplementary tests (the run test, tests based on low moments, 
		subdivision of chi square into components).
	},
}

@Article{
	Lib-Simulation-Robustness-Bradley-1978
	,
	author       = {
		Bradley, James V.
	},
	date         = {
		1978-11
	},
	journaltitle = {
		British Journal of Mathematical and Statistical Psychology
	},
	title        = {
		Robustness?
	},
	doi          = {
		10.1111/j.2044-8317.1978.tb00581.x
	},
	number       = {
		2
	},
	pages        = {
		144--152
	},
	volume       = {
		31
	},
	publisher    = {
		Wiley
	},
	file = {
		references/10.1111%2Fj.2044-8317.1978.tb00581.x.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Simulation-Robustness
	},
	abstract     = {
		The actual behaviour of the probability of a Type I error under assumption violation is quite complex,
		depending upon a wide variety of interacting factors.
		Yet allegations of robustness tend to ignore its highly particularistic nature and neglect to mention important qualifying conditions. 
		The result is often a vast overgeneralization which nevertheless is difficult to refute since a standard quantitative definition of what constitutes robustness does not exist. 
		Yet under any halfway reasonable quantitative definition,
		many of the most prevalent claims of robustness would be demonstrably false. 
		Therefore robustness is a highly questionable concept.
	},
}

@Article{
	Lib-Simulation-Robustness-Serlin-1985
	,
	author       = {
		Serlin, Ronald C. 
		and 
		Lapsley, Daniel K.
	},
	date         = {
		1985
	},
	journaltitle = {
		American Psychologist
	},
	title        = {
		Rationality in psychological research: The good-enough principle
	},
	doi          = {
		10.1037/0003-066x.40.1.73
	},
	number       = {
		1
	},
	pages        = {
		73--83
	},
	volume       = {
		40
	},
	publisher    = {
		American Psychological Association ({APA})
	},
	file = {
		references/10.1037%2F0003-066X.40.1.73.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Simulation-Robustness
	},
	abstract     = {
		Reexamines methodological and procedural issues raised by P. Meehl (1967; see also PA, Vol 62:5042) that question the rationality of psychological inquiry. 
		Issues concern the asymmetry in theory testing between psychology and physics and the slow progress observed in psychological research. 
		A good-enough principle is proposed to resolve Meehl's methodological paradox, 
		and a more powerful reconstruction of science developed by I. Lakatos (1978) is suggested to account for the actual practice of psychological researchers.
	},
}

@Article{
	Lib-Simulation-Robustness-Robey-1992
	,
	author       = {
		Robey, Randall R. 
		and 
		Barcikowski, Robert S.
	},
	date         = {
		1992-11
	},
	journaltitle = {
		British Journal of Mathematical and Statistical Psychology
	},
	title        = {
		Type {I} error and the number of iterations in {Monte Carlo} studies of robustness
	},
	doi          = {
		10.1111/j.2044-8317.1992.tb00993.x
	},
	number       = {
		2
	},
	pages        = {
		283--288
	},
	volume       = {
		45
	},
	publisher    = {
		Wiley
	},
	file = {
		references/10.1111%2Fj.2044-8317.1992.tb00993.x.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Simulation-Robustness
	},
	abstract     = {
		A recent survey of simulation studies concluded that an overwhelming majority of papers do not report a rationale for the decision regarding the number of Monte Carlo iterations. 
		A surprisingly large number of reports do not contain a justifiable definition of robustness and many studies are conducted with an insufficient number of iterations to achieve satisfactory statistical conclusion validity. 
		The implication is that we do not follow our own advice regarding the management of Type I and Type II errors when conducting Monte Carlo experiments. 
		This paper reports a straightforward application of a well-known procedure for the purpose of objectively determining the exact number of iterations necessary to confidently detect departures from robustness in Monte Carlo results. 
		A table of the number of iterations necessary to detect departures from a series of nominal Type I error rates is included.
	},
}

@Article{
	Lib-Simulation-Robustness-Serlin-2000
	,
	author       = {
		Serlin, Ronald C.
	},
	date         = {
		2000
	},
	journaltitle = {
		Psychological Methods
	},
	title        = {
		Testing for robustness in {Monte Carlo} studies
	},
	doi          = {
		10.1037/1082-989x.5.2.230
	},
	number       = {
		2
	},
	pages        = {
		230--240
	},
	volume       = {
		5
	},
	publisher    = {
		American Psychological Association ({APA})
	},
	file = {
		references/10.1037%2F1082-989x.5.2.230.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Simulation-Robustness
	},
	abstract     = {
		Monte Carlo studies provide the information needed to help researchers select appropriate analytical procedures under design conditions in which the underlying assumptions of the procedures are not met.
		In Monte Carlo studies, 
		the 2 errors that one could commit involve (a) concluding that a statistical procedure is robust when it is not or (b) concluding that it is not robust when it is. 
		In previous attempts to apply standard statistical design principles to Monte Carlo studies, 
		the less severe of these errors has been wrongly designated the Type I error. 
		In this article, 
		a method is presented for controlling the appropriate Type I error rate; 
		the determination of the number of iterations required in a Monte Carlo study to achieve desired power is described; 
		and a confidence interval for a test's true Type I error rate is derived. 
		A robustness criterion is also proposed that is a compromise between W. G. Cochran's (1952) and J. V. Bradley's (1978) criteria.
	},
}

@Comment{jabref-meta: databaseType:biblatex;}
% Encoding: US-ASCII

@Book{
	Lib-Structural-Equation-Modeling-Nonnormality-and-Misspecification-Fuller-1987
	,
	author    = {
		Fuller, Wayne A.
	},
	date      = {
		1987-06
	},
	title     = {
		Measurement error models
	},
	doi       = {
		10.1002/9780470316665
	},
	editor    = {
		Fuller, Wayne A.
	},
	publisher = {
		John Wiley {\&} Sons, Inc.
	},
	file = {
		references/9780470316665.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Structural-Equation-Modeling-Nonnormality-and-Misspecification
	},
}

@InBook{
	Lib-Structural-Equation-Modeling-Nonnormality-and-Misspecification-Satorra-1994
	,
	author    = {
		Satorra, A.
		and
		Bentler, P. M.
	},
	booktitle = {
		Latent variables analysis: Applications for developmental research
	},
	date      = {
		1994
	},
	title     = {
		Corrections to test statistics and standard errors in covariance structure analysis
	},
	editor    = {
		von Eye A.
		and
		Clogg, C. C.
	},
	pages     = {
		399--419
	},
	file = {},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Structural-Equation-Modeling-Nonnormality-and-Misspecification
	},
	abstract  = {
		A. Satorra and P. Bentler . . . developed an approach to the asymptotic behavior of covariance structure statistics that rather naturally yields corrections to the goodness-of-fit statistic of the scaling and Satterthwaite types / present these results and . . . illustrate how they improve upon the uncorrected statistics that are now implemented in the field of covariance structure analysis / [show] that the proposed corrections not only encompass the ones advocated by A. Shapiro and M. Browne (1987) in case of elliptical data but do not suffer from the drawback of Browne-Shapiro's corrections of lack of robustness against deviations from the assumption of an elliptical distribution / provides a theory for correcting the standard covariance matrix of the vector of parameter estimates},
}

@Article{
	Lib-Structural-Equation-Modeling-Nonnormality-and-Misspecification-Hoogland-1998
	,
	author       = {
		Hoogland, Jeffrey J.
		and
		Boosma, Anne
	},
	date         = {
		1998-02
	},
	journaltitle = {
		Sociological Methods {\&} Research
	},
	title        = {
		Robustness studies in covariance structure modeling
	},
	doi          = {
		10.1177/0049124198026003003
	},
	number       = {
		3
	},
	pages        = {
		329--367
	},
	volume       = {
		26
	},
	publisher    = {{SAGE} Publications},
	file = {
		references/10.1177%2F0049124198026003003.pdf
	},
	library = {},
	keywords = {},
	addendum = {},
	note = {},
	annotation = {
		Lib-Structural-Equation-Modeling-Nonnormality-and-Misspecification
	},
	abstract     = {
		In covariance structure modeling,
		several estimation methods are available.
		The robustness of an estimator against specific violations of assumptions can be determined empirically by means of a Monte Carlo study.
		Many such studies in covariance structure analysis have been published,
		but the conclusions frequently seem to contradict each other.
		An overview of robustness studies in covariance structure analysis is given,
		and an attempt is made to generalize findings.
		Robustness studies are described and distinguished from each other systematically by means of certain characteristics.
		These characteristics serve as explanatory variables in a meta-analysis concerning the behavior of parameter estimators,
		standard error estimators,
		and goodness-of-fit statistics when the model is correctly specified.
	},
}

@Article{
	Lib-Structural-Equation-Modeling-Nonnormality-and-Misspecification-Satorra-2001
	,
	author       = {
		Satorra, Albert 
		and
		Bentler, Peter M.
	},
	date         = {
		2001-12
	},
	journaltitle = {
		Psychometrika
	},
	title        = {
		A scaled difference chi-square test statistic for moment structure analysis
	},
	doi          = {
		10.1007/bf02296192
	},
	number       = {
		4
	},
	pages        = {
		507--514
	},
	volume       = {
		66
	},
	publisher    = {
		Springer Science and Business Media {LLC}
	},
	file = {
		references/10.1007%2Fbf02296192.pdf
	},
	library = {},
	keywords = {
		moment-structures,
		goodness-of-fit test,
		chi-square difference test statistic,
		chi-square distribution,
		nonnormality
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Structural-Equation-Modeling-Nonnormality-and-Misspecification
	},
	abstract     = {
		A family of scaling corrections aimed to improve the chi-square approximation of goodness-of-fit test statistics in small samples,
		large models,
		and nonnormal data was proposed in Satorra and Bentler (1994).
		For structural equations models,
		Satorra-Bentler's (SB) scaling corrections are available in standard computer software.
		Often,
		however,
		the interest is not on the overall fit of a model,
		but on a test of the restrictions that a null model say $M_{0}$ implies on a less restricted one $M_{1}$.
		If $T_{0}$ and $T_{1}$ denote the goodness-of-fit test statistics associated to $M_{0}$ and $M_{1}$,
		respectively,
		then typically the difference $T_{d} = T_{0} - T_{1}$ is used as a chi-square test statistic with degrees of freedom equal to the difference on the number of independent parameters estimated under the models $M_{0}$ and $M_{1}$.
		As in the case of the goodness-of-fit test,
		it is of interest to scale the statistic $T_{d}$ in order to improve its chi-square approximation in realistic,
		that is,
		nonasymptotic and nonormal,
		applications.
		In a recent paper,
		Satorra (2000) shows that the difference between two SB scaled test statistics for overall model fit does not yield the correct SB scaled difference test statistic.
		Satorra developed an expression that permits scaling the difference test statistic,
		but his formula has some practical limitations,
		since it requires heavy computations that are not available in standard computer software.
		The purpose of the present paper is to provide an easy way to compute the scaled difference chi-square statistic from the scaled goodness-of-fit test statistics of models $M_{0}$ and $M_{1}$.
		A Monte Carlo study is provided to illustrate the performance of the competing statistics.
	},
}

@Article{
	Lib-Structural-Equation-Modeling-Nonnormality-and-Misspecification-Savalei-2014
	,
	author       = {
		Savalei, Victoria
	},
	date         = {
		2014-01
	},
	journaltitle = {
		Structural Equation Modeling: A Multidisciplinary Journal
	},
	title        = {
		Understanding robust corrections in structural equation modeling
	},
	doi          = {
		10.1080/10705511.2013.824793
	},
	number       = {
		1
	},
	pages        = {
		149--160
	},
	volume       = {
		21
	},
	publisher    = {
		Informa {UK} Limited
	},
	file = {
		references/10.1080%2F10705511.2013.824793.pdf
	},
	library = {},
	keywords = {
		nonnormal data,
		robust standard errors,
		Satorra-Bentler scaled chi-square
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Structural-Equation-Modeling-Nonnormality-and-Misspecification
	},
	abstract     = {
		Robust corrections to standard errors and test statistics have wide applications in structural equation modeling (SEM). 
		The original SEM development, due to Satorra and Bentler (1988, 1994), was to account for the effect of nonnormality. 
		Muth\'{e}n (1993) proposed corrections to accompany certain categorical data estimators, such as cat-LS or cat-DWLS.
		Other applications of robust corrections exist.
		Despite the diversity of applications, all robust corrections are constructed using the same underlying rationale:
		They correct for inefficiency of the chosen estimator.
		The goal of this article is to make the formulas behind all types of robust corrections more intuitive.
		This is accomplished by building an analogy with similar equations in linear regression and then by reformulating the SEM model as a nonlinear regression model.
	},
}

@Article{
	Lib-Structural-Equation-Modeling-Nonnormality-and-Misspecification-Savalei-2021
	,
	author       = {
		Savalei, Victoria 
		and 
		Rosseel, Yves
	},
	date         = {
		2021-10
	},
	journaltitle = {
		Structural Equation Modeling: A Multidisciplinary Journal
	},
	title        = {
		Computational options for standard errors and test statistics with incomplete normal and nonnormal data in {SEM}
	},
	doi          = {
		10.1080/10705511.2021.1877548
	},
	number       = {
		2
	},
	pages        = {
		163--181
	},
	volume       = {
		29
	},
	publisher    = {
		Informa {UK} Limited
	},
	file = {
		references/10.1080%2F10705511.2021.1877548.pdf
	},
	library = {},
	keywords = {
		incomplete data,
		nonnormal data,
		robust corrections,
		software implementation
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Structural-Equation-Modeling-Nonnormality-and-Misspecification
	},
	abstract     = {
		This article provides an overview of different computational options for inference following normal theory maximum likelihood (ML) estimation in structural equation modeling (SEM) with incomplete normal and nonnormal data.
		Complete data are covered as a special case.
		These computational options include whether the information matrix is observed or expected,
		whether the observed information matrix is estimated numerically or using an analytic asymptotic approximation,
		and whether the information matrix and the outer product matrix of the score vector are evaluated at the saturated or at the structured estimates.
		A variety of different standard errors and robust test statistics become possible by varying these options.
		We review the asymptotic properties of these computational variations, and we show how to obtain them using lavaan in R.
		We hope that this article will encourage methodologists to study the impact of the available computational options on the performance of standard errors and test statistics in SEM.
	},
}

@Article{
	Lib-Structural-Equation-Modeling-Nonnormality-and-Misspecification-Du-2021
	,
	author       = {
		Du, Han
		and
		Bentler, Peter M.
	},
	date         = {
		2021-06
	},
	journaltitle = {
		Psychological Methods
	},
	title        = {
		Distributionally weighted least squares in structural equation modeling
	},
	doi          = {
		10.1037/met0000388
	},
	publisher    = {
		American Psychological Association ({APA})
	},
	file = {
		references/10.1037%2Fmet0000388.pdf
	},
	library = {},
	keywords = {
		SEM,
		robust,
		statistics
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Structural-Equation-Modeling-Nonnormality-and-Misspecification
	},
	abstract     = {
		In real data analysis with structural equation modeling,
		data are unlikely to be exactly normally distributed.
		If we ignore the non-normality reality,
		the parameter estimates,
		standard error estimates,
		and model fit statistics from normal theory based methods such as maximum likelihood (ML) and normal theory based generalized least squares estimation (GLS) are unreliable.
		On the other hand,
		the asymptotically distribution free (ADF) estimator does not rely on any distribution assumption but cannot demonstrate its efficiency advantage with small and modest sample sizes.
		The methods which adopt misspecified loss functions including ridge GLS (RGLS) can provide better estimates and inferences than the normal theory based methods and the ADF estimator in some cases.
		We propose a distributionally weighted least squares (DLS) estimator,
		and expect that it can perform better than the existing generalized least squares,
		because it combines normal theory based and ADF based generalized least squares estimation.
		Computer simulation results suggest that model-implied covariance based DLS (DLSM) provided relatively accurate and efficient estimates in terms of RMSE.
		In addition,
		the empirical standard errors,
		the relative biases of standard error estimates,
		and the Type I error rates of the Jiang-Yuan rank adjusted model fit test statistic (TJY) in DLSM were competitive with the classical methods including ML, GLS, and RGLS.
		The performance of DLSM depends on its tuning parameter a.
		We illustrate how to implement DLSM and select the optimal a by a bootstrap procedure in a real data example.
	},
		
}

@Comment{jabref-meta: databaseType:biblatex;}
% Encoding: US-ASCII

@Manual{
	Lib-Structural-Equation-Modeling-Software-Manuals-Arbuckle-2014
	,
	author    = {
		Arbuckle, James L.
	},
	date      = {
		2014
	},
	title     = {
		Amos 23.0 User's Guide
	},
	location  = {
		Chicago
	},
	publisher = {
		IBM SPSS
	},
}

@Manual{
	Lib-Structural-Equation-Modeling-Software-Manuals-Arbuckle-2019
	,
	author    = {
		Arbuckle, James L.
	},
	date      = {
		2019
	},
	title     = {
		Amos 26.0 User's Guide
	},
	location  = {
		Chicago
	},
	publisher = {
		IBM SPSS
	},
}

@Manual{
	Lib-Structural-Equation-Modeling-Software-Manuals-Arbuckle-2020
	,
	author    = {
		Arbuckle, James L.
	},
	date      = {
		2020
	},
	title     = {
		Amos 27.0 User's Guide
	},
	location  = {
		Chicago
	},
	publisher = {
		IBM SPSS
	},
}

@Manual{
	Lib-Structural-Equation-Modeling-Software-Manuals-Arbuckle-2021
	,
	author    = {
		Arbuckle, James L.
	},
	date      = {
		2021
	},
	title     = {
		Amos 28.0 User's Guide
	},
	location  = {
		Chicago
	},
	publisher = {
		IBM SPSS
	},
}

@Manual{
	Lib-Structural-Equation-Modeling-Software-Manuals-Bentler-2006
	,
	author    = {
		Bentler, Peter M.
	},
	date      = {
		2006
	},
	title     = {
		{EQS} 6 structural equations program manual
	},
	isbn      = {
		1-885898-03-7
	},
	location  = {
		Encino, CA
	},
	publisher = {
		Multivariate Software, Inc.
	},
}

@Manual{
	Lib-Structural-Equation-Modeling-Software-Manuals-Muthen-2017
	,
	author    = {
		Muth\'en, L. K. and Muth\'en, B. O.
	},
	date      = {
		2017
	}, % 1998-2017
	title     = {
		{Mplus} user’s guide. {Eighth} edition
	},
	location  = {
		Los Angeles, CA
	},
	publisher = {
		Muth\'en \& Muth\'en
	},
}

@Article{
	Lib-Structural-Equation-Modeling-Software-Manuals-Neale-2015
	,
	author       = {
		Neale, Michael C. 
		and 
		Hunter, Michael D. 
		and 
		Pritikin, Joshua N. 
		and 
		Zahery, Mahsa 
		and 
		Brick, Timothy R. 
		and 
		Kirkpatrick, Robert M. 
		and 
		Estabrook, Ryne 
		and 
		Bates, Timothy C. 
		and 
		Maes, Hermine H. 
		and 
		Boker, Steven M.
	},
	date         = {
		2015-01
	},
	journaltitle = {
		Psychometrika
	},
	title        = {
		{OpenMx} 2.0: Extended structural equation and statistical modeling
	},
	doi          = {
		10.1007/s11336-014-9435-8
	},
	number       = {
		2
	},
	pages        = {
		535--549
	},
	volume       = {
		81
	},
	publisher    = {
		Springer Science and Business Media {LLC}
	},
	file = {
		references/10.1007%2Fs11336-014-9435-8.pdf
	},
	library = {},
	keywords = {
		structural equation modeling,
		path analysis,
		item factor analysis,
		state space modeling,
		mixture distribution,
		latent class analysis,
		optimization,
		big data,
		time series,
		behavior genetics,
		substance use data analysis,
		full information maximum likelihood,
		ordinal data
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Structural-Equation-Modeling-Software-Manuals
	},
	abstract = {
		The new software package OpenMx 2.0 for structural equation and other statistical modeling is introduced and its features are described. 
		OpenMx is evolving in a modular direction and now allows a mix-and-match computational approach that separates model expectations from fit functions and optimizers. 
		Major backend architectural improvements include a move to swappable open-source optimizers such as the newly written CSOLNP. 
		Entire new methodologies such as item factor analysis and state space modeling have been implemented. 
		New model expectation functions including support for the expression of models in LISREL syntax and a simplified multigroup expectation function are available. 
		Ease-of-use improvements include helper functions to standardize model parameters and compute their Jacobian-based standard errors, access to model components through standard R \$ mechanisms, and improved tab completion from within the R Graphical User Interface.
	},
}

@Article{
	Lib-Structural-Equation-Modeling-Software-Manuals-Rosseel-2012
	,
	author       = {
		Rosseel, Yves
	},
	date         = {
		2012
	},
	journaltitle = {
		Journal of Statistical Software
	},
	title        = {
		{lavaan}: An {R} package for structural equation modeling
	},
	doi          = {
		10.18637/jss.v048.i02
	},
	number       = {
		2
	},
	volume       = {
		48
	},
	publisher    = {
		Foundation for Open Access Statistic
	},
	file = {
		references/10.18637%2Fjss.v048.i02.pdf
	},
	library = {},
	keywords = {
		structural equation modeling, 
		latent variables, 
		path analysis, 
		factor analysis
	},
	addendum = {},
	note = {},
	annotation = {
		Lib-Structural-Equation-Modeling-Software-Manuals
	},
	abstract = {
		Structural equation modeling (SEM) is a vast field and widely used by many applied researchers in the social and behavioral sciences. 
		Over the years, many software packages for structural equation modeling have been developed, both free and commercial. 
		However, perhaps the best state-of-the-art software packages in this field are still closed-source and/or commercial. 
		The R package lavaan has been developed to provide applied researchers, teachers, and statisticians, a free, fully open-source, but commercial-quality package for latent variable modeling. 
		This paper explains the aims behind the development of the package, gives an overview of its most important features, and provides some examples to illustrate how lavaan works in practice. 
	},
}

@Manual{
	Lib-Structural-Equation-Modeling-Software-Manuals-Jorgensen-2022
	,
	title = {
		{semTools}: Useful tools for structural equation modeling
	},
	author = {
		Jorgensen, Terrence D.
		and
		Pornprasertmanit, Sunthud
		and
		Schoemann, Alexander M.
		and
		Rosseel, Yves
	},
	year = {
		2022
	},
	note = {
		R package version 0.5-6
	},
	url = {
		"https://CRAN.R-project.org/package=semTools"
	},
}

@Comment{jabref-meta: databaseType:biblatex;}
